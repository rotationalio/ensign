[{"id":0,"href":"/system/configuration/","title":"Configuration","section":"System","content":"Note: This page is for internal Ensign development and will probably not be very useful to Ensign users.\nConfiguration # Ensign services are primarily configured using environment variables and will respect dotenv files in the current working directory. The canonical reference of the configuration for an Ensign service is the config package of that service (described below). This documentation enumerates the most important configuration variables, their default values, and any hints or warnings about how to use them.\nRequired Configuration\nIf a configuration parameter does not have a default value that means it is required and must be specified by the user! If the configuration parameter does have a default value then that environment variable does not have to be set. Ensign # The Ensign node is a replica of the Ensign eventing system. Its environment variables are all prefixed with the ENSIGN_ tag. The primary configuration is as follows:\nEnvVar Type Default Description ENSIGN_MAINTENANCE bool false Sets the node to maintenance mode, which will respond to requests with Unavailable except for status requests. ENSIGN_LOG_LEVEL string info The verbosity of logging, one of trace, debug, info, warn, error, fatal, or panic. ENSIGN_CONSOLE_LOG bool false If true will print human readable logs instead of JSON logs for machine consumption. ENSIGN_BIND_ADDR string :5356 The address and port the Ensign service will listen on. Monitoring # Ensign uses Prometheus for metrics and observability. The Prometheus metrics server is configured as follows:\nEnvVar Type Default Description ENSIGN_MONITORING_ENABLED bool true If true, the Prometheus metrics server is served. ENSIGN_MONITORING_BIND_ADDR string :1205 The address and port the metrics server will listen on. ENSIGN_MONITORING_NODE_ID string Optional - a server name to tag metrics with. Storage # The Ensign storage configuration defines on disk where Ensign keeps its data. Configure storage as follows:\nEnvVar Type Default Description ENSIGN_STORAGE_READ_ONLY bool false If true then no writes or deletes will be allowed to the database. ENSIGN_STORAGE_DATA_PATH string The path to a directory on disk where Ensign will store its meta and event data. ENSIGN_STORAGE_TESTING bool false If true then a mock store will be opened rather than a leveldb store (should not be used in production). Note that the Ensign data path must be to a directory. If the directory does not exist, it is created. An error occurs if the path is to a file or the process doesn\u0026rsquo;t have permissions to access the directory. Ensign will open two different data stores in the data path: one for metadata and the other to store event data locally.\nIf the testing flag is set to true, a mock store is created that can be used in unit and integration tests.\nAuthentication # Ensign uses Quarterdeck to authenticate and authorize requests. This configuration defines how Ensign accesses public keys for JWT verification and how the authentication interceptor behaves.\nEnvVar Type Default Description ENSIGN_AUTH_KEYS_URL string https://auth.rotational.app/.well-known/jwks.json The path to the public keys used to verify JWT tokens. ENSIGN_AUTH_AUDIENCE string https://ensign.rotational.app The audience to verify inside of JWT tokens. ENSIGN_AUTH_ISSUER string https://auth.rotational.app The issuer to verify inside of JWT tokens. ENSIGN_AUTH_MIN_REFRESH_INTERVAL duration 5m The minimum time to wait before refreshing the JWKS public keys in the validator. Sentry # Ensign uses Sentry to assist with error monitoring and performance tracing. Configure Ensign to use Sentry as follows:\nEnvVar Type Default Description ENSIGN_SENTRY_DSN string The DSN for the Sentry project. If not set then Sentry is considered disabled. ENSIGN_SENTRY_SERVER_NAME string Optional - a server name to tag Sentry events with. ENSIGN_SENTRY_ENVIRONMENT string The environment to report (e.g. development, staging, production). Required if Sentry is enabled. ENSIGN_SENTRY_RELEASE string {{version}} Specify the release of Ensign for Sentry tracking. By default this will be the package version. ENSIGN_SENTRY_TRACK_PERFORMANCE bool false Enable performance tracing to Sentry with the specified sample rate. ENSIGN_SENTRY_SAMPLE_RATE float64 0.2 The percentage of transactions to trace (0.0 to 1.0). ENSIGN_SENTRY_DEBUG bool false Set Sentry to debug mode for testing. Sentry is considered enabled if a DSN is configured. Performance tracing is only enabled if Sentry is enabled and track performance is set to true. If Sentry is enabled, an environment is required, otherwise the configuration will be invalid.\nGenerally speaking, Ensign should enable Sentry for panic reports but should not enable performance tracing as this slows down the server too much. Note also that the sentry.Config object has a field Repanic that should not be set by the user. This field is used to manage panics in chained interceptors.\nTenant # The Tenant API powers the user front-end for tenant management and configuration. Its environment variables are all prefixed with the TENANT_ tag. The primary configuration is as follows:\nEnvVar Type Default Description TENANT_MAINTENANCE bool false Sets the server to maintenance mode, which will respond to requests with Unavailable except for status requests. TENANT_BIND_ADDR string :8080 The address and port the Tenant service will listen on. TENANT_MODE string release Sets the Gin mode, one of debug, release, or test. TENANT_LOG_LEVEL string info The verbosity of logging, one of trace, debug, info, warn, error, fatal, or panic. TENANT_CONSOLE_LOG bool false If true will print human readable logs instead of JSON logs for machine consumption. TENANT_ALLOW_ORIGINS string http://localhost:3000 A comma separated list of allowed origins for CORS. Set to \u0026ldquo;*\u0026rdquo; to allow all origins. Authentication # Tenant uses Quarterdeck to authenticate and authorize requests. The following configuration defines how Tenant validates JWT tokens passed in from the user that were created by Quarterdeck and secures cookies:\nEnvVar Type Default Description TENANT_AUTH_KEYS_URL string https://auth.rotational.app/.well-known/jwks.json The path to the public keys used to verify JWT tokens. TENANT_AUTH_AUDIENCE string https://rotational.app The audience to verify inside of JWT tokens. TENANT_AUTH_ISSUER string https://auth.rotational.app The issuer to verify inside of JWT tokens. TENANT_AUTH_COOKIE_DOMAIN string rotational.app Defines the domain that is set on cookies including CSRF cookies and token cookies. Database # Tenant connects to a replicated trtl database for its data storage; the trtl connection is configured as follows:\nEnvVar Type Default Description TENANT_DATABASE_URL string trtl://localhost:4436 The endpoint of the trtl database including the scheme (usually the k8s trtl service). TENANT_DATABASE_INSECURE bool true Connect to the trtl database without TLS (default true inside of a k8s cluster). TENANT_DATABASE_CERT_PATH string The path to the mTLS certificate of the client to connect to trtl in an authenticated fashion. TENANT_DATABASE_POOL_PATH string The path to an x509 pool file with trusted trtl servers to connect to in. Quarterdeck # Tenant connects directly to Quarterdeck in order to send pass-through requests from the user (e.g. for registration, login, etc). This is separate from the authentication configuration used in middleware as this configuration is used to setup a Quarterdeck API client.\nEnvVar Type Default Description TENANT_QUARTERDECK_URL string https://auth.rotational.app The Quarterdeck endpoint to create the API client on. TENANT_QUARTERDECK_WAIT_FOR_READY duration 5m The amount of time to wait for Quarterdeck to come online before exiting with fatal. SendGrid # Tenant uses SendGrid to assist with email notifications. Configure Tenant to use SendGrid as follows:\nEnvVar Type Default Description TENANT_SENDGRID_API_KEY string API Key to authenticate to SendGrid with. TENANT_SENDGRID_FROM_EMAIL string ensign@rotational.io The email address in the \u0026ldquo;from\u0026rdquo; field of emails being sent to users. TENANT_SENDGRID_ADMIN_EMAIL string admins@rotational.io The email address to send admin emails to from the server. TENANT_SENDGRID_ENSIGN_LIST_ID string A contact list to add users to if they sign up for notifications. TENANT_SENDGRID_TESTING bool false If in testing mode no emails are actually sent but are stored in a mock email collection. TENANT_SENDGRID_ARCHIVE string If in testing mode, specify a directory to save emails to in order to review emails being generated. SendGrid is considered enabled if the SendGrid API Key is set. The from and admin email addresses are required if SendGrid is enabled.\nIf the Ensign List ID is configured then Tenant will add the contact requesting private beta access to that list, otherwise it will simply add the contact to \u0026ldquo;all contacts\u0026rdquo;.\nSentry # Tenant uses Sentry to assist with error monitoring and performance tracing. Configure Tenant to use Sentry as follows:\nEnvVar Type Default Description TENANT_SENTRY_DSN string The DSN for the Sentry project. If not set then Sentry is considered disabled. TENANT_SENTRY_SERVER_NAME string Optional - a server name to tag Sentry events with. TENANT_SENTRY_ENVIRONMENT string The environment to report (e.g. development, staging, production). Required if Sentry is enabled. TENANT_SENTRY_RELEASE string {{version}} Specify the release of Ensign for Sentry tracking. By default this will be the package version. TENANT_SENTRY_TRACK_PERFORMANCE bool false Enable performance tracing to Sentry with the specified sample rate. TENANT_SENTRY_SAMPLE_RATE float64 0.2 The percentage of transactions to trace (0.0 to 1.0). TENANT_SENTRY_DEBUG bool false Set Sentry to debug mode for testing. Sentry is considered enabled if a DSN is configured. Performance tracing is only enabled if Sentry is enabled and track performance is set to true. If Sentry is enabled, an environment is required, otherwise the configuration will be invalid.\nNote also that the sentry.Config object has a field Repanic that should not be set by the user. This field is used to manage panics in chained interceptors.\nQuarterdeck # The Quarterdeck API handles authentication and authorization as well as API keys and billing management for the Ensign managed service. Its environment variables are all prefixed with the QUARTERDECK_ tag. The primary configuration is as follows:\nEnvVar Type Default Description QUARTERDECK_MAINTENANCE bool false Sets the server to maintenance mode, which will respond to requests with Unavailable except for status requests. QUARTERDECK_BIND_ADDR string :8088 The address and port the Quarterdeck service will listen on. QUARTERDECK_MODE string release Sets the Gin mode, one of debug, release, or test. QUARTERDECK_LOG_LEVEL string info The verbosity of logging, one of trace, debug, info, warn, error, fatal, or panic. QUARTERDECK_CONSOLE_LOG bool false If true will print human readable logs instead of JSON logs for machine consumption. QUARTERDECK_ALLOW_ORIGINS string http://localhost:3000 A comma separated list of allowed origins for CORS. Set to \u0026ldquo;*\u0026rdquo; to allow all origins. QUARTERDECK_VERIFY_BASE_URL string https://rotational.app/verify The base url to generate verify email links to direct the user to in the email verification path. SendGrid # Quarterdeck uses SendGrid to assist with email notifications. Configure Quarterdeck to use SendGrid as follows:\nEnvVar Type Default Description QUARTERDECK_SENDGRID_API_KEY string API Key to authenticate to SendGrid with. QUARTERDECK_SENDGRID_FROM_EMAIL string ensign@rotational.io The email address in the \u0026ldquo;from\u0026rdquo; field of emails being sent to users. QUARTERDECK_SENDGRID_ADMIN_EMAIL string admins@rotational.io The email address to send admin emails to from the server. QUARTERDECK_SENDGRID_ENSIGN_LIST_ID string A contact list to add users to if they sign up for notifications. QUARTERDECK_SENDGRID_TESTING bool false If in testing mode no emails are actually sent but are stored in a mock email collection. QUARTERDECK_SENDGRID_ARCHIVE string If in testing mode, specify a directory to save emails to in order to review emails being generated. SendGrid is considered enabled if the SendGrid API Key is set. The from and admin email addresses are required if SendGrid is enabled.\nIf the Ensign List ID is configured then Quarterdeck will add the contact to that list to ensure they receive marketing emails about Ensign.\nDatabase # EnvVar Type Default Description QUARTERDECK_DATABASE_URL string sqlite3:////data/db/quarterdeck.db The DSN for the sqlite3 database. QUARTERDECK_DATABASE_READ_ONLY bool false If true only read-only transactions are allowed. Quarterdeck uses a Raft replicated Sqlite3 database for authentication. The URI should have the scheme sqlite3:// and then a path to the database. For a relative path, use sqlite3:///path/to/relative.db and for an absolute path use sqlite3:////path/to/absolute.db.\nTokens # EnvVar Type Default Description QUARTERDECK_TOKEN_KEYS map[string]string The private keys to load into quarterdeck to issue JWT tokens with. QUARTERDECK_TOKEN_AUDIENCE string https://rotational.app The audience to add to the JWT keys for verification. QUARTERDECK_TOKEN_REFRESH_AUDIENCE string An optional additional audience to add only to refresh tokens. QUARTERDECK_TOKEN_ISSUER string https://auth.rotational.app The issuer to add to the JWT keys for verification. QUARTERDECK_TOKEN_ACCESS_DURATION duration 1h The amount of time that an access token is valid for before it expires. QUARTERDECK_TOKEN_REFRESH_DURATION duration 2h The amount of time that a refresh token is valid for before it expires. QUARTERDECK_TOKEN_REFRESH_OVERLAP duration -15m A negative duration that sets how much time the access and refresh tokens overlap in time validity. To create an environment variable that is a map[string]string use a string in the following form: key1:value1,key2:value2 The token keys should be ULIDs keys (for ordering) and a path value to the key pair to load from disk. Generally speaking there should be two keys - the current key and the most recent previous key, though more keys can be added for verification. Only the most recent key will be used to issue tokens, however. For example, here is a valid key map:\n01GECSDK5WJ7XWASQ0PMH6K41K:/data/keys/01GECSDK5WJ7XWASQ0PMH6K41K.pem,01GECSJGDCDN368D0EENX23C7R:/data/keys/01GECSJGDCDN368D0EENX23C7R.pem Future Feature\nNote that in the future quarterdeck will generate its own keys and will not need them to be set as in the configuration above. Sentry # Quarterdeck uses Sentry to assist with error monitoring and performance tracing. Configure Quarterdeck to use Sentry as follows:\nEnvVar Type Default Description QUARTERDECK_SENTRY_DSN string The DSN for the Sentry project. If not set then Sentry is considered disabled. QUARTERDECK_SENTRY_SERVER_NAME string Optional - a server name to tag Sentry events with. QUARTERDECK_SENTRY_ENVIRONMENT string The environment to report (e.g. development, staging, production). Required if Sentry is enabled. QUARTERDECK_SENTRY_RELEASE string {{version}} Specify the release of Ensign for Sentry tracking. By default this will be the package version. QUARTERDECK_SENTRY_TRACK_PERFORMANCE bool false Enable performance tracing to Sentry with the specified sample rate. QUARTERDECK_SENTRY_SAMPLE_RATE float64 0.2 The percentage of transactions to trace (0.0 to 1.0). QUARTERDECK_SENTRY_DEBUG bool false Set Sentry to debug mode for testing. Sentry is considered enabled if a DSN is configured. Performance tracing is only enabled if Sentry is enabled and track performance is set to true. If Sentry is enabled, an environment is required, otherwise the configuration will be invalid.\nNote also that the sentry.Config object has a field Repanic that should not be set by the user. This field is used to manage panics in chained interceptors.\nBeacon # A React app delivers Beacon, the Ensign UI. Its environment variables are all prefixed with the REACT_APP tag. The primary configuration is as follows:\nEnvVar Type Default Description REACT_APP_VERSION_NUMBER string The version number of the application build (set by tags in GitHub actions). REACT_APP_GIT_REVISION string The git revision (short) of the application build (set by tags in GitHub actions). REACT_APP_USE_DASH_LOCALE bool false If true the \u0026ldquo;dash\u0026rdquo; language is included for i18n debugging. API Information # Connection information for the backend is specified as follows:\nEnvVar Type Default Description REACT_APP_QUARTERDECK_BASE_URL string https://auth.rotational.app/v1 The endpoint and the version of the API to connect to Quarterdeck on. REACT_APP_TENANT_BASE_URL string https://api.rotational.app/v1 The endpoint and the version of the API to connect to Tenant on. Google Analytics # The React app uses Google Analytics to monitor website traffic. Configure the React app to use Google Analytics as follows:\nEnvVar Type Default Description REACT_APP_ANALYTICS_ID string Google Analytics tracking ID for the React App. Sentry # The React app uses Sentry to assist with error monitoring and performance tracing. Configure the React app to use Sentry as follows:\nEnvVar Type Default Description REACT_APP_SENTRY_DSN string The DSN for the Sentry project. If not set then Sentry is considered disabled. REACT_APP_SENTRY_ENVIRONMENT string The environment to report (e.g. development, staging, production). Required if Sentry is enabled. REACT_APP_SENTRY_EVENT_ID string Sentry is considered enabled if a DSN is configured. If Sentry is enabled, an environment is strongly suggested, otherwise the NODE_ENV environment will be used.\nDevelopment # Keep up to Date!\nIt is essential that we keep this configuration documentation up to date. The devops team uses it to ensure its services are configured correctly. Any time a configuration is changed ensure this documentation is also updated! TODO: this section will discuss envconfig, how to interpret environment variables from the configuration struct, how to test configuration, and how to add and change configuration variables. This section should also discuss dotenv files, docker compose, and all of the places where configuration can be influenced (e.g. GitHub actions for React builds).\n"},{"id":1,"href":"/getting-started/","title":"Getting Started","section":"Ensign Documentation","content":" Welcome! # Ready to get started with eventing? Let\u0026rsquo;s go!\nWhat is Ensign? # Ensign is a new eventing tool that make it fast, convenient, and fun to create event-driven microservices without needing a big team of devOps or platform engineers. All you need is a free API key to get started.\nGetting Started # The first step is to get an Ensign API key by visiting the sign-up page. Similar to getting a developer API key for Youtube, Twitter or Data.gov, you will need an API key to use Ensign and to follow along with the rest of this Quickstart guide.\nEnsign API Keys # Your key consists of two parts, a ClientID and a ClientSecret. The ClientID uniquely identifies you, and the ClientSecret proves that you have permission to create and access event data.\nAPI Key Component Name Length Characters Example ClientID 32 alphabetic (no digits) DbIxBEtIUgNIClnFMDmvoZeMrLxUTJVa ClientSecret 64 alphanumeric wAfRpXLTiWn7yo7HQzOCwxMvveqiHXoeVJghlSIK2YbMqOMCUiSVRVQOLT0ORrVS Together, the ClientID and ClientSecret uniquely identify you. They enable you to create Ensign topics, publishers, and subscribers, which will be the building blocks of your microservice! Keep in mind that the ClientID and ClientSecret should be kept private and not shared.\nPrerequisites # Ensign\u0026rsquo;s SDK currently supports Golang (Python and Javascript coming soon!). If you haven\u0026rsquo;t already:\ndownload and install Golang according to your operating system set up your GOPATH and workspace Install Ensign # In your command line, type the following to install the ensign API, SDK, and library code for Go:\ngo get -u github.com/rotationalio/ensign/sdks/go@latest Create a Client # After you\u0026rsquo;ve made a new Go project for this example, create a main.go file and add the dependencies you\u0026rsquo;ll need, which will include importing the Ensign API, SDK, and mimetypes.\nNext, create an Ensign client, which is similar to establishing a connection to a database like PostgreSQL or Mongo. To create the client, use the New method and pass in an ensign.Options struct that specifies your Client ID and Client Secret (described in the section above on getting an API key).\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;context\u0026#34; api \u0026#34;github.com/rotationalio/go-ensign/api/v1beta1\u0026#34; mimetype \u0026#34;github.com/rotationalio/go-ensign/mimetype/v1beta1\u0026#34; ensign \u0026#34;github.com/rotationalio/go-ensign\u0026#34; ) client, err := ensign.New(\u0026amp;ensign.Options{ ClientID: \u0026#34;DbIxBEtIUgNIClnFMDmvoZeMrLxUTJVa\u0026#34;, ClientSecret: \u0026#34;wAfRpXLTiWn7yo7HQzOCwxMvveqiHXoeVJghlSIK2YbMqOMCUiSVRVQOLT0ORrVS\u0026#34;, }) if err != nil { fmt.Errorf(\u0026#34;could not create client: %s\u0026#34;, err) } Congratulations, you now have an open connection to Ensign!\nCreate a Publisher # The next step is to start publishing data onto your event stream. Start by creating a publisher using the Publish method:\npub, err := client.Publish(context.Background()) if err != nil { fmt.Errorf(\u0026#34;could not create publisher: %s\u0026#34;, err) } Next, we need some data! Generally this is the place where you\u0026rsquo;d connect to your live data source (a database, Twitter feed, weather data, etc). But to keep things simple, we\u0026rsquo;ll just create a single event, which starts with a map.\ndata := make(map[string]string) data[\u0026#34;sender\u0026#34;] = \u0026#34;Twyla\u0026#34; data[\u0026#34;timestamp\u0026#34;] = time.Now().String() data[\u0026#34;message\u0026#34;] = \u0026#34;Let\u0026#39;s get this started!\u0026#34; Next, we will convert our map into an event, which will allow you to specify the mimetype of the message you intend to send (in this case, we\u0026rsquo;ll say it\u0026rsquo;s JSON), and the event type (which will be a generic event for this example). You\u0026rsquo;ll also need to pass in a TopicId, which will be a string. If you aren\u0026rsquo;t sure what TopicId to use, you can quickly log into your Ensign dashboard and look it up. For this example, we\u0026rsquo;ll pretend it\u0026rsquo;s \u0026quot;quality-lemon-time\u0026quot;:\ne := \u0026amp;api.Event{ TopicId: \u0026#34;quality-lemon-time\u0026#34;, Mimetype: mimetype.ApplicationJSON, Type: \u0026amp;api.Type{ Name: \u0026#34;Generic\u0026#34;, Version: 1, }, } Next, we\u0026rsquo;ll marshal our dictionary into the Data attribute of our sample event, and publish it by calling the Publish method on the publisher we created above:\ne.Data, _ = json.Marshal(data) pub.Publish(e) Create a Subscriber # Creating a subscriber is a bit more straightforward:\nsub, err := client.Subscribe(context.Background()) if err != nil { fmt.Errorf(\u0026#34;could not create subscriber: %s\u0026#34;, err) } var events \u0026lt;-chan *api.Event if events, err = sub.Subscribe(); err != nil { panic(\u0026#34;failed to create subscribe stream: \u0026#34; + err.Error()) } for msg := range events { fmt.Println(msg.Data) } Next Steps # You\u0026rsquo;re already well on your way to building your first event-driven microservice with Ensign!\nIf you\u0026rsquo;re ready to see some more advanced examples with code, check out the End-to-end Examples.\nIf you\u0026rsquo;re looking for more on the basics of event-driven systems, check out Eventing 101.\nHappy eventing!\n"},{"id":2,"href":"/eventing/use_cases/","title":"Use Cases","section":"Eventing 101","content":" Oh, The Places You’ll Go! (with Ensign) # Ensign is an eventing platform for developers that dramatically simplifies real-time apps and analytics. Here’s a list of ideas we dreamed up that are possible to build on Ensign. We grouped them by use case, but it\u0026rsquo;s by no means exhaustive. We hope it gets your creative wheels turning!\nCivic Engagement # Active First Responders Real-Time Civic Notices Digital Democracy Moderator-less Message Boards Anti-Human Trafficking Tools Climate Change # Environmental, Social, and Governance (ESG) Dashboard Climate Change Monitor Carbon Credit Exchange Weather Change Monitors Customer Experience # Live Customer Support \u0026amp; On-Call Management In-Store IoT for Point of Sale (e.g. \u0026ldquo;Buy as you shop\u0026rdquo; inventory) Real-Time Package Tracking Better Online Restaurant Ordering System Next Generation Ad Tracking Developer Tools # Application Performance Monitoring (APM) \u0026amp; Alerting Observability Tools Production \u0026amp; Test Environments Synchronization (that don\u0026rsquo;t interfere with process) Spot Instance Price Alerting Tool for Multi-Cloud Enterprise Experience # Real-time Anonymization Access Control \u0026amp; Identity Management Employee Performance Management/Human Performance Management Health # Integrated Patient Therapy Management Contract Tracing/ Outbreak Modeling Unified Digital Self (for Holistic Nutrition/Health) Industrial Applications # Industrial Maintenance \u0026amp; Repairs Synthetic Swarms Machine Learning Applications # Time Series Analytics CRDT-Powered Collaborative Jupyter Notebooks Real-Time Entity Resolution (De-Duplication \u0026amp; Canonicalization) Mobility # Flight Tracker Public Transport Tracker Advanced Car Maintenance \u0026amp; Diagnostics Social Events # Massively Multiplayer Online Live (MMOL) Scavenger Hunts Live Streaming Events (sports, conventions, etc.) Watch Party for Geo-Distributed Friends (virtual events) Fantasy Sports Gambling (as you watch) Social Media Feed Aggregation Multi-channel/Thread Dynamic Group Chat Personalized Multi-Source Newsfeed Supply Chain # Advanced Inventory Control \u0026amp; Sales Forecasting Disaster Recovery Supply Chain Dynamics Monitoring Ready to get started?\nWant to brainstorm a use case with us? Let us know at support@rotational.io.\nHappy Eventing!\n"},{"id":3,"href":"/eventing/data_sources/","title":"Data Sources","section":"Eventing 101","content":"Looking for inspiration? Check out some of these public streaming data sources!\nRealtime Source Data API Type SDKs Account Required Limits Finnhub Stock prices, company profiles, company \u0026amp; market news REST, Web Socket Go SDK Yes Unknown CoinCap Cryptocurrency prices across exchanges REST, Web Socket N/A No 200 requests/min Flight Data Vehicle and flight locations Open REST API Python API No 4000 daily credits DC WMATA Bus \u0026amp; train trip updates, alerts, and vehicle positions GTFS protocol buffers N/A Yes No Weather API Weather data REST Python, Go Yes No USGS Earthquake Data Earthquake data (time, location, etc) REST API No No No "},{"id":4,"href":"/examples/","title":"End-to-End Examples","section":"Ensign Documentation","content":" End-to-End Examples # This section of the documentation provides end-to-end examples using Ensign to help get you started!\nEnsign for Application Developers: In this end-to-end example, see how to curate a custom Twitter feed using Ensign. Create a publisher to start emitting tweets to a topic stream in just a few minutes! See how to create one or more asynchronous subscribers that can read off the stream and process the data as needed.\nEnsign for Data Engineers: This end-to-end example demonstrates how to retrieve and save weather data using Ensign and Watermill. Create a publisher to call the Weather API and emit the data to a topic stream and use Watermill\u0026rsquo;s router and SQL Pub/Sub to save the data into a PostgreSQL database.\nEnsign for Data Scientists: What does event-driven data science look like? In this example, see how to create an Ensign subscriber to Baleen, a live RSS ingestion engine, and use the incoming data to perform streaming HTML parsing, entity extraction, and sentiment analysis.\n"},{"id":5,"href":"/eventing/","title":"Eventing 101","section":"Ensign Documentation","content":" Eventing 101 # Still getting familiar with eventing basics? You\u0026rsquo;ve come to the right place!\nEventing Glossary Ensign: Frequently Asked Questions Resources # Prototyping Event-Driven Applications The Eventing Platform Landscape Gently Down the Stream Watermill "},{"id":6,"href":"/sdk/","title":"SDKs","section":"Ensign Documentation","content":" SDKs # This is the primary section for Ensign users.\n"},{"id":7,"href":"/examples/developers/","title":"Ensign for Application Developers","section":"End-to-End Examples","content":" Ensign for Application Developers # Hi there! This tutorial is targeted towards Golang application developers. If you are interested in or currently writing event-driven applications in Go you are in the right place! In this code-driven tutorial we will use the Ensign Golang SDK to publish curated tweets to an event stream and retrieve them in real time.\nIf you came here for the code the full example is available here.\nPrerequisites # To follow along with this tutorial you\u0026rsquo;ll need to:\nGenerate an API key to access Ensign Set up a developer account with Twitter (it\u0026rsquo;s free) Add a phone number to your Twitter developer account Set up your GOPATH and workspace Project Setup # The first thing we need to do is setup an environment to run our code. Let\u0026rsquo;s create a blank module with a suitable name for our project:\n$ mkdir tweets $ go mod init github.com/rotationalio/ensign-examples/go/tweets Next we\u0026rsquo;ll need to install the Go SDK client and its dependencies from the GitHub repo. In this tutorial we also use the go-twitter client to interact with the twitter API (although you can also create the requests yourself)!\n$ go get -u github.com/rotationalio/ensign/sdks/go@latest $ go get -u github.com/g8rswimmer/go-twitter/v2@latest Our project needs a publisher to write events to Ensign and a subscriber to read those events (asynchronously). In a real application these would most likely be independent microservices that run in different execution contexts (e.g. containers in a k8s cluster or even across different regions). Let\u0026rsquo;s create separate packages for the two command line applications as well as a shared package for our event schemas.\n$ mkdir publish $ mkdir subscribe $ mkdir schemas Sourcing Tweets # In event-driven systems, events are the main unit of data. In production applications events might be sourced from user actions, IoT devices, webhooks, or act as control signals between microservices.\nFor this example our data source is curated tweets from twitter. Create a file called main.go in the publish directory and add the following code to it.\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; \u0026#34;context\u0026#34; twitter \u0026#34;github.com/g8rswimmer/go-twitter/v2\u0026#34; ) type authorize struct { Token string } func (a authorize) Add(req *http.Request) { req.Header.Add(\u0026#34;Authorization\u0026#34;, fmt.Sprintf(\u0026#34;Bearer %s\u0026#34;, a.Token)) } func main() { var ( err error token string ) if token = os.Getenv(\u0026#34;TWITTER_API_BEARER_TOKEN\u0026#34;); token == \u0026#34;\u0026#34; { panic(\u0026#34;TWITTER_API_BEARER_TOKEN environment variable is required\u0026#34;) } query := flag.String(\u0026#34;query\u0026#34;, \u0026#34;distributed systems\u0026#34;, \u0026#34;Twitter search query\u0026#34;) flag.Parse() tweets := \u0026amp;twitter.Client{ Authorizer: authorize{ Token: *token, }, Client: http.DefaultClient, Host: \u0026#34;https://api.twitter.com\u0026#34;, } ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second) defer cancel() var rep *tweets.TweetRecentSearchResponse if rep, err = client.TweetRecentSearch(ctx, *query, twitter.TweetRecentSearchOpts{}); err != nil { panic(err) } for _, errs := range rep.Raw.Errors { fmt.Printf(\u0026#34;Error: %s\\n\u0026#34;, errs.Detail) } for _, tweet := range rep.Raw.Tweets { fmt.Printf(\u0026#34;%s: %s\\n\u0026#34;, tweet.AuthorID, tweet.Text) } } This is a simple command line application that will retrieve a single page of search results from twitter and print them out. Feel free to build the program and run it with any search query to make sure it works!\n$ export TWITTER_API_BEARER_TOKEN=# Your Twitter API bearer token goes here $ cd publish $ go build -o publish main.go $ ./publish --query \u0026#34;distributed systems\u0026#34; Creating a Publisher # Now that we have a data source, the next step is to create an Ensign client using the Client ID and Client Secret pair you received when generating your API key.\nimport ( ... twitter \u0026#34;github.com/g8rswimmer/go-twitter/v2\u0026#34; ensign \u0026#34;github.com/rotationalio/go-ensign\u0026#34; ) func main() { var ( err error token string ) if token = os.Getenv(\u0026#34;TWITTER_API_BEARER_TOKEN\u0026#34;); token == \u0026#34;\u0026#34; { panic(\u0026#34;TWITTER_API_BEARER_TOKEN environment variable is required\u0026#34;) } query := flag.String(\u0026#34;query\u0026#34;, \u0026#34;distributed systems\u0026#34;, \u0026#34;Twitter search query\u0026#34;) flag.Parse() // ENSIGN_CLIENT_ID and ENSIGN_CLIENT_SECRET environment variables must be set var client *ensign.Client if client, err = ensign.New(\u0026amp;ensign.Options{}); err != nil { panic(\u0026#34;failed to create Ensign client: \u0026#34; + err.Error()) } ... In the Go SDK, creating a Publisher interface from the client is straightforward.\nvar pub ensign.Publisher if pub, err = client.Publish(context.Background()); err != nil { panic(\u0026#34;failed to create publisher from Ensign client: \u0026#34; + err.Error()) } Publishing Events # In Ensign, events include a lot more than the data itself. As we can see from the protocol buffer, events are self-descriptive and are quite flexible.\ntype Event struct { state protoimpl.MessageState sizeCache protoimpl.SizeCache unknownFields protoimpl.UnknownFields Id string `protobuf:\u0026#34;bytes,1,opt,name=id,proto3\u0026#34; json:\u0026#34;id,omitempty\u0026#34;` TopicId string `protobuf:\u0026#34;bytes,2,opt,name=topic_id,json=topicId,proto3\u0026#34; json:\u0026#34;topic_id,omitempty\u0026#34;` Mimetype v1beta1.MIME `protobuf:\u0026#34;varint,3,opt,name=mimetype,proto3,enum=mimetype.v1beta1.MIME\u0026#34; json:\u0026#34;mimetype,omitempty\u0026#34;` Type *Type `protobuf:\u0026#34;bytes,4,opt,name=type,proto3\u0026#34; json:\u0026#34;type,omitempty\u0026#34;` Key []byte `protobuf:\u0026#34;bytes,5,opt,name=key,proto3\u0026#34; json:\u0026#34;key,omitempty\u0026#34;` Data []byte `protobuf:\u0026#34;bytes,6,opt,name=data,proto3\u0026#34; json:\u0026#34;data,omitempty\u0026#34;` Encryption *Encryption `protobuf:\u0026#34;bytes,7,opt,name=encryption,proto3\u0026#34; json:\u0026#34;encryption,omitempty\u0026#34;` Compression *Compression `protobuf:\u0026#34;bytes,8,opt,name=compression,proto3\u0026#34; json:\u0026#34;compression,omitempty\u0026#34;` Geography *Region `protobuf:\u0026#34;bytes,9,opt,name=geography,proto3\u0026#34; json:\u0026#34;geography,omitempty\u0026#34;` Publisher *Publisher `protobuf:\u0026#34;bytes,10,opt,name=publisher,proto3\u0026#34; json:\u0026#34;publisher,omitempty\u0026#34;` UserDefinedId string `protobuf:\u0026#34;bytes,11,opt,name=user_defined_id,json=userDefinedId,proto3\u0026#34; json:\u0026#34;user_defined_id,omitempty\u0026#34;` Created *timestamppb.Timestamp `protobuf:\u0026#34;bytes,14,opt,name=created,proto3\u0026#34; json:\u0026#34;created,omitempty\u0026#34;` Committed *timestamppb.Timestamp `protobuf:\u0026#34;bytes,15,opt,name=committed,proto3\u0026#34; json:\u0026#34;committed,omitempty\u0026#34;` } For this tutorial we are mostly concerned with the following fields.\nTopicId: Events are organized into topics and events in a topic usually follow a similar schema Mimetype: In Ensign all event data is generic \u0026ldquo;blob\u0026rdquo; data to allow for heterogenous event streams. The mimetype allows subcribers to deserialize data back into an understandable format. Type: Events in Ensign are tagged with schema type and versioning info to allow publishers and subscribers to lookup schemas in a shared registry. This is important because certain serialization methods (e.g. protobuf, parquet) require explicit schemas for deserialization and schema-less methods (e.g. JSON) can be enhanced with versioning. In this example we can get away with structured JSON. In production workflows we would most likely want to store the definition in a schema registry but for now let\u0026rsquo;s add it to tweets.go in the schemas directory so both our producer and subscriber can access it.\npackage schemas type Tweet struct { Author string `json:\u0026#34;author\u0026#34;` Text string `json:\u0026#34;text\u0026#34;` CreatedAt string `json:\u0026#34;created_at\u0026#34;` } Now that we know how to serialize JSON, in the tweet loop instead of printing to the console let\u0026rsquo;s go ahead and publish some events.\nfor _, tweet := range rep.Raw.Tweets { e := \u0026amp;api.Event{ TopicId: \u0026#34;tweets\u0026#34;, Mimetype: mimetype.ApplicationJSON, Type: \u0026amp;api.Type{ Name: \u0026#34;tweet\u0026#34;, Version: 1, }, } tweetObj := \u0026amp;schemas.Tweet{ Author: tweet.AuthorID, Text: tweet.Text, CreatedAt: tweet.CreatedAt, } if e.Data, err = json.Marshal(tweetObj); err != nil { panic(\u0026#34;could not marshal tweet to JSON: \u0026#34; + err.Error()) } // Publish the event to Ensign pub.Publish(e) // Check for errors if err = pub.Err(); err != nil { panic(\u0026#34;failed to publish event(s): \u0026#34; + err.Error()) } } If your IDE did not resolve the imports for you, you will need to specify them manually:\nimport ( ... api \u0026#34;github.com/rotationalio/go-ensign/api/v1beta1\u0026#34; mimetype \u0026#34;github.com/rotationalio/go-ensign/mimetype/v1beta1\u0026#34; ... ) Note that pub.Publish(e) does not return an immediate error, it\u0026rsquo;s an asynchronous operation so if we want to check for errors we have to do so after the fact. This means that we can\u0026rsquo;t be sure which event actually triggered the error.\nFinally, to make our publisher feel like a real service, we can add an outer loop with a ticker so that the program periodically pulls the most recent tweets our search query of choice. Another useful improvement might be to utilize the SinceID on the twitter search options so that we aren\u0026rsquo;t producing duplicate tweets!\nticker := time.NewTicker(10 * time.Second) sinceID := \u0026#34;\u0026#34; for { ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second) defer cancel() select { case \u0026lt;-ctx.Done(): return case \u0026lt;-ticker.C: fmt.Println(\u0026#34;searching for tweets...\u0026#34;) opts := twitter.TweetRecentSearchOpts{ SortOrder: twitter.TweetSearchSortOrderRecency, SinceID: sinceID, } var rep *twitter.TweetRecentSearchResponse if rep, err = tweets.TweetRecentSearch(ctx, *query, opts); err != nil { panic(err) } for _, errs := range rep.Raw.Errors { fmt.Printf(\u0026#34;Error: %s\\n\u0026#34;, errs.Detail) } for _, tweet := range rep.Raw.Tweets { e := \u0026amp;api.Event{ TopicId: \u0026#34;tweets\u0026#34;, Mimetype: mimetype.ApplicationJSON, Type: \u0026amp;api.Type{ Name: \u0026#34;Generic\u0026#34;, Version: 1, }, } if e.Data, err = json.Marshal(tweet); err != nil { panic(\u0026#34;could not marshal tweet to JSON: \u0026#34; + err.Error()) } pub.Publish(e) if err = pub.Err(); err != nil { panic(\u0026#34;failed to publish event(s): \u0026#34; + err.Error()) } fmt.Printf(\u0026#34;published tweet with ID: %s\\n\u0026#34;, tweet.ID) } if len(rep.Raw.Tweets) \u0026gt; 0 { sinceID = rep.Raw.Tweets[0].ID } } } At this point our publisher will be able to request some new tweets from Twitter every 10 seconds and publish them as events to the tweets topic. Go ahead and try it out!\n$ export ENSIGN_CLIENT_ID=# Your Ensign Client ID goes here $ export ENSIGN_CLIENT_SECRET=# Your Ensign Client Secret goes here $ go build -o publish main.go $ ./publish --query \u0026#34;otters\u0026#34; Note: Here the Ensign Client ID and Client Secret are retrieved from environment variables but it\u0026rsquo;s also possible to specify them in code\nCreating a subscriber # Similarly to the Publisher, a Subscriber interface can be created from an Ensign client. Once created, the Subscriber allows us to read events directly from a Go channel. Create a main.go in the subscribe directory and add the following code to it.\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/rotationalio/ensign-examples/go/tweets/schemas\u0026#34; api \u0026#34;github.com/rotationalio/go-ensign/api/v1beta1\u0026#34; ensign \u0026#34;github.com/rotationalio/go-ensign\u0026#34; ) func main() { var ( err error client *ensign.Client ) // ENSIGN_CLIENT_ID and ENSIGN_CLIENT_SECRET environment variables must be set if client, err = ensign.New(\u0026amp;ensign.Options{}); err != nil { panic(\u0026#34;failed to create Ensign client: \u0026#34; + err.Error()) } // Create a subscriber from the client var sub ensign.Subscriber if sub, err = client.Subscribe(context.Background()); err != nil { panic(\u0026#34;failed to create subscriber from client: \u0026#34; + err.Error()) } defer sub.Close() // Create the event stream as a channel var events \u0026lt;-chan *api.Event if events, err = sub.Subscribe(); err != nil { panic(\u0026#34;failed to create subscribe stream: \u0026#34; + err.Error()) } // Events are processed as they show up on the channel for event := range events { tweet := \u0026amp;schemas.Tweet{} if err = json.Unmarshal(event.Data, tweet); err != nil { panic(\u0026#34;failed to unmarshal event: \u0026#34; + err.Error()) } fmt.Printf(\u0026#34;received tweet %s\\n\u0026#34;, tweet.ID) fmt.Println(tweet.Text) fmt.Println() } } At this point you should be able to build and the run the subscriber in a second command window to retrieve tweet events in real time!\n$ export ENSIGN_CLIENT_ID=# Your Ensign Client ID $ export ENSIGN_CLIENT_SECRET=# Your Ensign Client Secret $ cd subscribe $ go build -o subscribe main.go $ ./subscribe What Next? # Hopefully this gets you on the right track and inspires some ideas for event-driven applications. If this example were to become a real application, here are some things we might consider.\nEvent Schemas # Remember that an Ensign event encodes a lot of metadata. When dealing with more strutured or versioned serialization formats such as protobuf, we definitely want to consider adding some logic to the subscriber to lookup the event schema in the schema registry or a local cache with the event.Type field.\nAdditional Topic Streams # With Ensign it\u0026rsquo;s easy to scale up by adding new topics. We might want to have different topics for error observability (e.g. if the Twitter API changes or schemas unexpectedly change), metrics capturing, or different types of Twitter queries.\nDownstream Processing # Once we have an event stream, what do we do with it? A traditional approach is to capture data into a database for persistence and to make it easy to materialize data views for application users. This is certainly possible with Ensign. However, Ensign also offers persistence of event streams, which makes it possible to perform historical queries on the streams themselves.\n"},{"id":8,"href":"/eventing/glossary/","title":"Glossary","section":"Eventing 101","content":"Here\u0026rsquo;s a handy list of terms to help get you started:\napi key # \u0026ldquo;API\u0026rdquo; stands for \u0026ldquo;Application Programming Interface\u0026rdquo;, which is a very broad term that refers (super high level) to the ways in which users or other applications can interact with an application.\nSome applications (like Ensign) require permission to interact with, such as a password, token, or key.\nYou can get a free Ensign API key by visiting rotational.io/ensign. Your key will consist of two parts, a ClientID and a ClientSecret. The ClientID uniquely identifies you, and the ClientSecret proves that you have permission to create and access event data. You will need to pass both of these in to create an Ensign client connection.\nasynchronous # An asynchronous microservice is one in which requests to a service and the subsequent responses are decoupled and can occur independently of each other.\nThis differs from the synchronous pattern, in which a client request (e.g. a query) is blocked from moving forward until a server response is received. Synchronous microservices can result in cascading failures and compounding latencies in applications.\nAsynchronous microservices can make it a lot easier for teams to develop and deploy components independently.\nAsynchronous microservices require an intermediary service usually known as a broker to hold messages emitted by a publisher that are awaiting retrieval from subscribers.\nbroker # An event broker is an intermediary service inside an asynchronous eventing system that stores events sent by publishers until they are received by all subscribers.\nBrokers are also in charge of things like keeping events in the correct order, remembering which subscribers are listening to a topic stream, recording the last message each subscriber retrieved, etc.\nIn Ensign, brokers can save events permanently even after they have been retrieved (to support \u0026ldquo;time travel\u0026rdquo; — the ability to retroactively scan through an event stream to support analytics and machine learning).\nclient # In order to write or read data from an underlying data system (like a database or event stream), you need a client to connect to the data system and interact with it as needed (such as reading and writing data). This connection often looks something like conn = DBConnection(credentials), and after creating the conn variable, subsequent lines of code can leverage it to perform the kinds of data interactions you wish to make.\nTo establish a client in Ensign you need an API key.\nimport ( ensign \u0026#34;github.com/rotationalio/go-ensign\u0026#34; ) client, err := ensign.New(\u0026amp;ensign.Options{ ClientID: \u0026#34;FMDmvoZeMrLxUTJVaDbIxBEtIUgNICln\u0026#34;, ClientSecret: \u0026#34;oeVJghlSIK2YbMqOMCUiSVRVQOLT0ORrVSwAfRpXLTiWn7yo7HQzOCwxMvveqiHX\u0026#34;, }) event # In an event-driven or microservice architecture, an event is the atomic element of data.\nAn event might look something like a dictionary, which is then wrapped in an object or struct that provides some schema information to help Ensign know how to serialize and deserialize your data.\norder := make(map[string]string) order[\u0026#34;item\u0026#34;] = \u0026#34;large mushroom pizza\u0026#34; order[\u0026#34;customer_id\u0026#34;] = \u0026#34;984445\u0026#34; order[\u0026#34;customer_name\u0026#34;] = \u0026#34;Enson J. Otterton\u0026#34; order[\u0026#34;timestamp\u0026#34;] = time.Now().String() evt := \u0026amp;api.Event{ TopicId: \u0026#34;order-feed\u0026#34;, Mimetype: mimetype.ApplicationJSON, Type: \u0026amp;api.Type{ Name: \u0026#34;Generic\u0026#34;, Version: 1, }, } evt.Data, _ = json.Marshal(order) latency # Latency can refer to both application-level communication lag (e.g. the time it takes for one part of the code to finish running before moving on to the next part) or to network communication lag (e.g. the time it takes for two remote servers on two different continents to send a single message back and forth).\nLess latency is better, generally speaking.\nIn a microservices context, we can reduce application latency by using asynchronous communications and parallelizing functions so they run faster. Network latency can be reduced by creating more efficient communications between servers (e.g. using more scalable consensus algorithms).\nmicroservice # A microservice is a computer application composed of a collection of lightweight services, each of which is responsible for some discrete task.\nMicroservices can be coordinated to communicate via events.\nmime type # A MIME (Multipurpose Internet Mail Extensions) type is a label that identifies a type of data, such as CSV, HTML, JSON, or protocol buffer.\nMIME types allow an application to understand how to handle incoming and outgoing data.\norganization # An Ensign organization is a collection of users who are working under the same Ensign tenant.\npublisher # In an event-driven microservice, a publisher is responsible for emitting events to a topic stream.\nIn Ensign, you can create a publisher once you have established a client:\npub, err := client.Publish(...) real-time # This is a tricky one because real-time can be used to mean different things. In some cases, \u0026ldquo;real-time\u0026rdquo; is used as a synonym for synchronous (i.e. the opposite of asynchronous). However, the term is also used to mean \u0026ldquo;very fast\u0026rdquo; or \u0026ldquo;low latency\u0026rdquo;.\nsdk # SDK stands for \u0026ldquo;Software Development Kit\u0026rdquo;. Software applications designed for a technical/developer audience frequently are considerate enough to provide user-facing SDKs in a few languages (e.g. Golang, Python, JavaScript). These SDKs give users a convenient way to interact with the application using a programming language with which they are familiar.\nEnsign currently offers two SDKs: the Golang SDK and a Watermill API-compatible SDK.\nstream # An event stream is a flow composed of many, many individual pieces of data called events.\nsubscriber # In an event-driven context, a subscriber is a downstream component that is listening for events published by a publisher onto a topic.\ntenant # A tenant is a user, group of users, team or company who share computing and/or storage resources.\ntopic # In event-driven microservices, a topic is a rough approximation of a traditional relational database table. In a relational DB, a table is a collection of related data fields arrayed as columns and rows. In an eventing context, a topic is a sequence of individual events populated with the same fields (aka schema).\n"},{"id":9,"href":"/examples/data_engineers/","title":"Ensign for Data Engineers","section":"End-to-End Examples","content":" Ensign for Data Engineers # We love data engineers — it\u0026rsquo;s how a lot of us got our starts in tech. One of the main reasons we made Ensign is to make it easier for you to put your data in motion. We know that a clumsy ETL routine can quickly turn a data lake into a data landfill.\nIn this example we\u0026rsquo;ll see how to move data around with Ensign. We\u0026rsquo;ll be using Watermill-Ensign and Watermill to call a Weather API and insert weather data into a PostgreSQL database. If you haven\u0026rsquo;t used Watermill yet, you\u0026rsquo;re in for a treat! Check out this introductory post that covers the basics.\nJust want the code? Check out this repo for the full example.\nETL Design # The architecture for this weather ingestor is composed of three components:\nAn Ensign publisher that calls the Weather API and publishes the weather data to a topic. An Ensign subscriber that listens on this topic and runs a check against the PostgreSQL database to see if this is a new record. The weather data doesn\u0026rsquo;t change that often, so it is possible to receive a duplicate record. If the record is new, we\u0026rsquo;ll put the record into a second topic. A sql publisher that inserts the records from the second topic into the database. The Ensign subscriber and the sql publisher are chained together using the router and handler functionality described in this post.\nPrerequisites # This tutorial assumes that the following steps have been completed:\nYou have installed watermil, ensign, watermill-ensign, and watermill-sql. You have received an Ensign Client ID and Client Secret. Refer to the getting started guide on how to obtain the key. You have received an API key from the Weather API website (it\u0026rsquo;s free!). You have Docker installed and running on your machine. Project Setup # First, let\u0026rsquo;s create a root directory weather_data called for the application.\nmkdir weather_data We will then create two subfolders, one for the component that calls the Weather API to get the latest weather data and the other for the component that receives the data and inserts it into the database.\ncd weather_data mkdir producer mkdir consumer Create the Ensign Publisher # Creating a publisher is very straightforward. You will need to have environment variables set up for the Ensign Client ID and Client Secret from your API Key. (Need a new key?)\npublisher, err := ensign.NewPublisher( ensign.PublisherConfig{ EnsignConfig: \u0026amp;ensign.Options{ ClientID: os.Getenv(\u0026#34;ENSIGN_CLIENT_ID\u0026#34;), ClientSecret: os.Getenv(\u0026#34;ENSIGN_CLIENT_SECRET\u0026#34;), }, Marshaler: ensign.EventMarshaler{}, }, logger, ) Call the Weather API # Before we call the Weather API, we need to create the following structs:\nFirst, a high level struct to represent the updates that come back from the Weather API.\ntype Response struct { Current Current `json:\u0026#34;current,omitempty\u0026#34;` } Next, a more detailed struct to help us parse all of the components of the Weather API\u0026rsquo;s response. Some of this will depend on how much detail you need to ingest for your downstream data users (will the data scientists on your team complain if you forget to ingest the full text description provided by the response?)\ntype Current struct { LastUpdated string `json:\u0026#34;last_updated,omitempty\u0026#34;` TempF float64 `json:\u0026#34;temp_f,omitempty\u0026#34;` Condition *CurrentCondition `json:\u0026#34;condition,omitempty\u0026#34;` WindMph float64 `json:\u0026#34;wind_mph,omitempty\u0026#34;` WindDir string `json:\u0026#34;wind_dir,omitempty\u0026#34;` PrecipIn float64 `json:\u0026#34;precip_in,omitempty\u0026#34;` Humidity int32 `json:\u0026#34;humidity,omitempty\u0026#34;` FeelslikeF float64 `json:\u0026#34;feelslike_f,omitempty\u0026#34;` VisMiles float64 `json:\u0026#34;vis_miles,omitempty\u0026#34;` } type CurrentCondition struct { Text string `json:\u0026#34;text,omitempty\u0026#34;` } Finally, a struct to represent whatever structure makes the most sense for the weather data in your organization (e.g. with your company\u0026rsquo;s database schemas or use cases in mind):\ntype ApiWeatherInfo struct { LastUpdated string Temperature float64 FeelsLike float64 Humidity int32 Condition string WindMph float64 WindDirection string Visibility float64 Precipitation float64 } Here is the code to create the request object:\nreq, err := http.NewRequest(\u0026#34;GET\u0026#34;, \u0026#34;http://api.weatherapi.com/v1/current.json?\u0026#34;, nil) Next, we will define the query parameters and add it to req. Note that you will need to create an environment variable called WAPIKEY that will be set to the API key you received from the Weather API.\nq := req.URL.Query() q.Add(\u0026#34;key\u0026#34;, os.Getenv(\u0026#34;WAPIKEY\u0026#34;)) q.Add(\u0026#34;q\u0026#34;, \u0026#34;Washington DC\u0026#34;) req.URL.RawQuery = q.Encode() Let\u0026rsquo;s create the http client to call the Weather API, parse the response object, and create an ApiWeatherInfo object.\n// create the client object client := \u0026amp;http.Client{} // retrieve the response resp, err := client.Do(req) // read the body of the response body, _ := ioutil.ReadAll(resp.Body) // unmarshal the body into a Response object err = json.Unmarshal(body, \u0026amp;response) // Convert the Response object into a ApiWeatherInfo object current := response.Current currentWeatherInfo := ApiWeatherInfo{ LastUpdated: current.LastUpdated, Temperature: current.TempF, FeelsLike: current.FeelslikeF, Humidity: current.Humidity, Condition: current.Condition.Text, WindMph: current.WindMph, WindDirection: current.WindDir, Visibility: current.VisMiles, Precipitation: current.PrecipIn, } Here is the complete function with some additional error handling.\nPublish the Data to a Topic # We\u0026rsquo;ll create a helper method publishWeatherData that takes in a publisher and a channel that is used as a signal to stop publishing. First, create a ticker to call the Weather API every 5 seconds. Next, we will call the GetCurrentWeather function that we constructed previously to retrieve weather data, serialize it, construct a Watermill message, and publish the message to the current_weather topic.\nfunc publishWeatherData(publisher message.Publisher, closeCh chan struct{}) { //weather doesn\u0026#39;t change that often - call the Weather API every 5 minutes ticker := time.NewTicker(5 * time.Minute) for { select { //if a signal has been sent through closeCh, publisher will stop publishing case \u0026lt;-closeCh: ticker.Stop() return case \u0026lt;-ticker.C: } //call the API to get the weather data weatherData, err := GetCurrentWeather() if err != nil { fmt.Println(\u0026#34;Issue retrieving weather data: \u0026#34;, err) continue } //serialize the weather data payload, err := json.Marshal(weatherData) if err != nil { fmt.Println(\u0026#34;Could not marshall weatherData: \u0026#34;, err) continue } //construct a watermill message msg := message.NewMessage(watermill.NewUUID(), payload) // Use a middleware to set the correlation ID, it\u0026#39;s useful for debugging middleware.SetCorrelationID(watermill.NewShortUUID(), msg) //publish the message to the \u0026#34;current weather\u0026#34; topic err = publisher.Publish(\u0026#34;current_weather\u0026#34;, msg) if err != nil { fmt.Println(\u0026#34;cannot publish message: \u0026#34;, err) continue } } } Start the First Stream # Next we want to create a long-running process that will continue pinging the Weather API, parsing the response, and publishing it for downstream consumption.\nIn our main() function, we will first create a logger using watermill.NewStdLogger. Then we create the publisher and create the closeCh channel that will be used to send a signal to the publisher to stop publishing. We then pass the publisher and the closeCh to the publishWeatherData function and run it in a goroutine.\nWe then create another channel that listens for a os.Interrupt signal and will close when it receives the signal. If it receives the signal (e.g. because we want to stop the process, or if something goes wrong), it closes and the code moves on to close the closeCh channel and that notifies the publisher to stop publishing.\nfunc main() { // add a logger logger := watermill.NewStdLogger(false, false) logger.Info(\u0026#34;Starting the producer\u0026#34;, watermill.LogFields{}) //create the publisher publisher, err := ensign.NewPublisher( ensign.PublisherConfig{ Marshaler: ensign.EventMarshaler{}, }, logger, ) if err != nil { panic(err) } defer publisher.Close() //used to signal the publisher to stop publishing closeCh := make(chan struct{}) go publishWeatherData(publisher, closeCh) // wait for SIGINT - this will end processing c := make(chan os.Signal, 1) signal.Notify(c, os.Interrupt) \u0026lt;-c // signal for the publisher to stop publishing close(closeCh) logger.Info(\u0026#34;All messages published\u0026#34;, nil) } Create the Ensign Subscriber # Next we\u0026rsquo;ll create a subscriber in much the same way as we created our publisher. Our subscriber will be in charge of listing for incoming weather messages from the publisher, and checking to see if the incoming data is actually new.\nsubscriber, err := ensign.NewSubscriber( ensign.SubscriberConfig{ EnsignConfig: \u0026amp;ensign.Options{ ClientID: os.Getenv(\u0026#34;ENSIGN_CLIENT_ID\u0026#34;), ClientSecret: os.Getenv(\u0026#34;ENSIGN_CLIENT_SECRET\u0026#34;), }, Unmarshaler: ensign.EventMarshaler{}, }, logger, ) Connect to the Database # Let\u0026rsquo;s write a quick function createPostgresConnection that will allow us to connect to our database. You will need to create the following environment variables to connect to your local PostgreSQL database: POSTGRES_USER, POSTGRES_PASSWORD, and POSTGRES_DB. You can use any values of your choosing for these variables. For convenience, in this example, we\u0026rsquo;ll use a docker PostgreSQL container. The function is below.\nfunc createPostgresConnection() *stdSQL.DB { host := \u0026#34;weather_db\u0026#34; port := 5432 user := os.Getenv(\u0026#34;POSTGRES_USER\u0026#34;) password := os.Getenv(\u0026#34;POSTGRES_PASSWORD\u0026#34;) dbname := os.Getenv(\u0026#34;POSTGRES_DB\u0026#34;) dsn := fmt.Sprintf( \u0026#34;host=%s port=%d user=%s password=%s dbname=%s sslmode=disable\u0026#34;, host, port, user, password, dbname ) db, err := stdSQL.Open(\u0026#34;postgres\u0026#34;, dsn) if err != nil { panic(err) } err = db.Ping() if err != nil { panic(err) } createQuery := `CREATE TABLE IF NOT EXISTS weather_info ( id SERIAL NOT NULL PRIMARY KEY, last_updated VARCHAR(50) NOT NULL, temperature DECIMAL, feels_like DECIMAL, humidity INTEGER, condition VARCHAR(36), wind_mph DECIMAL, wind_direction VARCHAR(36), visibility DECIMAL, precipitation DECIMAL, created_at VARCHAR(100) NOT NULL );` _, err = db.ExecContext(context.Background(), createQuery) if err != nil { panic(err) } log.Println(\u0026#34;created table weather_info\u0026#34;) return db } Note that above, the host is defined as weather_db and will match the name of the container. More on that later in the tutorial!\nWe will also define a WeatherInfo struct that will contain the fields of the weather_info table. It is similar to the ApiWeatherInfo struct with the exception of the CreatedAt, which is an additional field in the table.\ntype WeatherInfo struct { LastUpdated string Temperature float64 FeelsLike float64 Humidity int32 Condition string WindMph float64 WindDirection string Visibility float64 Precipitation float64 CreatedAt string } Does the Record Exist? # Next, we\u0026rsquo;ll create a function that will check the database to see if the record already exists there. Before we create the function, we need to create a dbHandler struct which implements the stdSQL.DB interface that will be used to query the database.\ntype dbHandler struct { db *stdSQL.DB } We then write our checkRecordExists function, which will get executed when a message arrives on the current_info topic.\nThe first step is to unmarshal the message into a ApiWeatherInfo object. Next we will execute a query to see if a record with the LastUpdated value exists and if it does not, we will create a new WeatherInfo object and add the current timestamp for the CreatedAt field. We will then marshal this object, create, and return a new watermill message which will get published to the weather_info topic. If LastUpdated already exists, we simply note that the record exists and return nil.\nfunc (d dbHandler) checkRecordExists(msg *message.Message) ([]*message.Message, error) { weatherInfo := ApiWeatherInfo{} err := json.Unmarshal(msg.Payload, \u0026amp;weatherInfo) if err != nil { return nil, err } log.Printf(\u0026#34;received weather info: %+v\u0026#34;, weatherInfo) var count int query := \u0026#34;SELECT count(*) FROM weather_info WHERE last_updated = $1\u0026#34; err = d.db.QueryRow(query, weatherInfo.LastUpdated).Scan(\u0026amp;count) switch { case err != nil: return nil, err default: if count \u0026gt; 0 { log.Println(\u0026#34;Found existing record in the database\u0026#34;) // not throwing an error here because this is not an issue return nil, nil } newWeatherInfo := WeatherInfo{ LastUpdated: weatherInfo.LastUpdated, Temperature: weatherInfo.Temperature, FeelsLike: weatherInfo.FeelsLike, Humidity: weatherInfo.Humidity, Condition: weatherInfo.Condition, WindMph: weatherInfo.WindMph, WindDirection: weatherInfo.WindDirection, Visibility: weatherInfo.Visibility, Precipitation: weatherInfo.Precipitation, CreatedAt: time.Now().String(), } log.Println(newWeatherInfo) log.Println(len(newWeatherInfo.CreatedAt)) var payload bytes.Buffer encoder := gob.NewEncoder(\u0026amp;payload) err := encoder.Encode(newWeatherInfo) if err != nil { panic(err) } newMessage := message.NewMessage(watermill.NewULID(), payload.Bytes()) return []*message.Message{newMessage}, nil } } Prepping the Database # Now we need to create a SQL publisher. A SQL publisher is a Watermill implementation of a SQL based pub/sub mechanism whereby you can use publishers to insert or upsert records and you can use subscribers to retrieve records. For more details, refer to this post on the Watermill site.\nThe SQL publisher is created as follows. Note that we are going to set AutoInitializeSchema to false because we\u0026rsquo;ve already created the table. The postgresSchemaAdapter is an extension of Watermill\u0026rsquo;s SchemaAdapter which provides the schema-dependent queries and arguments.\npub, err := sql.NewPublisher( db, sql.PublisherConfig{ SchemaAdapter: postgresSchemaAdapter{}, AutoInitializeSchema: false, }, logger, ) Then we\u0026rsquo;ll create a SchemaInitializingQueries function to make a table based on the topic name, but since we\u0026rsquo;ve already created the table, we\u0026rsquo;ll set this parameter to false and simply return an empty list.\nfunc (p postgresSchemaAdapter) SchemaInitializingQueries(topic string) []string { return []string{} } Inserting New Data # Next we will create an InsertQuery function that unmarshals the list of messages and creates an insertQuery sql statement that will be executed. The topic name is the same as the table name and it is used in the insertQuery sql statement. It then extracts the fields of the WeatherInfo object and puts them in the list of args. It then returns the sql statement and the arguments.\nfunc (p postgresSchemaAdapter) InsertQuery(topic string, msgs message.Messages) (string, []interface{}, error) { insertQuery := fmt.Sprintf( `INSERT INTO %s (last_updated, temperature, feels_like, humidity, condition, wind_mph, wind_direction, visibility, precipitation, created_at) VALUES %s`, topic, strings.TrimRight(strings.Repeat(`($1,$2,$3,$4,$5,$6,$7,$8,$9,$10),`, len(msgs)), \u0026#34;,\u0026#34;), ) var args []interface{} for _, msg := range msgs { weatherInfo := WeatherInfo{} decoder := gob.NewDecoder(bytes.NewBuffer(msg.Payload)) err := decoder.Decode(\u0026amp;weatherInfo) if err != nil { return \u0026#34;\u0026#34;, nil, err } args = append( args, weatherInfo.LastUpdated, weatherInfo.Temperature, weatherInfo.FeelsLike, weatherInfo.Humidity, weatherInfo.Condition, weatherInfo.WindMph, weatherInfo.WindDirection, weatherInfo.Visibility, weatherInfo.Precipitation, weatherInfo.CreatedAt, ) } return insertQuery, args, nil } Here\u0026rsquo;s the part where you\u0026rsquo;d probably set up a subscriber stream for those data scientists who are teaching GPTChat to be more conversant about the weather (or something like that). You\u0026rsquo;ll want to create custom SelectQuery and UnmarshalMessage functions, but since we are not using a SQL subscriber for this tutorial, we\u0026rsquo;ll skip that for now.\nfunc (p postgresSchemaAdapter) SelectQuery(topic string, consumerGroup string, offsetsAdapter sql.OffsetsAdapter) (string, []interface{}) { // No need to implement this method, as PostgreSQL subscriber is not used in this example. return \u0026#34;\u0026#34;, nil } func (p postgresSchemaAdapter) UnmarshalMessage(row *stdSQL.Row) (offset int, msg *message.Message, err error) { return 0, nil, errors.New(\u0026#34;not implemented\u0026#34;) } Start the Second Stream # Now we need to put all those last pieces together so that we\u0026rsquo;re storing data back to PostgreSQL.\nHere we will use the router functionality described here. We could have had the Ensign subscriber do the entire work of checking the database and inserting new records and not create a publisher at all. However, by decoupling the checking and the inserting functions, we can enable Ensign to scale up and down them independently, which could save some serious $$$ depending on how much throughput you\u0026rsquo;re dealing with.\nHere, we instantiate a new router, add a SignalsHandler plugin that will shut down the router if it receives a SIGTERM message. We also add a Recoverer middleware which handles any panics sent by the handler.\nNext, we will create the PostgreSQL connection and create the weather_info table. We then create the ensign subscriber and the sql publisher.\nWe will then add a handler to the router called weather_info_inserter, pass in the subscriber topic, subscriber, publisher, publisher topic, and the handlerFunc that will be executed when a new message appears on the subscriber topic.\nFinally, we will run the router.\nfunc main() { router, err := message.NewRouter(message.RouterConfig{}, logger) if err != nil { panic(err) } //SignalsHandler will gracefully shutdown Router when SIGTERM is received router.AddPlugin(plugin.SignalsHandler) //The Recoverer middleware handles panics from handlers router.AddMiddleware(middleware.Recoverer) postgresDB := createPostgresConnection() log.Println(\u0026#34;added postgres connection and created weather_info table\u0026#34;) subscriber := createSubscriber(logger) publisher := createPublisher(postgresDB) router.AddHandler( \u0026#34;weather_info_inserter\u0026#34;, weather_api_topic, //subscriber topic subscriber, weather_insert_topic, //publisher topic publisher, dbHandler{db: postgresDB}.checkRecordExists, ) if err = router.Run(context.Background()); err != nil { panic(err) } } Composing our Docker Container # Almost done! In this section we\u0026rsquo;ll create a docker-compose file in the weather_data directory to run our application.\nThe docker-compose file contains three services. The first service is the producer which requires the WAPIKEY environment variable used to call the Weather API. It will also need the ENSIGN_CLIENT_ID, and ENSIGN_CLIENT_SECRET to use Ensign. The second service is the consumer which needs the POSTGRES_USER, POSTGRES_DB, and POSTGRES_PASSWORD environment variables in order to connect to the database and it will also need ENSIGN_CLIENT_ID, and ENSIGN_CLIENT_SECRET to use Ensign. The third service is the postgres database, which is a docker image that will also require the same environment variables as the consumer. You will notice that the container name is weather_db, which is the host name that the consumer application uses to connect to the database and it has also got the same Postgres environment variables as the consumer.\nversion: \u0026#39;3\u0026#39; services: producer: image: golang:1.19 restart: unless-stopped volumes: - .:/app - $GOPATH/pkg/mod:/go/pkg/mod working_dir: /app/producer/ command: go run main.go environment: WAPIKEY: ${WAPIKEY} ENSIGN_CLIENT_ID: ${ENSIGN_CLIENT_ID} ENSIGN_CLIENT_SECRET: ${ENSIGN_CLIENT_SECRET} consumer: image: golang:1.19 restart: unless-stopped depends_on: - postgres volumes: - .:/app - $GOPATH/pkg/mod:/go/pkg/mod working_dir: /app/consumer/ command: go run main.go db.go environment: POSTGRES_USER: ${POSTGRES_USER} POSTGRES_DB: ${POSTGRES_DB} POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} ENSIGN_CLIENT_ID: ${ENSIGN_CLIENT_ID} ENSIGN_CLIENT_SECRET: ${ENSIGN_CLIENT_SECRET} postgres: image: postgres:12 restart: unless-stopped ports: - 5432:5432 container_name: weather_db environment: POSTGRES_USER: ${POSTGRES_USER} POSTGRES_DB: ${POSTGRES_DB} POSTGRES_PASSWORD: ${POSTGRES_PASSWORD} Let\u0026rsquo;s Gooooooooo # We made it to the end! Once you have all of the code in place, run the following commands on the terminal in the producer and consumer directories:\ngo mod init go mod tidy This will create the go.mod and the go.sum files in both directories. Next, move up to the weather_data directory and run the following command:\ndocker-compose up You will see all the applications running and messages printing to the screen.\nOn a separate terminal window, run the following command to view the contents of the weather_info table:\ndocker-compose exec weather_db psql -U $POSTGRES_USER -d $POSTGRES_DB -c \u0026#39;select * from weather_info;\u0026#39; Next Steps # Hopefully running this example gives you a general idea on how to build an event-driven application using Watermill and Ensign. You can modify this example slightly and have the Ensign consumer do the entire work of checking and inserting new weather records into the database (replace the handler with a NoPublisherHandler), but remember that loose coupling is the name of the game with event driven architectures! You can also challenge yourself by creating a consumer that takes the records produced by the publisher and updates a front end application with the latest weather data\u0026hellip;\nLet us know (info@rotational.io) what you end up making with Ensign!\n"},{"id":10,"href":"/examples/data_scientists/","title":"Ensign for Data Scientists","section":"End-to-End Examples","content":" Ensign for Data Scientists # Here\u0026rsquo;s a question we frequently get from our data scientist friends:\nWhat does event-driven data science even look like??\nIn this tutorial we\u0026rsquo;ll find out! Join along in implementing an event-driven Natural Language Processing tool that does streaming HTML parsing, entity extraction, and sentiment analysis.\nJust here for the code? Check it out here!\nPrerequisites # To follow along with this tutorial you\u0026rsquo;ll need to:\nGenerate an API key to access Ensign Set up your GOPATH and workspace Create an Ensign client Back to the Future # Did you know? Some of the earliest deployed machine learning apps were event-driven! That’s right — back in the 90’s, email spam filters used Bayesian models to learn on the fly.\nSpam filtering is an awesome example of a natural use case for online modeling. Each newly flagged spam message was a new training event, an opportunity to update the model in real time. While most machine learning bootcamps teach us to expect data in batches, there are a TON of natural use cases for streaming data science (maybe even more than for offline aka batchwise modeling!).\nAnother great use case for event-driven data science is Natural Language Processing tasks such as named entity recognition, sentiment analysis, and text classification. In this tutorial, we\u0026rsquo;ll tap into a live data feed and see how to process the text content as it streams in.\nA Whale of a Problem # The data we\u0026rsquo;re going to be working with in this tutorial comes from a live RSS feed ingestion engine called Baleen.\nIf you want to run your own Baleen, check out this branch. Then install the Baleen CLI:\n$ go install ./cmd/baleen/ Then you can add posts with\n$ baleen posts:add https://www.news-source-of-your-choice.com/link-to-article Baleen has an Ensign Publisher that emits new events every time a new article is ingested, and we can write a Subscriber to connect to the Baleen topic feed in order to tap into the feed stream (Note: make sure you create an Ensign client first!):\nimport ( // ... ensign \u0026#34;github.com/rotationalio/go-ensign\u0026#34; ) func main() { // ... var sub ensign.Subscriber if sub, err = client.Subscribe(context.Background()); err != nil { panic(\u0026#34;failed to create subscriber from client: \u0026#34; + err.Error()) } defer sub.Close() // ... } Next, you\u0026rsquo;ll want to create a channel to consume events from the stream:\nimport ( // ... api \u0026#34;github.com/rotationalio/go-ensign/api/v1beta1\u0026#34; ) // still inside main(): var events \u0026lt;-chan *api.Event if events, err = sub.Subscribe(); err != nil { panic(\u0026#34;failed to create subscribe stream: \u0026#34; + err.Error()) } // ... Now we\u0026rsquo;ve got the data feed ready, and the next step is create a loop that will listen on the events channel, and for each event it retrieves, do some NLP magic.\nNLP Magic Time # Ok, in the last section we set up our Ensign client and connected an Ensign Subscriber to Baleen\u0026rsquo;s Ensign Publisher.\nimport ( // ... post \u0026#34;github.com/rotationalio/baleen/events\u0026#34; ) // still inside main(): // Events are processed as they show up on the channel for event := range events { if event.Type.Name == \u0026#34;Document\u0026#34; { // Unmarshal the event into an HTML Document doc := \u0026amp;post.Document{} if _, err = doc.UnmarshalMsg(event.Data); err != nil { panic(\u0026#34;failed to unmarshal event: \u0026#34; + err.Error()) } // Do NLP magic here! } Parsing the Beautiful Soup # The first step in all real world text processing and modeling projects (well, after ingestion of course ;-D) is parsing. The specific parsing technique has a lot to do with the data; but in this case we\u0026rsquo;re starting with HTML documents, which is what Baleen\u0026rsquo;s Publisher delivers.\nMost data scientists are probably used to using BeautifulSoup for HTML document parsing (just as long as it\u0026rsquo;s not regex!). For those doing their data science tasks in Golang, check out Anas Khan\u0026rsquo;s soup package. Like the original Python package, it has a lot of great utilities for retrieving and preprocessing HTML.\nSince we already have our HTML (which is in the form of bytes), all we need to do to prepare the soup and grab all the paragraphs using the \u0026lt;p\u0026gt; tags:\ndoc := soup.HTMLParse(string(html_bytes)) paras := doc.FindAll(\u0026#34;p\u0026#34;) Now we can iterate over paras to process each paragraph chunk by chunk.\nMore than a Feeling # Let\u0026rsquo;s say that we want to do streaming sentiment analysis so that we can gauge the sentiment levels of the documents right away rather than in a batch analysis a month from now, when it may be too late to intervene!\nFor this we\u0026rsquo;ll leverage the sentiment analysis tools implemented in Connor DiPaolo\u0026rsquo;s Golang Sentiment Library.\nFirst we load the pre-trained model (trained using a Naive Bayes classifier, if you\u0026rsquo;re curious!) using the Restore function:\nimport ( \u0026#34;github.com/cdipaolo/sentiment\u0026#34; ) // ... var model sentiment.Models if model, err = sentiment.Restore(); err != nil { fmt.Println(\u0026#34;unable to load pretrained model\u0026#34;) } // ... Now, let\u0026rsquo;s iterate over the paras we extracted from the HTML in the section above and score the text of each using the pre-trained sentiment model:\n// ... var sentimentScores []uint8 for _, p := range paras { // Get the sentiment score for each paragraph analysis := model.SentimentAnalysis(p.Text(), sentiment.English) sentimentScores = append(sentimentScores, analysis.Score) // ... more magic coming soon! } We could look at the sentiment of each paragraph, but for tutorial purposes we\u0026rsquo;ll just take an average sentiment for the overall article:\n// Get the average sentiment score across all the paragraphs var total float32 = 0 for _, s := range sentimentScores { total += float32(s) } avgSentiment = total / float32(len(sentimentScores)) } But think of all the other things we can do with all that text!\nFinding the Who, Where, and What # Let\u0026rsquo;s add an entity extraction step to our iteration over the paras. For this we need another dependency, the prose library created by Joseph Kato.\nimport ( // ... prose \u0026#34;github.com/jdkato/prose/v2\u0026#34; ) // allocate empty entity map entities = make(map[string]string) for _, p := range paras { // Get sentiment score // Parse out the entities var parsed *prose.Document if parsed, err = prose.NewDocument(p.Text()); err != nil { fmt.Println(\u0026#34;unable to parse text\u0026#34;) } for _, ent := range parsed.Entities() { // add entities to map entities[ent.Text] = ent.Label } } For those familiar with the Python library spaCy, prose works in a similar fashion. You first create a prose.Document by passing in the text content, which invokes the entity parsing. You can then iterate over the resulting Entities, which consist of tuples of the form (Text, Label).\nTake for example the sentence:\nRobyn Rihanna Fenty, born February 20, 1988, is a Barbadian singer, actress, and businesswoman.\nThe resulting entities and labels will be:\n{ \u0026#34;Barbadian\u0026#34;: \u0026#34;GPE\u0026#34;, \u0026#34;Robyn Rihanna Fenty\u0026#34;: \u0026#34;PERSON\u0026#34; } Love you, Riri! Thanks to soup, sentiment, and prose (as well as Ensign and Baleen) we now have:\na live feed of RSS articles a way to parse incoming HTML text into component parts a way to score the sentiment of incoming articles a way to extract entities from those articles What\u0026rsquo;s Next? # So many possibilities! We could create a live alerting system that throws a flag every time a specific entity is mentioned. We could configure those alerts to fire only when the sentiment is below some threshold. Reach out to us at info@rotational.io and let us know what else you\u0026rsquo;d want to make!\nBreaking Free from the Batch # Applied machine learning has come a loooong way in the last ten years. Open source libraries like scikit-learn, TensorFlow, spaCy, and HuggingFace have put ML into the hands of everyday practitioners like us. However, many of us are still struggling to get our models into production.\nAnd if you know how applied machine learning works, you know delays are bad! As new data naturally \u0026ldquo;drifts\u0026rdquo; away from historic data, the training input of our models becomes less and less relevent to the real world problems we\u0026rsquo;re trying to use prediction to solve. Imagine how much more robust your applications would be if they were not only trained on the freshest data, but could alert you to drifts as soon as they happen \u0026ndash; you\u0026rsquo;d be able to react immediately as opposed to a batchwise process where you\u0026rsquo;d be lucky to catch the issue within a day!\nEvent-driven data science is one of the best solutions to the MLOps problem. MLOps often requires us to shoehorn our beautiful models into the existing data flows of our organizations. With a few very special exceptions (we especially love Vowpal Wabbit and Chip Huyen\u0026rsquo;s introduction to streaming for data scientists), ML tools and training teach us to expect our data in batches, but that\u0026rsquo;s not usually how data flows organically through an app or into a database. If you can figure out how to reconfigure your data science flow to more closely match how data travels in your organization, the pain of MLOps can be reduced to almost nil.\nHappy Eventing!\n"},{"id":11,"href":"/sdk/groups/","title":"Consumer Groups","section":"SDKs","content":" Consumer Groups # Consumer groups allow multiple subscribers in different processes to coordinate how they consume events from a topic.\nConsumer Group Identification # All subscribers must specify the same consumer group ID or name in order to join the same group. Consumer groups are stored in Ensign with a 16 byte ID; therefore in order to ensure uniqueness, users have the following options:\nSpecify a 16 byte ID directly (e.g. a UUID or a ULID) Specify a name or an ID of any length that is unique to the project. In the first case, Ensign will not modify the ID at all, guaranteeing its uniqueness. However, in the second case, Ensign will use a murmur3 128 bit hash to ensure that the computed ID is 16 bytes. If the ID is specified it is hashed, otherwise the name of the group is hashed. It is strongly recommended that a name string is used for the hash.\nWhile the murmur3 hash does create the possibility of collisions, this will only happen for consumer groups in the same project (e.g. a consumer group with the same name in a different project will not cause a conflict). Therefore the probability is very low that a collision will occur. However, if you are creating a large number of consumer groups, it is generally better to use a UUID or ULID as the ID.\n"},{"id":12,"href":"/eventing/faq/","title":"FAQ","section":"Eventing 101","content":" What is an event (or data) stream? # You can think of an event stream as a pipeline through which data flows, much like pipes that move water in your home. You can use an event stream to connect data sources that produce or emit data (events) to data sinks that consume or process the data. A data source or producer that emits data can be a database, a data warehouse, a data lake, machines, edge devices or sensors, microservices, or applications. A data sink or consumer can be a web or mobile application, a machine learning model, a custom dashboard, or other downstream microservices, sensors, machines, or devices like wearables.\nWith Ensign, you can set up any number of secure event streams — also called topics — for a project or use case. This allows you to customize how to move data in your application or organization, to where it is most useful. You don’t need specialized skills, tools, or infrastructure. Just an API key and a few lines of code.\nWhat can I use an event stream for? # Event streams are applicable to almost any use case that benefits from the continuous flow of data.\nGenerally speaking, developers can use an event stream to build event-driven applications. Think about how satisfying it is as a customer when you see your data update in real time — whether it\u0026rsquo;s being able to watch your package move closer and closer to being delivered, or your fitness tracker updating throughout your cardio session. These kind of rich, interactive, and personalized experiences depend on an underlying stream of data flowing in to the application. Check out our Ensign resources for developers here.\nData scientists can also find substantial value in event streams; in fact, some of the earliest deployed machine learning apps were event-driven! That’s right — back in the 90’s, email spam filters used Bayesian models to learn on the fly. Imagine how much more robust our models would be if they were not only trained on the freshest data, but could alert you to things like data drift as soon as they happened — you’d be able to react immediately as opposed to a batchwise process where you’d be lucky to catch the issue within a day! Check out our Ensign resources for data scientists here.\nEvent streams can also play a huge role in process automation for data engineers and database administrators. Eventing can streamline ETL processes and enable database maintenance and scaling without the danger of data loss or application downtime. You can even use eventing to provision data more securely, updating event encryption to turn on and off access for downstream processes and users. Check out our Ensign resources for data engineers here.\nWhat is change data capture (CDC)? # Databases are constantly changing, particularly transactional databases. However, databases must also support the underlying consistency models of the applications and organizations they support.\nFor example, when you look at your checking account balance, you expect to get a single amount, even though in reality that amount is continually going up and down (hopefully more up than down :-D). And let\u0026rsquo;s say at the same time you were checking your account balance, your account manager also checks your balance — you would expect to both see the same value, even if you were checking from your phone while on vacation in Tokyo and your account manager is checking from her desk in Nebraska.\nBut it\u0026rsquo;s tough for a database to be able to provide that level of consistency while also providing detailed insights about the up-and-down movement of your account balance. That\u0026rsquo;s where change data capture comes in!\nEnsign\u0026rsquo;s event streams can provide bolt-on change data capture (CDC) by logging all changes to a database over time. In our bank example, an Ensign CDC log could be used for a lot of useful things — from training models to forecast negative balances and flag account holders before they incur fines, to real time alerting to protect customers from fraud.\nWhat does it mean to persist data? # Data persistence is a mechanism for saving data in a way that will protect it from failures like power outages, earthquakes, server crashes, etc.\nThink about how stressful it would be if you were depositing money at an ATM, and the ATM screen shorted out and went black before your bank had a chance to update your account balance in their database! Or what if you were buying concert tickets online, and the ticket sales website crashed after your payment went through but before your Taylor Swift tickets were issued!\nConsidering how important data persistence is to fostering trust in consumer-facing contexts, you might be surprised to learn that most streaming tools don\u0026rsquo;t provide data persistence! Some only save data for a few days before it is discarded, some must be specially configured to save data, and others do not have an ability to save data at all.\nEnsign, on the other hand, persists all data by default — because it\u0026rsquo;s better to be safe than sorry!\nIn what way are my event streams and data secure? # We designed Ensign with security-first principles, including Transparent Data Encryption (TDE), symmetric key cryptography, key rotation, JWT-based authentication and authorization, and Argon2 hashing. All events are encrypted by default, and customers are empowered to control how cryptography occurs for their own data within the Ensign system.\nWhen you set up your Ensign tenant, you\u0026rsquo;re issued server-side global keys. Your API keys are shown to you and only you, once and only once. These keys are stored in a globally replicated key management system and you can revoke them using the management UI. Once you revoke keys, data encrypted with those keys is no longer accessible.\nEnsign never stores raw passwords to access the developer portal. We use the Argon2 key derivation algorithm to store passwords and API key secrets as hashes that add computation and memory requirements for validation.\nFor enterprise clients, we’re working on deploying Ensign in virtual private clouds (VPCs).\nWhat is the cost? # We are working on figuring that out. For now, there is no cost because we want developers and data scientists to experiment and build what they can imagine with eventing or streaming superpowers ;-D\nEventually, we will have to charge to be sustainable, but we do commit to always having a free tier for experimenters and builders.\nWhat can I build? # Glad you asked! Check out some ideas here.\n"},{"id":13,"href":"/system/staging/","title":"Staging","section":"System","content":"Note: This page is for internal Ensign development and will probably not be very useful to Ensign users. The staging environment has the latest code deployed frequently, may introduce breaking changes, and has it\u0026rsquo;s data routinely deleted.\nStaging Environment # Ensign developers can access the staging environment in order to perform testing and development or to QA release candidates before they are deployed.\nTo get started, make sure that you\u0026rsquo;ve created an API Key in the staging environment using the Beacon UI at https://ensign.world. Once you\u0026rsquo;ve obtained those credentials, add the following environment variables so that your script can access the credentials:\n$ENSIGN_CLIENT_ID $ENSIGN_CLIENT_SECRET If you\u0026rsquo;re working on the Go SDK in staging, make sure you have the latest version from the commit rather than the latest tagged version so that your client code is up to date with what is in staging:\n$ go get github.com/rotationalio/go-ensign@main By default the Ensign client connects to the Ensign production environment. To connect to Staging you need to specify the staging endpoints in your credentials:\nclient, err := ensign.New(\u0026amp;ensign.Options{ Endpoint: \u0026#34;staging.ensign.world:443\u0026#34;, ClientID: os.GetEnv(\u0026#34;ENSIGN_CLIENT_ID\u0026#34;), ClientSecret: os.GetEnv(\u0026#34;ENSIGN_CLIENT_SECRET\u0026#34;), AuthURL: \u0026#34;https://auth.ensign.world\u0026#34;, }) If you\u0026rsquo;re feeling extra, you can also use the ensign.ninja:443 endpoint which is an alias for staging.ensign.world:443.\n"},{"id":14,"href":"/system/","title":"System","section":"Ensign Documentation","content":" System # This section of the documentation describes the Ensign system.\n"}]