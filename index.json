[{"categories":null,"content":"At the core of Ensign is the Broker.\nEvent brokers are what differentiate eventing systems (e.g. Kafka, Pulsar, Redpanda, Google PubSub, Ensign) from synchronous messaging systems (e.g. RabbitMQ, Ably, Amazon SQS).\nWhat Does a Broker Do? Brokers are responsible for a lot.\nHere are some of the main responsibilities of an Ensign broker:\nPersisting Data The Ensign Broker persists data written to any topic by the Publisher so that multiple Subscribers can read data from that same topic. Persisting data also means that Ensign has geodistributed backups of data, so it\u0026rsquo;s safe from things like earthquakes, floods, and other catastrophes.\nIn a synchronous messaging system like RabbitMQ, messages are discarded after they are acknowledged, so there is no expectation of persistence whatsoever.\nWhile tools like Kafka, Pulsar, and Google PubSub can be configured by editing YAML files to persist data indefinitely, enabling persistence has two consequences. First, it significantly reduces throughput and increases latency, because it requires the broker to do more work than usual. Enabling persistence by default also increases costs in these other systems, because storing data costs money.\nKeeping Events in the Right Order The Ensign Broker keeps all events in order for every topic, even if there are publishers writing to that topic from two different sides of the Earth. Knowing that your events will always be totally ordered provides a powerful semantic that teams can use to design applications that will always respect the same ordering of events.\nEnsign events are ordered by the Broker using an RLID. An RLID is a totally ordered, 80-byte data structure that encodes both time and a monotonically increasing sequence number using Crockford\u0026rsquo;s base32. It is inspired by ULID and Snowflake IDs.\nTo date, Ensign is the only eventing system in the world that guarantees totally ordered concurrent events within topics.\nRemembering the Offsets The Ensign Broker remembers subscriber offsets. This means that an application reading data from Ensign does not have to maintain any state related to Ensign â€” this is why Ensign is particularly convenient for teams that deploy stateless applications.\nConsider two subscribers connecting to their Ensign Broker, Subscriber A that is connecting for the first time, and Subscriber B that has been dormant/inactive for some period of time. Presumably, Subscriber A and Subscriber B will want to start reading the data from different points in the topic stream; Subscriber A might want to read in all the events from the very beginning, while Subscriber B might prefer to start back from where it left off so that it can recover its function using the minimum needed computation or memory. Ensign\u0026rsquo;s Broker maintains a mapping between topics and their subscribers.\nWhat Makes the Ensign Broker Unique? There are several things that are unique in the implementation of the Ensign Broker that diverge from similar systems. Here are a few differentiators:\nIn Ensign, Data is persisted by default: In most similar systems, persistence is disabled by default, must be activated using configuration/YAML, and will instantly and significantly increase the costs. The Ensign Broker stores the Subscriber offsets: This means Ensign Subscribers can be fully stateless. Consensus is flexible: Similar systems use Raft or Zookeeper for consensus. Ensign is built so that decision-making in the Broker can automatically shift into a hierarchical consensus mode. Hierarchical consensus enables small local consensus groups to function independently when possible, enabling far more geographic scaling than would be possible in any other system. In combination, these differences mean that Ensign fulfills the criteria of both a database and an eventing platform, and is far safer for data (aka more fault tolerant) than any comparable eventing system.\n","description":null,"href":"https://ensign.rotational.dev/system/broker/","section":"system","title":"Event Brokering"},{"categories":null,"content":"Consumer groups allow multiple subscribers in different processes to coordinate how they consume events from a topic.\nConsumer Group Identification All subscribers must specify the same consumer group ID or name in order to join the same group. Consumer groups are stored in Ensign with a 16 byte ID; therefore in order to ensure uniqueness, users have the following options:\nSpecify a 16 byte ID directly (e.g. a UUID or a ULID) Specify a name or an ID of any length that is unique to the project. In the first case, Ensign will not modify the ID at all, guaranteeing its uniqueness. However, in the second case, Ensign will use a murmur3 128 bit hash to ensure that the computed ID is 16 bytes. If the ID is specified it is hashed, otherwise the name of the group is hashed. It is strongly recommended that a name string is used for the hash.\nWhile the murmur3 hash does create the possibility of collisions, this will only happen for consumer groups in the same project (e.g. a consumer group with the same name in a different project will not cause a conflict). Therefore the probability is very low that a collision will occur. However, if you are creating a large number of consumer groups, it is generally better to use a UUID or ULID as the ID.\n","description":null,"href":"https://ensign.rotational.dev/system/groups/","section":"system","title":"Consumer Groups"},{"categories":null,"content":"Note: This page is for internal Ensign development and will probably not be very useful to Ensign users.\nEnsign services are primarily configured using environment variables and will respect dotenv files in the current working directory. The canonical reference of the configuration for an Ensign service is the config package of that service (described below). This documentation enumerates the most important configuration variables, their default values, and any hints or warnings about how to use them.\nRequired Configuration\nIf a configuration parameter does not have a default value that means it is required and must be specified by the user! If the configuration parameter does have a default value then that environment variable does not have to be set.\nEnsign The Ensign node is a replica of the Ensign eventing system. Its environment variables are all prefixed with the ENSIGN_ tag. The primary configuration is as follows:\nEnvVar Type Default Description ENSIGN_MAINTENANCE bool false Set node to maintenance mode, which will respond to requests with Unavailable except for status requests. ENSIGN_LOG_LEVEL string info The verbosity of logging, one of trace, debug, info, warn, error, fatal, or panic. ENSIGN_CONSOLE_LOG bool false If true will print human readable logs instead of JSON logs for machine consumption. ENSIGN_BIND_ADDR string :5356 The address and port the Ensign service will listen on. Monitoring Ensign uses Prometheus for metrics and observability. The Prometheus metrics server is configured as follows:\nEnvVar Type Default Description ENSIGN_MONITORING_ENABLED bool true If true, the Prometheus metrics server is served. ENSIGN_MONITORING_BIND_ADDR string :1205 The address and port the metrics server will listen on. ENSIGN_MONITORING_NODE_ID string Optional - a server name to tag metrics with. Storage The Ensign storage configuration defines on disk where Ensign keeps its data. Configure storage as follows:\nEnvVar Type Default Description ENSIGN_STORAGE_READ_ONLY bool false If true then no writes or deletes will be allowed to the database. ENSIGN_STORAGE_DATA_PATH string The path to a directory on disk where Ensign will store its meta and event data. ENSIGN_STORAGE_TESTING bool false If true then a mock store will be opened rather than a leveldb store (should not be used in production). Note that the Ensign data path must be to a directory. If the directory does not exist, it is created. An error occurs if the path is to a file or the process doesn\u0026rsquo;t have permissions to access the directory. Ensign will open two different data stores in the data path: one for metadata and the other to store event data locally.\nIf the testing flag is set to true, a mock store is created that can be used in unit and integration tests.\nAuthentication Ensign uses Quarterdeck to authenticate and authorize requests. This configuration defines how Ensign accesses public keys for JWT verification and how the authentication interceptor behaves.\nEnvVar Type Default Description ENSIGN_AUTH_KEYS_URL string https://auth.rotational.app/.well-known/jwks.json The path to the public keys used to verify JWT tokens. ENSIGN_AUTH_AUDIENCE string https://ensign.rotational.app The audience to verify inside of JWT tokens. ENSIGN_AUTH_ISSUER string https://auth.rotational.app The issuer to verify inside of JWT tokens. ENSIGN_AUTH_MIN_REFRESH_INTERVAL duration 5m The minimum time to wait before refreshing the JWKS public keys in the validator. Sentry Ensign uses Sentry to assist with error monitoring and performance tracing. Configure Ensign to use Sentry as follows:\nEnvVar Type Default Description ENSIGN_SENTRY_DSN string The DSN for the Sentry project. If not set then Sentry is considered disabled. ENSIGN_SENTRY_SERVER_NAME string Optional - a server name to tag Sentry events with. ENSIGN_SENTRY_ENVIRONMENT string The environment to report (e.g. development, staging, production). Required if Sentry is enabled. ENSIGN_SENTRY_RELEASE string {{version}} Specify the release of Ensign for Sentry tracking. By default this will be the package version. ENSIGN_SENTRY_TRACK_PERFORMANCE bool false Enable performance tracing to Sentry with the specified sample rate. ENSIGN_SENTRY_SAMPLE_RATE float64 0.2 The percentage of transactions to trace (0.0 to 1.0). ENSIGN_SENTRY_DEBUG bool false Set Sentry to debug mode for testing. | Sentry is considered enabled if a DSN is configured. Performance tracing is only enabled if Sentry is enabled and track performance is set to true. If Sentry is enabled, an environment is required, otherwise the configuration will be invalid.\nGenerally speaking, Ensign should enable Sentry for panic reports but should not enable performance tracing as this slows down the server too much. Note also that the sentry.Config object has a field Repanic that should not be set by the user. This field is used to manage panics in chained interceptors.\nTenant The Tenant API powers the user front-end for tenant management and configuration. Its environment variables are all prefixed with the TENANT_ tag. The primary configuration is as follows:\nEnvVar Type Default Description TENANT_MAINTENANCE bool false Sets the server to maintenance mode, which will respond to requests with Unavailable except for status requests. TENANT_BIND_ADDR string :8080 The address and port the Tenant service will listen on. TENANT_MODE string release Sets the Gin mode, one of debug, release, or test. TENANT_LOG_LEVEL string info The verbosity of logging, one of trace, debug, info, warn, error, fatal, or panic. TENANT_CONSOLE_LOG bool false If true will print human readable logs instead of JSON logs for machine consumption. TENANT_ALLOW_ORIGINS string http://localhost:3000 A comma separated list of allowed origins for CORS. Set to \u0026ldquo;*\u0026rdquo; to allow all origins. Authentication Tenant uses Quarterdeck to authenticate and authorize requests. The following configuration defines how Tenant validates JWT tokens passed in from the user that were created by Quarterdeck and secures cookies:\nEnvVar Type Default Description TENANT_AUTH_KEYS_URL string https://auth.rotational.app/.well-known/jwks.json The path to the public keys used to verify JWT tokens. TENANT_AUTH_AUDIENCE string https://rotational.app The audience to verify inside of JWT tokens. TENANT_AUTH_ISSUER string https://auth.rotational.app The issuer to verify inside of JWT tokens. TENANT_AUTH_COOKIE_DOMAIN string rotational.app Defines the domain that is set on cookies including CSRF cookies and token cookies. Database Tenant connects to a replicated trtl database for its data storage; the trtl connection is configured as follows:\nEnvVar Type Default Description TENANT_DATABASE_URL string trtl://localhost:4436 The endpoint of the trtl database including the scheme (usually the k8s trtl service). TENANT_DATABASE_INSECURE bool true Connect to the trtl database without TLS (default true inside of a k8s cluster). TENANT_DATABASE_CERT_PATH string The path to the mTLS certificate of the client to connect to trtl in an authenticated fashion. TENANT_DATABASE_POOL_PATH string The path to an x509 pool file with trusted trtl servers to connect to in. Quarterdeck Tenant connects directly to Quarterdeck in order to send pass-through requests from the user (e.g. for registration, login, etc). This is separate from the authentication configuration used in middleware as this configuration is used to setup a Quarterdeck API client.\nEnvVar Type Default Description TENANT_QUARTERDECK_URL string https://auth.rotational.app The Quarterdeck endpoint to create the API client on. TENANT_QUARTERDECK_WAIT_FOR_READY duration 5m The amount of time to wait for Quarterdeck to come online before exiting with fatal. SendGrid Tenant uses SendGrid to assist with email notifications. Configure Tenant to use SendGrid as follows:\nEnvVar Type Default Description TENANT_SENDGRID_API_KEY string API Key to authenticate to SendGrid with. TENANT_SENDGRID_FROM_EMAIL string ensign@rotational.io The email address in the \u0026ldquo;from\u0026rdquo; field of emails being sent to users. TENANT_SENDGRID_ADMIN_EMAIL string admins@rotational.io The email address to send admin emails to from the server. TENANT_SENDGRID_ENSIGN_LIST_ID string A contact list to add users to if they sign up for notifications. TENANT_SENDGRID_TESTING bool false If in testing mode no emails are actually sent but are stored in a mock email collection. TENANT_SENDGRID_ARCHIVE string If in testing mode, specify a directory to save emails to in order to review emails being generated. SendGrid is considered enabled if the SendGrid API Key is set. The from and admin email addresses are required if SendGrid is enabled.\nIf the Ensign List ID is configured then Tenant will add the contact requesting private beta access to that list, otherwise it will simply add the contact to \u0026ldquo;all contacts\u0026rdquo;.\nSentry Tenant uses Sentry to assist with error monitoring and performance tracing. Configure Tenant to use Sentry as follows:\nEnvVar Type Default Description TENANT_SENTRY_DSN string The DSN for the Sentry project. If not set then Sentry is considered disabled. TENANT_SENTRY_SERVER_NAME string Optional - a server name to tag Sentry events with. TENANT_SENTRY_ENVIRONMENT string The environment to report (e.g. development, staging, production). Required if Sentry is enabled. TENANT_SENTRY_RELEASE string {{version}} Specify the release of Ensign for Sentry tracking. By default this will be the package version. TENANT_SENTRY_TRACK_PERFORMANCE bool false Enable performance tracing to Sentry with the specified sample rate. TENANT_SENTRY_SAMPLE_RATE float64 0.2 The percentage of transactions to trace (0.0 to 1.0). TENANT_SENTRY_DEBUG bool false Set Sentry to debug mode for testing. Sentry is considered enabled if a DSN is configured. Performance tracing is only enabled if Sentry is enabled and track performance is set to true. If Sentry is enabled, an environment is required, otherwise the configuration will be invalid.\nNote also that the sentry.Config object has a field Repanic that should not be set by the user. This field is used to manage panics in chained interceptors.\nQuarterdeck The Quarterdeck API handles authentication and authorization as well as API keys and billing management for the Ensign managed service. Its environment variables are all prefixed with the QUARTERDECK_ tag. The primary configuration is as follows:\nEnvVar Type Default Description QUARTERDECK_MAINTENANCE bool false Sets the server to maintenance mode, which will respond to requests with Unavailable except for status requests. QUARTERDECK_BIND_ADDR string :8088 The address and port the Quarterdeck service will listen on. QUARTERDECK_MODE string release Sets the Gin mode, one of debug, release, or test. QUARTERDECK_LOG_LEVEL string info The verbosity of logging, one of trace, debug, info, warn, error, fatal, or panic. QUARTERDECK_CONSOLE_LOG bool false If true will print human readable logs instead of JSON logs for machine consumption. QUARTERDECK_ALLOW_ORIGINS string http://localhost:3000 A comma separated list of allowed origins for CORS. Set to \u0026ldquo;*\u0026rdquo; to allow all origins. QUARTERDECK_VERIFY_BASE_URL string https://rotational.app/verify The base url to generate verify email links to direct the user to in the email verification path. SendGrid Quarterdeck uses SendGrid to assist with email notifications. Configure Quarterdeck to use SendGrid as follows:\nEnvVar Type Default Description QUARTERDECK_SENDGRID_API_KEY string API Key to authenticate to SendGrid with. QUARTERDECK_SENDGRID_FROM_EMAIL string ensign@rotational.io The email address in the \u0026ldquo;from\u0026rdquo; field of emails being sent to users. QUARTERDECK_SENDGRID_ADMIN_EMAIL string admins@rotational.io The email address to send admin emails to from the server. QUARTERDECK_SENDGRID_ENSIGN_LIST_ID string A contact list to add users to if they sign up for notifications. QUARTERDECK_SENDGRID_TESTING bool false If in testing mode no emails are actually sent but are stored in a mock email collection. QUARTERDECK_SENDGRID_ARCHIVE string If in testing mode, specify a directory to save emails to in order to review emails being generated. SendGrid is considered enabled if the SendGrid API Key is set. The from and admin email addresses are required if SendGrid is enabled.\nIf the Ensign List ID is configured then Quarterdeck will add the contact to that list to ensure they receive marketing emails about Ensign.\nRate Limit In order to prevent brute force attacks on the Quarterdeck system we\u0026rsquo;ve implemented a rate limiting middleware to prevent abuse. Rate limiting is configured as follows:\nEnvVar Type Default Description QUARTERDECK_RATE_LIMIT_PER_SECOND float64 10 The maximum number of allowed requests per second. QUARTERDECK_RATE_LIMIT_BURST int 30 Maximum number of requests that is used to track rate of requests (if zero then all requests will be rejected) QUARTERDECK_RATE_LIMIT_TTL duration 5m How long an IP address is cached for rate limiting purposes. It is strongly recommended that the default configuration is used.\nDatabase EnvVar Type Default Description QUARTERDECK_DATABASE_URL string sqlite3:////data/db/quarterdeck.db The DSN for the sqlite3 database. QUARTERDECK_DATABASE_READ_ONLY bool false If true only read-only transactions are allowed. Quarterdeck uses a Raft replicated Sqlite3 database for authentication. The URI should have the scheme sqlite3:// and then a path to the database. For a relative path, use sqlite3:///path/to/relative.db and for an absolute path use sqlite3:////path/to/absolute.db.\nTokens EnvVar Type Default Description QUARTERDECK_TOKEN_KEYS map[string]string The private keys to load into quarterdeck to issue JWT tokens with. QUARTERDECK_TOKEN_AUDIENCE string https://rotational.app The audience to add to the JWT keys for verification. QUARTERDECK_TOKEN_REFRESH_AUDIENCE string An optional additional audience to add only to refresh tokens. QUARTERDECK_TOKEN_ISSUER string https://auth.rotational.app The issuer to add to the JWT keys for verification. QUARTERDECK_TOKEN_ACCESS_DURATION duration 1h The amount of time that an access token is valid for before it expires. QUARTERDECK_TOKEN_REFRESH_DURATION duration 2h The amount of time that a refresh token is valid for before it expires. QUARTERDECK_TOKEN_REFRESH_OVERLAP duration -15m A negative duration that sets how much time the access and refresh tokens overlap in time validity. To create an environment variable that is a map[string]string use a string in the following form:\nkey1:value1,key2:value2 The token keys should be ULIDs keys (for ordering) and a path value to the key pair to load from disk. Generally speaking there should be two keys - the current key and the most recent previous key, though more keys can be added for verification. Only the most recent key will be used to issue tokens, however. For example, here is a valid key map:\n01GECSDK5WJ7XWASQ0PMH6K41K:/data/keys/01GECSDK5WJ7XWASQ0PMH6K41K.pem,01GECSJGDCDN368D0EENX23C7R:/data/keys/01GECSJGDCDN368D0EENX23C7R.pem Future Feature\nNote that in the future quarterdeck will generate its own keys and will not need them to be set as in the configuration above.\nReporting Ensign has a Daily PLG report that is sent to the Rotational admins for product led growth. This reporting tool is configured as follows:\nEnvVar Type Default Description QUARTERDECK_REPORTING_ENABLE_DAILY_PLG bool true Enables the Daily PLG report scheduler. QUARTERDECK_REPORTING_DOMAIN string \u0026ldquo;rotational.app\u0026rdquo; The domain that the report is being generated for. QUARTERDECK_REPORTING_DASHBOARD_URL string URL to the Grafana dashboard to include in the email. Sentry Quarterdeck uses Sentry to assist with error monitoring and performance tracing. Configure Quarterdeck to use Sentry as follows:\nEnvVar Type Default Description QUARTERDECK_SENTRY_DSN string The DSN for the Sentry project. If not set then Sentry is considered disabled. QUARTERDECK_SENTRY_SERVER_NAME string Optional - a server name to tag Sentry events with. QUARTERDECK_SENTRY_ENVIRONMENT string The environment to report (e.g. development, staging, production). Required if Sentry is enabled. QUARTERDECK_SENTRY_RELEASE string {{version}} Specify the release of Ensign for Sentry tracking. By default this will be the package version. QUARTERDECK_SENTRY_TRACK_PERFORMANCE bool false Enable performance tracing to Sentry with the specified sample rate. QUARTERDECK_SENTRY_SAMPLE_RATE float64 0.2 The percentage of transactions to trace (0.0 to 1.0). QUARTERDECK_SENTRY_DEBUG bool false Set Sentry to debug mode for testing. Sentry is considered enabled if a DSN is configured. Performance tracing is only enabled if Sentry is enabled and track performance is set to true. If Sentry is enabled, an environment is required, otherwise the configuration will be invalid.\nNote also that the sentry.Config object has a field Repanic that should not be set by the user. This field is used to manage panics in chained interceptors.\nBeacon A React app delivers Beacon, the Ensign UI. Its environment variables are all prefixed with the REACT_APP tag. The primary configuration is as follows:\nEnvVar Type Default Description REACT_APP_VERSION_NUMBER string The version number of the application build (set by tags in GitHub actions). REACT_APP_GIT_REVISION string The git revision (short) of the application build (set by tags in GitHub actions). REACT_APP_USE_DASH_LOCALE bool false If true the \u0026ldquo;dash\u0026rdquo; language is included for i18n debugging. API Information Connection information for the backend is specified as follows:\nEnvVar Type Default Description REACT_APP_QUARTERDECK_BASE_URL string https://auth.rotational.app/v1 The endpoint and the version of the API to connect to Quarterdeck on. REACT_APP_TENANT_BASE_URL string https://api.rotational.app/v1 The endpoint and the version of the API to connect to Tenant on. Google Analytics The React app uses Google Analytics to monitor website traffic. Configure the React app to use Google Analytics as follows:\nEnvVar Type Default Description REACT_APP_ANALYTICS_ID string Google Analytics tracking ID for the React App. Sentry The React app uses Sentry to assist with error monitoring and performance tracing. Configure the React app to use Sentry as follows:\nEnvVar Type Default Description REACT_APP_SENTRY_DSN string The DSN for the Sentry project. If not set then Sentry is considered disabled. REACT_APP_SENTRY_ENVIRONMENT string The environment to report (e.g. development, staging, production). Required if Sentry is enabled. REACT_APP_SENTRY_EVENT_ID string Sentry is considered enabled if a DSN is configured. If Sentry is enabled, an environment is strongly suggested, otherwise the NODE_ENV environment will be used.\nDevelopment Keep up to Date!\nIt is essential that we keep this configuration documentation up to date. The devops team uses it to ensure its services are configured correctly. Any time a configuration is changed ensure this documentation is also updated!\nTODO: this section will discuss confire, how to interpret environment variables from the configuration struct, how to test configuration, and how to add and change configuration variables. This section should also discuss dotenv files, docker compose, and all of the places where configuration can be influenced (e.g. GitHub actions for React builds).\n","description":null,"href":"https://ensign.rotational.dev/system/configuration/","section":"system","title":"Configuration"},{"categories":null,"content":"What does event-driven data science even look like??\nIn this tutorial we\u0026rsquo;ll find out! Join along for a tour of implementing an event-driven Natural Language Processing tool that does streaming HTML parsing, entity extraction, and sentiment analysis.\nJust here for the code? Check it out here!\nBack to the Future Some of the earliest deployed machine learning apps were event-driven.\nSpam filtering is an awesome example of a natural use case for online modeling. Each newly flagged spam message was a new training event, an opportunity to update the model in real time. While most machine learning bootcamps teach us to expect data in batches, there are a TON of natural use cases for streaming data science (maybe even more than for offline aka batchwise modeling!).\nAnother great use case for event-driven data science is Natural Language Processing tasks such as:\nnamed entity recognition text classification sentiment analysis In this tutorial, we\u0026rsquo;ll tap into a live data feed and see how to process the text content as it streams in.\nA Whale of a Problem Baleen is a project incubated at Rotational Labs for building experimental corpora for Natural Language Processing\nBaleen works on a schedule; every hour it fetches news articles from public RSS feeds and stores them to Ensign. Baleenâ€™s Ensign Publisher stores each news article as an event in a topic stream called documents. You can think of a topic stream like a database table in a traditional relational database.\nOur app is going to read off of that documents stream using an Ensign Subscriber to perform and report analytics on the text of each article as soon as it was published.\nCreating our Ensign Subscriber We can write a Subscriber to connect to the Baleen documents topic feed in order to tap into the stream of parsed RSS news articles:\nclass BaleenSubscriber: \u0026#34;\u0026#34;\u0026#34; Implementing an event-driven Natural Language Processing tool that does streaming HTML parsing, entity extraction, and sentiment analysis \u0026#34;\u0026#34;\u0026#34; def __init__(self, topic=\u0026#34;documents\u0026#34;, ensign_creds=\u0026#34;\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Initialize the BaleenSubscriber, which will allow a data consumer to subscribe to the topic that the publisher is pushing articles to \u0026#34;\u0026#34;\u0026#34; self.topic = topic self.ensign = Ensign( cred_path=ensign_creds ) self.NER = spacy.load(\u0026#39;en_core_web_sm\u0026#39;) The next step is to add a subscribe method to access the topic stream:\nasync def subscribe(self): \u0026#34;\u0026#34;\u0026#34; Subscribe to the article and parse the events. \u0026#34;\u0026#34;\u0026#34; id = await self.ensign.topic_id(self.topic) async for event in self.ensign.subscribe(id): await self.parse_event(event) And another method to run the subscribe method in a continuous loop:\ndef run(self): \u0026#34;\u0026#34;\u0026#34; Run the subscriber forever. \u0026#34;\u0026#34;\u0026#34; asyncio.run(self.subscribe()) If we were to run the BaleenSubscriber now, e.g. with this if-main block:\nif __name__ == \u0026#34;__main__\u0026#34;: subscriber = BaleenSubscriber(ensign_creds = \u0026#39;secret/ensign_creds.json\u0026#39;) subscriber.run() Note: This code assumes you have defined a JSON file with your Ensign API key credentials at secret/ensign_creds.json, however you can also specify your credentials in the environment \u0026hellip; you\u0026rsquo;d see your terminal run the command and just\u0026hellip; wait!\nDon\u0026rsquo;t worry, that\u0026rsquo;s normal. The job of an Ensign Subscriber is to do exactly that; it will come online and just wait for an upstream Publisher to start sending data.\nOnce it\u0026rsquo;s running, our BaleenSubscriber will wait until the next batch of RSS feeds is available.\nNLP Magic Time Now it\u0026rsquo;s time to write the fun data science parts!\nIn this section, we\u0026rsquo;ll add some functionality for text processing, entity recognition, and sentiment analysis so that these tasks are performed in real time on every new RSS document published to the documents feed.\nWe\u0026rsquo;ll write this as a function called parse_event. The first step is to unmarshal each new document from MessagePack format into json (the Baleen application publishes documents in msgpack because it\u0026rsquo;s more efficient!):\nasync def parse_event(self, event): \u0026#34;\u0026#34;\u0026#34; Decode the msgpack payload, in preparation for applying our NLP \u0026#34;magic\u0026#34; \u0026#34;\u0026#34;\u0026#34; try: data = msgpack.unpackb(event.data) except Exception: print(\u0026#34;Received invalid msgpack data in event payload:\u0026#34;, event.data) await event.nack(Nack.Code.UNKNOWN_TYPE) return # Parse the soup next! Parsing the Beautiful Soup The first step in all real world text processing and modeling projects (well, after ingestion of course ;-D) is parsing. The specific parsing technique has a lot to do with the data; but in this case we\u0026rsquo;re starting with HTML documents, which is what Baleen\u0026rsquo;s Publisher delivers.\nWe\u0026rsquo;ll use the amazing BeautifulSoup library:\nasync def parse_event(self, event): \u0026#34;\u0026#34;\u0026#34; Decode the msgpack payload, in preparation for applying our NLP \u0026#34;magic\u0026#34; \u0026#34;\u0026#34;\u0026#34; try: data = msgpack.unpackb(event.data) except json.JSONDecodeError: print(\u0026#34;Received invalid JSON in event payload:\u0026#34;, event.data) await event.nack(Nack.Code.UNKNOWN_TYPE) return # Parsing the content using BeautifulSoup soup = BeautifulSoup(data[b\u0026#39;content\u0026#39;], \u0026#39;html.parser\u0026#39;) # Finding all the \u0026#39;p\u0026#39; tags in the parsed content paras = soup.find_all(\u0026#39;p\u0026#39;) Now we can iterate over paras to process each paragraph chunk by chunk.\nMore than a Feeling Let\u0026rsquo;s say that we want to do streaming sentiment analysis so that we can gauge the sentiment levels of the documents right away rather than in a batch analysis a month from now, when it may be too late to intervene!\nFor this we\u0026rsquo;ll leverage the sentiment analysis tools implemented in textblob, iterating over the paras we extracted from the HTML in the section above and score the text of each using the pre-trained TextBlob sentiment model.\nWe could look at the sentiment of each paragraph, but for tutorial purposes we\u0026rsquo;ll just take an average sentiment for the overall article:\nasync def handle(self, event): # ... # ... # Finding all the \u0026#39;p\u0026#39; tags in the parsed content paras = soup.find_all(\u0026#39;p\u0026#39;) score = [] # ... for para in paras: text = TextBlob(para.get_text()) score.append(text.sentiment.polarity) Let\u0026rsquo;s add an entity extraction step to our iteration over the paras using the excellent SpaCy NLP library. You first create a spacy.Document by passing in the text content to the pretrained parser (which we previously added to our BaleenSubscriber class with spacy.load('en_core_web_sm')). This invokes the entity parsing, after which you can iterate over the resulting entities (ents), which consist of tuples of the form (text, label).\n# .. # .. ner_dict = {} for para in paras: ner_text = self.NER(str(para.get_text())) for word in ner_text.ents: if word.label_ in ner_dict.keys(): if word.text not in ner_dict[word.label_]: ner_dict[word.label_].append(word.text) else : ner_dict[word.label_] = [word.text] Finally, we\u0026rsquo;ll acknowledge that we\u0026rsquo;ve received the event and print out some feedback to ourselves on the command line so we can see what\u0026rsquo;s happening!\n# ... # ... print(\u0026#34;\\nSentiment Average Score : \u0026#34;, sum(score) / len(score)) print(\u0026#34;\\n------------------------------\\n\u0026#34;) print(\u0026#34;Named Entities : \\n\u0026#34;,json.dumps( ner_dict, sort_keys=True, indent=4, separators=(\u0026#39;,\u0026#39;, \u0026#39;: \u0026#39;) ) ) await event.ack() Now, every time a new article is published, we\u0026rsquo;ll get something like this:\nSentiment Average Score : 0.05073840565119635 ------------------------------ Named Entities : { \u0026#34;CARDINAL\u0026#34;: [ \u0026#34;two\u0026#34;, \u0026#34;one\u0026#34;, \u0026#34;five\u0026#34;, \u0026#34;18\u0026#34;, \u0026#34;2\u0026#34; ], \u0026#34;DATE\u0026#34;: [ \u0026#34;recent months\u0026#34;, \u0026#34;Friday\u0026#34;, \u0026#34;her first day\u0026#34;, \u0026#34;four years\u0026#34;, \u0026#34;March\u0026#34;, \u0026#34;The next month\u0026#34;, \u0026#34;this week\u0026#34;, \u0026#34;Saturday\u0026#34;, \u0026#34;the next two days\u0026#34; ], \u0026#34;FAC\u0026#34;: [ \u0026#34;the Great Hall of the People\u0026#34;, \u0026#34;Tiananmen Square\u0026#34; ], \u0026#34;GPE\u0026#34;: [ \u0026#34;U.S.\u0026#34;, \u0026#34;China\u0026#34;, \u0026#34;the United States\u0026#34;, \u0026#34;Beijing\u0026#34;, \u0026#34;Shanghai\u0026#34;, \u0026#34;The United States\u0026#34;, \u0026#34;Washington\u0026#34;, \u0026#34;Hong Kong\u0026#34;, \u0026#34;Detroit\u0026#34; ], \u0026#34;NORP\u0026#34;: [ \u0026#34;American\u0026#34;, \u0026#34;Chinese\u0026#34;, \u0026#34;Americans\u0026#34; ], \u0026#34;ORDINAL\u0026#34;: [ \u0026#34;first\u0026#34; ], \u0026#34;ORG\u0026#34;: [ \u0026#34;Treasury\u0026#34;, \u0026#34;the Treasury Department\u0026#34;, \u0026#34;the American Chamber of Commerce\u0026#34;, \u0026#34;Boeing\u0026#34;, \u0026#34;Bank of America\u0026#34;, \u0026#34;the Mintz Group\u0026#34;, \u0026#34;Bain \u0026amp; Company\u0026#34;, \u0026#34;TikTok\u0026#34;, \u0026#34;ByteDance\u0026#34;, \u0026#34;the Center for American Studies at\u0026#34;, \u0026#34;Peking University\u0026#34;, \u0026#34;Renmin University\u0026#34;, \u0026#34;The U.S. State Department\u0026#34;, \u0026#34;the Chamber of Commerce\u0026#34;, \u0026#34;the People\\u2019s Bank of China\u0026#34;, \u0026#34;Treasury Department\u0026#34;, \u0026#34;CCTV\u0026#34;, \u0026#34;The Financial Times\u0026#34;, \u0026#34;The Times\u0026#34; ], \u0026#34;PERSON\u0026#34;: [ \u0026#34;Janet Yellen\u0026#34;, \u0026#34;Alan Rappeport\u0026#34;, \u0026#34;Keith Bradsher\u0026#34;, \u0026#34;Janet L. Yellen\u0026#34;, \u0026#34;Yellen\u0026#34;, \u0026#34;Biden\u0026#34;, \u0026#34;Li Qiang\u0026#34;, \u0026#34;Cargill\u0026#34;, \u0026#34;Wang Yong\u0026#34;, \u0026#34;Wang\u0026#34;, \u0026#34;Shi Yinhong\u0026#34;, \u0026#34;Michael Hart\u0026#34;, \u0026#34;Hart\u0026#34;, \u0026#34;Liu He\u0026#34;, \u0026#34;Yi Gang\u0026#34;, \u0026#34;Li\u0026#34;, \u0026#34;Claire Fu\u0026#34;, \u0026#34;Christopher Buckley\u0026#34; ], \u0026#34;TIME\u0026#34;: [ \u0026#34;five hours\u0026#34;, \u0026#34;more than an hour\u0026#34;, \u0026#34;afternoon\u0026#34;, \u0026#34;over an hour\u0026#34; ] } Thanks to BeautifulSoup, TextBlob, SpaCy, and Ensign we now have:\na live feed of RSS articles a way to parse incoming HTML text into component parts a way to score the sentiment of incoming articles a way to extract entities from those articles What\u0026rsquo;s Next? So many possibilities! We could create a live alerting system that throws a flag every time a specific entity is mentioned. We could configure those alerts to fire only when the sentiment is below some threshold.\nWant to try your hand with real time NLP? Check out the Data Playground to look for interesting data sets to experiment with doing event-driven data science!\nReach out to us at info@rotational.io and let us know what else you\u0026rsquo;d want to make!\nBreaking Free from the Batch Applied machine learning has come a loooong way in the last ten years. Open source libraries like scikit-learn, TensorFlow, spaCy, and HuggingFace have put ML into the hands of everyday practitioners like us. However, many of us are still struggling to get our models into production.\nAnd if you know how applied machine learning works, you know delays are bad! As new data naturally \u0026ldquo;drifts\u0026rdquo; away from historic data, the training input of our models becomes less and less relevant to the real world problems we\u0026rsquo;re trying to use prediction to solve. Imagine how much more robust your applications would be if they were not only trained on the freshest data, but could alert you to drifts as soon as they happen \u0026ndash; you\u0026rsquo;d be able to react immediately as opposed to a batchwise process where you\u0026rsquo;d be lucky to catch the issue within a day!\nEvent-driven data science is one of the best solutions to the MLOps problem. MLOps often requires us to shoehorn our beautiful models into the existing data flows of our organizations. With a few very special exceptions (we especially love Vowpal Wabbit and Chip Huyen\u0026rsquo;s introduction to streaming for data scientists), ML tools and training teach us to expect our data in batches, but that\u0026rsquo;s not usually how data flows organically through an app or into a database. If you can figure out how to reconfigure your data science flow to more closely match how data travels in your organization, the pain of MLOps can be reduced to almost nil.\nHappy Eventing!\n","description":null,"href":"https://ensign.rotational.dev/examples/data_scientists/","section":"examples","title":"Ensign for Data Scientists"},{"categories":null,"content":"Oh, The Places Youâ€™ll Go! with Ensign ðŸ˜‰\nEnsign is an eventing platform for developers that dramatically simplifies real-time apps and analytics. Hereâ€™s a list of ideas we dreamed up that are possible to build on Ensign. We grouped them by use case, but it\u0026rsquo;s by no means exhaustive. We hope it gets your creative wheels turning!\nCivic Engagement Active First Responders Real-Time Civic Notices Digital Democracy Moderator-less Message Boards Anti-Human Trafficking Tools Climate Change Environmental, Social, and Governance (ESG) Dashboard Climate Change Monitor Carbon Credit Exchange Weather Change Monitors Customer Experience Live Customer Support \u0026amp; On-Call Management In-Store IoT for Point of Sale (e.g. \u0026ldquo;Buy as you shop\u0026rdquo; inventory) Real-Time Package Tracking Better Online Restaurant Ordering System Next Generation Ad Tracking Developer Tools Application Performance Monitoring (APM) \u0026amp; Alerting Observability Tools Production \u0026amp; Test Environments Synchronization (that don\u0026rsquo;t interfere with process) Spot Instance Price Alerting Tool for Multi-Cloud Enterprise Experience Real-time Anonymization Access Control \u0026amp; Identity Management Employee Performance Management/Human Performance Management Health Integrated Patient Therapy Management Contract Tracing/ Outbreak Modeling Unified Digital Self (for Holistic Nutrition/Health) Industrial Applications Industrial Maintenance \u0026amp; Repairs Synthetic Swarms Machine Learning Applications Time Series Analytics CRDT-Powered Collaborative Jupyter Notebooks Real-Time Entity Resolution (De-Duplication \u0026amp; Canonicalization) Mobility Flight Tracker Public Transport Tracker Advanced Car Maintenance \u0026amp; Diagnostics Social Events Massively Multiplayer Online Live (MMOL) Scavenger Hunts Live Streaming Events (sports, conventions, etc.) Watch Party for Geo-Distributed Friends (virtual events) Fantasy Sports Gambling (as you watch) Social Media Feed Aggregation Multi-channel/Thread Dynamic Group Chat Personalized Multi-Source Newsfeed Supply Chain Advanced Inventory Control \u0026amp; Sales Forecasting Disaster Recovery Supply Chain Dynamics Monitoring Ready to get started?\nWant to brainstorm a use case with us? Let us know at support@rotational.io.\nHappy Eventing!\n","description":null,"href":"https://ensign.rotational.dev/eventing/use_cases/","section":"eventing","title":"Use Cases"},{"categories":null,"content":"EnSQL supports a subset of standard operators that are defined in SQL:1999. Operators include comparison operators and logical/boolean operators.\nComparison Operators Comparison operators are used to evaluate an expression that is composed of a left-side (e.g. a in the examples below), the operator, and the right-side (e.g. b in the examples below). These operators are typically used in the WHERE clause of a query for filtering results returned in an Ensign stream.\nOperator Syntax Description = a = b a is equal to b != a != b a is not equal to b \u0026lt;\u0026gt; a \u0026lt;\u0026gt; b a is not equal to b (alternate) \u0026gt; a \u0026gt; b a is greater than b \u0026gt;= a \u0026gt;= b a is greater than or equal to b \u0026lt; a \u0026lt; b a is less than b \u0026lt;= a \u0026lt;= b a is less than or equal to b like a like 'pattern' The pattern 'pattern' is found in a ilike a ilike 'pattern' Case-insensitive like search Logical/Boolean Operators Logical operators return the result of a Boolean operation on an input expression that is composed of a left-side (e.g. a in the examples below), the operator, and the right-side (e.g. b in the examples below). Both input expressions on the left and ride side must evaluate to a boolean value.\nLogical operators can only be used as a predicate/condition e.g. in the WHERE clause of a SQL statement.\nOperator Syntax Description AND a AND b Both a and b must evaluate to true to be true otherwise false OR a OR b Either a or b must evaluate to true to be true otherwise false The order of precedence of these operators is shown below from highest to lowest:\nAND OR NOTE: When we add the NOT logical operator in the future, it will have the highest precedence.\n","description":null,"href":"https://ensign.rotational.dev/ensql/operators/","section":"ensql","title":"Query Operators"},{"categories":null,"content":"An event-driven architecture (EDA) is a plan for how data will flow through your application. It can be helpful to decompose these architectures into different handlers that are responsible for performing operations on the data (e.g. ingestion, statistical inference, prediction) and routing it between the layers of your application via the topics.\nHandlers usually fall into two categories, \u0026ldquo;Publishers\u0026rdquo; and \u0026ldquo;Subscribers\u0026rdquo;. Publishers are responsible for writing data to topics, while consumers read data from those topics and perform some transformation on it (e.g. feature extraction, normalization, standardization, de-noising, model training). Some layers of your application may include both a publisher and a subscriber, or even multiple subscribers and publishers!\nIn the figure above, we have an architecture for a lightweight Python web-based application that uses raw data from a streaming weather API, trains an online model, predicts the weather for tomorrow, and displays it alongside a timeseries plot of the last two weeks of weather reports.\n","description":"Designing your first EDA","href":"https://ensign.rotational.dev/getting-started/edas/","section":"getting-started","title":"Designing Event-Driven Architectures"},{"categories":null,"content":"Looking for inspiration? Check out some of these public streaming data sources!\nRealtime Source Data API Type SDKs Account Required Limits Finnhub Stock prices, company profiles, company \u0026amp; market news REST, Web Socket Go SDK Yes Unknown CoinCap Cryptocurrency prices across exchanges REST, Web Socket N/A No 200 requests/min Flight Data Vehicle and flight locations Open REST API Python API No 4000 daily credits DC WMATA Bus \u0026amp; train trip updates, alerts, and vehicle positions GTFS protocol buffers N/A Yes No Weather API Weather data REST Python, Go Yes No USGS Earthquake Data Earthquake data (time, location, etc) REST API No No No ","description":null,"href":"https://ensign.rotational.dev/eventing/data_sources/","section":"eventing","title":"Data Sources"},{"categories":null,"content":"We love data engineers â€” it\u0026rsquo;s how a lot of us got our starts in tech. One of the main reasons we made Ensign is to make it easier for you to put your data in motion. We know that a clumsy ETL routine can quickly turn a data lake into a data landfill.\nIn this example we\u0026rsquo;ll see how to move data around with Ensign. We\u0026rsquo;ll be calling a weather API and using PyEnsign as a way to both stream and persist weather updates.\nJust want the code? Check out this repo for the full example.\nETL Design The architecture for this weather ingestor is composed of two components:\nAn Ensign publisher that calls the Weather API and publishes the weather data to a topic. An Ensign subscriber that listens on this topic for weather updates. Prerequisites This tutorial assumes that the following steps have been completed:\nYou have received an Ensign Client ID and Client Secret. Refer to the getting started guide on how to obtain the key. You have received an API key from the Weather API website (it\u0026rsquo;s free!). You have Docker installed and running on your machine. Project Setup First, you will need to set the environment variables for ENSIGN_CLIENT_ID and ENSIGN_CLIENT_SECRET from your API Key. (Need a new key?). You will also need to set your weather API key to some environment variable you can retrieve later.\nexport ENSIGN_CLIENT_ID=\u0026lt;your-client-id\u0026gt; export ENSIGN_CLIENT_SECRET=\u0026lt;your-client-secret\u0026gt; export WEATHER_API_KEY=\u0026lt;your-weather-api-key\u0026gt; Next, let\u0026rsquo;s create a root directory called weather_data for the application.\nmkdir weather_data We will then create two files, one for the publisher that calls the Weather API to get the latest weather data and the other for the subscriber that consumes the weather updates from the topic stream.\ncd weather_data touch publisher.py touch subscriber.py We\u0026rsquo;ll also need a requirements.txt to install the two main dependencies for the project: the Ensign Python SDK (PyEnsign) and the ever-helpful requests library for making HTTP requests to the weather API.\nrequirements.txt\npyensign\u0026gt;=0.8b0 requests==2.31.0 Create the Ensign Publisher Classes in Python are a good way to organize code and create useful abstractions. In publisher.py, we\u0026rsquo;ll create a WeatherPublisher class to publish weather updates.\nimport os import json import asyncio from datetime import datetime import requests from pyensign.events import Event from pyensign.ensign import Ensign class WeatherPublisher: def __init__(self, topic=\u0026#34;current-weather\u0026#34;, location=\u0026#34;Washington, DC\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Create a publisher that publishes weather events for a location to a topic. \u0026#34;\u0026#34;\u0026#34; self.topic = topic self.location = location self.weather_api_key = os.environ.get(\u0026#34;WEATHER_API_KEY\u0026#34;) self.ensign = Ensign() Pro Tip: Calling Ensign() will automatically load your client ID and client secret from the environment\nReceive and Publish Most publishers follow the wait-and-publish pattern. They do a lot of waiting, and then occasionally publish one or more events when something happens (e.g. a timer expires or an asynchronous signal is received). In Python, this usually looks like a coroutine with a loop.\nasync def recv_and_publish(self): \u0026#34;\u0026#34;\u0026#34; Receive weather events and publish them to the topic. \u0026#34;\u0026#34;\u0026#34; # Ensure the topic exists await self.ensign.ensure_topic_exists(self.topic) while True: # Make a request to the weather API response = requests.get(\u0026#34;http://api.weatherapi.com/v1/current.json\u0026#34;, params={ \u0026#34;key\u0026#34;: self.weather_api_key, \u0026#34;q\u0026#34;: self.location, }) try: response.raise_for_status() except requests.exceptions.HTTPError as e: print(\u0026#34;Error fetching weather data: {}\u0026#34;.format(e)) await asyncio.sleep(60) continue # Parse the response and publish the event data = response.json() event = Event(json.dumps(data).encode(\u0026#34;utf-8\u0026#34;), mimetype=\u0026#34;application/json\u0026#34;) await self.ensign.publish(self.topic, event, on_ack=self.print_ack, on_nack=self.print_nack) # Wait 60 seconds in between requests await asyncio.sleep(60) Let\u0026rsquo;s break this down. We first make a call to ensure that the topic exists in the Ensign project that\u0026rsquo;s associated with the API key. This will create the topic if it doesn\u0026rsquo;t already exist. Alternatively, we could create the topic from the project dashboard and skip this step.\n# Ensure the topic exists await self.ensign.ensure_topic_exists(self.topic) Note: The await syntax is necessary because the PyEnsign client is asynchronous. If you\u0026rsquo;re unfamiliar with the asyncio library, read more about that here.\nNext is the loop to query the weather API and create events. We\u0026rsquo;ll also include try/except handling to catch HTTP exceptions. HTTP errors can be anything from running into rate limits to the weather API being deprecated. Ideally we would want to utilize a logging tool here to be able to tell what happened externally, but for right now we\u0026rsquo;ll settle for printing to STDOUT.\nwhile True: # Make a request to the weather API response = requests.get(\u0026#34;http://api.weatherapi.com/v1/current.json\u0026#34;, params={ \u0026#34;key\u0026#34;: self.weather_api_key, \u0026#34;q\u0026#34;: self.location, }) try: response.raise_for_status() except requests.exceptions.HTTPError as e: print(\u0026#34;Error fetching weather data: {}\u0026#34;.format(e)) await asyncio.sleep(60) continue The requests library gives us a dictionary, but Ensign requires event data to be bytes. We could choose any serialization format. For this example we\u0026rsquo;ll use JSON, so we\u0026rsquo;ll create an Event with the encoded JSON data and corresponding mimetype.\n# Parse the response and publish the event data = response.json() event = Event(json.dumps(data).encode(\u0026#34;utf-8\u0026#34;), mimetype=\u0026#34;application/json\u0026#34;) await self.ensign.publish(self.topic, event, on_ack=self.print_ack, on_nack=self.print_nack) The publish API allows us to define asynchronous callbacks to be invoked when an event is acked or nacked by the Ensign service. These are optional, but are useful for debugging and/or logging.\nasync def print_ack(self, ack): ts = datetime.fromtimestamp(ack.committed.seconds + ack.committed.nanos / 1e9) print(\u0026#34;Event committed at {}\u0026#34;.format(ts)) async def print_nack(self, nack): print(\u0026#34;Event was not committed with error {}: {}\u0026#34;.format(nack.code, nack.error)) Finally, we will sleep until the next time we want to call the weather API. How long to sleep is dependent on the use case; some factors to be considered are API rate limits, how often the data source changes, and the desired event granularity (e.g. do we want to capture weather updates every hour? every day? every week?).\nKicking off the publish loop Finally we need a way to run the publisher. The easiest way to run coroutines in Python is asyncio.run, but we will also potentially want some additional configuration.\ndef run_forever(self): \u0026#34;\u0026#34;\u0026#34; Run the publisher forever. \u0026#34;\u0026#34;\u0026#34; asyncio.run(self.recv_and_publish()) if __name__ == \u0026#34;__main__\u0026#34;: # Create a publisher topic = os.environ.get(\u0026#34;WEATHER_TOPIC\u0026#34;) location = os.environ.get(\u0026#34;WEATHER_LOCATION\u0026#34;) publisher = WeatherPublisher(topic=topic, location=location) # Run the publisher forever publisher.run_forever() Create the Ensign Subscriber Next we\u0026rsquo;ll create the subscriber to consume from the weather events topic. Subscribers also usually have a loop; they listen on a topic for events and process events as they come in. In order to process events one at a time, we can use the async for syntax.\nimport os import json import asyncio from pyensign import nack from pyensign.ensign import Ensign class WeatherSubscriber: def __init__(self, topic=\u0026#34;current-weather\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Create a subscriber that subscribes to the weather topic. \u0026#34;\u0026#34;\u0026#34; self.topic = topic self.ensign = Ensign() async def subscribe(self): \u0026#34;\u0026#34;\u0026#34; Subscribe to weather events on the topic. \u0026#34;\u0026#34;\u0026#34; # Ensure the topic exists await self.ensign.ensure_topic_exists(self.topic) async for event in self.ensign.subscribe(self.topic): # Attempt to decode the JSON event try: data = json.loads(event.data.decode(\u0026#34;utf-8\u0026#34;)) except json.JSONDecodeError as e: print(\u0026#34;Error decoding event data: {}\u0026#34;.format(e)) await event.nack(nack.UnknownType) continue print(\u0026#34;Received weather event for {} at {} local time\u0026#34;.format(data[\u0026#34;location\u0026#34;][\u0026#34;name\u0026#34;], data[\u0026#34;location\u0026#34;][\u0026#34;localtime\u0026#34;])) print(\u0026#34;Current temperature is {}Â°F\u0026#34;.format(data[\u0026#34;current\u0026#34;][\u0026#34;temp_f\u0026#34;])) print(\u0026#34;Feels like {}Â°F\u0026#34;.format(data[\u0026#34;current\u0026#34;][\u0026#34;feelslike_f\u0026#34;])) print(\u0026#34;Humidity is {}%\u0026#34;.format(data[\u0026#34;current\u0026#34;][\u0026#34;humidity\u0026#34;])) print(\u0026#34;Wind is {} mph from {}\u0026#34;.format(data[\u0026#34;current\u0026#34;][\u0026#34;wind_mph\u0026#34;], data[\u0026#34;current\u0026#34;][\u0026#34;wind_dir\u0026#34;])) print(\u0026#34;Visibility is {} miles\u0026#34;.format(data[\u0026#34;current\u0026#34;][\u0026#34;vis_miles\u0026#34;])) print(\u0026#34;Precipitation is {} inches\u0026#34;.format(data[\u0026#34;current\u0026#34;][\u0026#34;precip_in\u0026#34;])) # Success! Acknowledge the event await event.ack() def run_forever(self): \u0026#34;\u0026#34;\u0026#34; Run the subscriber forever. \u0026#34;\u0026#34;\u0026#34; asyncio.run(self.subscribe()) if __name__ == \u0026#34;__main__\u0026#34;: topic = os.environ.get(\u0026#34;WEATHER_TOPIC\u0026#34;) subscriber = WeatherSubscriber(topic) subscriber.run_forever() Remember that when publishing an event we wrap the data into the Event object. The subscribe API yields the same Event data type, so we can directly inspect the data payload, metadata, and other attributes on the event. This also allows us to ack an event, indicating to the server that it was successfully processed, or nack an event, indicating to the server that it should be redelivered to another subscriber.\nWhat it means to process an event is different depending on the use case. In this example, we are just interested in viewing the event data, but in other cases a subscriber might perform intermediate processing, train an online model, etc. and publish new events to downstream topics.\nDocker-izing the application Eventually we will probably want to deploy our app somewhere. For now we\u0026rsquo;ll settle for running things locally, but building a docker image is a first step towards running the app in production. The minimal Dockerfile just needs to install the project requirements and the Python source files.\nDockerfile\nFROM python:3.8-slim-buster WORKDIR /app COPY requirements.txt requirements.txt RUN pip3 install -r requirements.txt COPY . . We can test the application locally by creating a docker compose file, making sure to include all of the required environment variables:\ndocker-compose.yaml\nversion: \u0026#39;3\u0026#39; services: publisher: build: . command: python -u publisher.py environment: WEATHER_API_KEY: ${WEATHER_API_KEY} WEATHER_TOPIC: ${WEATHER_TOPIC} WEATHER_LOCATION: ${WEATHER_LOCATION} ENSIGN_CLIENT_ID: ${ENSIGN_CLIENT_ID} ENSIGN_CLIENT_SECRET: ${ENSIGN_CLIENT_SECRET} subscriber: build: . command: python -u subscriber.py environment: WEATHER_TOPIC: ${WEATHER_TOPIC} ENSIGN_CLIENT_ID: ${ENSIGN_CLIENT_ID} ENSIGN_CLIENT_SECRET: ${ENSIGN_CLIENT_SECRET} Let\u0026rsquo;s Gooooooooo We made it to the end! Once you have all of the code in place, ensure that you have the WEATHER_TOPIC and WEATHER_LOCATION environment variables set to your preference.\nexport WEATHER_TOPIC=current-weather export WEATHER_LOCATION=\u0026#34;Washington, DC\u0026#34; Then, use the following commands on the terminal to build and run the application.\ndocker-compose -f docker-compose.yaml build docker-compose -f docker-compose.yaml up You should see the publisher and subscriber running and printing messages to the screen.\nNext Steps Hopefully running this example gives you a general idea on how to build an event-driven application using PyEnsign. You can challenge yourself by creating another subscriber that takes the records produced by the publisher and updates a front end application with the latest weather data.\nEnsign is an event streaming platform, but it\u0026rsquo;s also a database! This means that you don\u0026rsquo;t have to worry about events being deleted, and you can even execute SQL queries over topics using enSQL! Embracing event-driven architectures and data streams gives you more flexibility. You no longer have to deal with all your users hitting a single database. Instead, you can simply publish different data streams from your database to meet all your various end user data requirements. By controlling access to data streams, you can enable developers to build applications directly with production data and help them deploy those applications faster and with less headache.\nLet us know (info@rotational.io) what you end up making with Ensign!\n","description":null,"href":"https://ensign.rotational.dev/examples/data_engineers/","section":"examples","title":"Ensign for Data Engineers"},{"categories":null,"content":"The Python SDK is the quickest way to get started with Ensign. In this example we\u0026rsquo;ll create a simple Python project from scratch to publish and subscribe to Ensign!\nPrerequisites create a free Ensign account and API key download and install Python according to your operating system The Python SDK is currently compatible with Python 3.7, 3.8, 3.9, and 3.10.\nProject Setup Create a project directory and install the official Python SDK using pip.\nmkdir hello-ensign cd hello-ensign pip install pyensign Create a Client Create a main.py file and create the Ensign client in code, which is similar to a database client like PostgreSQL or Mongo. The Python SDK is an asyncio API, which means we need to run the client methods as coroutines. Python\u0026rsquo;s async/await syntax abstracts most of this for us.\nimport asyncio from datetime import datetime from pyensign.events import Event from pyensign.ensign import Ensign async def main(): client = Ensign() status = await client.status() print(status) if __name__ == \u0026#39;__main__\u0026#39;: asyncio.run(main()) The Python SDK requires a Client ID and Client Secret to communicate with Ensign. We recommend specifying them in the environment like so (replace with the values in your API key).\nexport ENSIGN_CLIENT_ID=DbIxBEtIUgNIClnFMDmvoZeMrLxUTJVa export ENSIGN_CLIENT_SECRET=wAfRpXLTiWn7yo7HQzOCwxMvveqiHXoeVJghlSIK2YbMqOMCUiSVRVQOLT0ORrVS If you find yourself having to manage multiple API keys on the same machine, you can also specify a path to a JSON file with your credentials.\nmy_project_key.json\n{ \u0026#34;ClientID\u0026#34;: \u0026#34;DbIxBEtIUgNIClnFMDmvoZeMrLxUTJVa\u0026#34;, \u0026#34;ClientSecret\u0026#34;: \u0026#34;wAfRpXLTiWn7yo7HQzOCwxMvveqiHXoeVJghlSIK2YbMqOMCUiSVRVQOLT0ORrVS\u0026#34; } client = Ensign(cred_path=\u0026#34;my_project_key.json\u0026#34;) Run the code as a Python program.\npython main.py If you see a message like the following, then congratulations! You\u0026rsquo;ve successfully connected to Ensign!\nstatus: 1 version: 0.12.7-beta.21 ([GIT HASH]) uptime: seconds: 130150 nanos: 862300696 Make Some Data Next, we need some data! Generally this is the place where you\u0026rsquo;d connect to your live data source (a database, weather data, etc). But to keep things simple, we\u0026rsquo;ll just create a single event, which starts with a dictionary.\ndata = { \u0026#34;sender\u0026#34;: \u0026#34;Twyla\u0026#34;, \u0026#34;timestamp\u0026#34;: datetime.now(), \u0026#34;message\u0026#34;: \u0026#34;Let\u0026#39;s get this started!\u0026#34; } Next, we will convert our map into an event. Usually, an event contains some data (encoded to bytes), a mimetype which indicates how the data was encoded, and a schema type. The schema type consists of a name and a semantic version string.\nevent = Event( json.dumps(data).encode(\u0026#34;utf-8\u0026#34;), mimetype=\u0026#34;application/json\u0026#34;, schema_name=\u0026#34;Generic\u0026#34;, schema_version=\u0026#34;1.0.0\u0026#34; ) Publish Your Event Now we can publish your event by awaiting the publish method on the Ensign client we created above. You\u0026rsquo;ll also need to pass in a topic name, which will be a string. If you aren\u0026rsquo;t sure what topic to use, you can quickly log into your Ensign dashboard and look it up.\nawait client.publish(\u0026#34;quality-lemon-time\u0026#34;, event) You can publish many events at a time if you want!\nawait client.publish(\u0026#34;quality-lemon-time\u0026#34;, event, event2, event3, event4) Create a Subscriber So now you\u0026rsquo;ve published some events to a topic. We can consume those events with the subscribe method. subscribe works a bit differently than publish. Instead of immediately returning, it yields events to the caller. We can use the async for syntax to process the events as they come in on the stream.\nasync for event in client.subscribe(\u0026#34;quality-lemon-time\u0026#34;): msg = json.loads(event.data) print(msg[\u0026#34;message\u0026#34;]) Try running the program again and see if you can get the message!\npython main.go Let's get this started!\nNext Steps You\u0026rsquo;re already well on your way to building your first event-driven microservice with Ensign!\nIf you\u0026rsquo;re ready to see some more advanced examples with code, check out the End-to-end Examples.\nIf you\u0026rsquo;re looking for more on the basics of event-driven systems, check out Eventing 101.\nHappy eventing!\n","description":null,"href":"https://ensign.rotational.dev/sdk/python/","section":"sdk","title":"Python"},{"categories":null,"content":"What\u0026rsquo;s the best way to name your topics? This is an excellent question!\nRemember, a topic is just like a table in a traditional relational database, so it can be helpful to think about that as you name them.\nEach team may have slightly different naming conventions, and the most important thing when it comes to naming is that your teammates understand the names you use!\nThat said, our favorite technique is to give each data source and type its own topic, for instance:\nuser_logins_plaintext: We might expect this topic to contain data about user logins that could be stored as plaintext, meaning it doesn\u0026rsquo;t contain any publicly identifiable information (PII).\nproduct_reviews_xml: Here the topic likely contains multi-field product reviews that might include text content, numeric ratings (e.g. stars), etc., stored as XML.\nweather_reports_json: With this topic, you could expect the data to be weather reports formatted as JSON data.\nmodel_results_pickle: This topic might contain machine learning models that have been trained and serialized in the Python pickle format.\nAdding the type at the end of the topic name might not always be necessary, but it can be a very helpful way for the Publishers to communicate to the Subscribers what the MIME type of the data will be.\nThink about how it will feel to query the topic later (which you can do with EnSQL!), e.g.\nSELECT DISTINCT reviewer_id FROM product_reviews_xml WHERE timestamp \u0026lt; 2022-05-11T13:00:00-04:00 ","description":"Best practices for naming your topics","href":"https://ensign.rotational.dev/getting-started/topics/","section":"getting-started","title":"Naming Topics"},{"categories":null,"content":"When you\u0026rsquo;re learning a new technology, there always a LOT of new lingo. We\u0026rsquo;ve tried to gather them all together here to help get you started:\napi key \u0026ldquo;API\u0026rdquo; stands for \u0026ldquo;Application Programming Interface\u0026rdquo;, which is a very broad term that refers (super high level) to the ways in which users or other applications can interact with an application.\nSome applications (like Ensign) require permission to interact with, such as a password, token, or key.\nYou can get a free Ensign API key by visiting rotational.io/ensign. Your key will consist of two parts, a ClientID and a ClientSecret. The ClientID uniquely identifies you, and the ClientSecret proves that you have permission to create and access event data. You will need to pass both of these in to create an Ensign client connection.\nasynchronous An asynchronous microservice is one in which requests to a service and the subsequent responses are decoupled and can occur independently of each other.\nThis differs from the synchronous pattern, in which a client request (e.g. a query) is blocked from moving forward until a server response is received. Synchronous microservices can result in cascading failures and compounding latencies in applications.\nAsynchronous microservices can make it a lot easier for teams to develop and deploy components independently.\nAsynchronous microservices require an intermediary service usually known as a broker to hold messages emitted by a publisher that are awaiting retrieval from subscribers.\nbroker An event broker is an intermediary service inside an asynchronous eventing system that stores events sent by publishers until they are received by all subscribers.\nBrokers are also in charge of things like keeping events in the correct order, remembering which subscribers are listening to a topic stream, recording the last message each subscriber retrieved, etc.\nIn Ensign, brokers can save events permanently even after they have been retrieved (to support \u0026ldquo;time travel\u0026rdquo; â€” the ability to retroactively scan through an event stream to support analytics and machine learning).\nclient In order to write or read data from an underlying data system (like a database or event stream), you need a client to connect to the data system and interact with it as needed (such as reading and writing data). This connection often looks something like conn = DBConnection(credentials), and after creating the conn variable, subsequent lines of code can leverage it to perform the kinds of data interactions you wish to make.\nTo establish a client in Ensign you need an API key. By default Ensign will read credentials from the ENSIGN_CLIENT_ID and ENSIGN_CLIENT_SECRET environment variables. If you include these in your bash profile, you can connect to Ensign with the following without having to specify your credentials in code.\npackage main import ( \u0026#34;fmt\u0026#34; ensign \u0026#34;github.com/rotationalio/go-ensign\u0026#34; ) client, err := ensign.New() if err != nil { panic(fmt.Errorf(\u0026#34;could not create client: %s\u0026#34;, err)) } event In an event-driven or microservice architecture, an event is the atomic element of data.\nAn event might look something like a dictionary, which is then wrapped in an object or struct that provides some schema information to help Ensign know how to serialize and deserialize your data.\norder := make(map[string]string) order[\u0026#34;item\u0026#34;] = \u0026#34;large mushroom pizza\u0026#34; order[\u0026#34;customer_id\u0026#34;] = \u0026#34;984445\u0026#34; order[\u0026#34;customer_name\u0026#34;] = \u0026#34;Enson J. Otterton\u0026#34; order[\u0026#34;timestamp\u0026#34;] = time.Now().String() evt := \u0026amp;ensign.Event{ Mimetype: mimetype.ApplicationJSON, Type: \u0026amp;api.Type{ Name: \u0026#34;Generic\u0026#34;, MajorVersion: 1, MinorVersion: 0, PatchVersion: 0, }, } evt.Data, _ = json.Marshal(order) latency Latency can refer to both application-level communication lag (e.g. the time it takes for one part of the code to finish running before moving on to the next part) or to network communication lag (e.g. the time it takes for two remote servers on two different continents to send a single message back and forth).\nLess latency is better, generally speaking.\nIn a microservices context, we can reduce application latency by using asynchronous communications and parallelizing functions so they run faster. Network latency can be reduced by creating more efficient communications between servers (e.g. using more scalable consensus algorithms).\nmicroservice A microservice is a computer application composed of a collection of lightweight services, each of which is responsible for some discrete task.\nMicroservices can be coordinated to communicate via events.\nmime type A MIME (Multipurpose Internet Mail Extensions) type is a label that identifies a type of data, such as CSV, HTML, JSON, or protocol buffer.\nMIME types allow an application to understand how to handle incoming and outgoing data.\norganization An Ensign organization is a collection of users who are working under the same Ensign tenant.\npublisher In an event-driven microservice, a publisher is responsible for emitting events to a topic stream.\nIn Ensign, you can create a publisher once you have established a client. On publish, the client checks to see if it has an open publish stream created and if it doesn\u0026rsquo;t, it opens a stream to the correct Ensign node.\nclient.Publish(yourTopic, yourEvents...) real-time This is a tricky one because real-time can be used to mean different things. In some cases, \u0026ldquo;real-time\u0026rdquo; is used as a synonym for synchronous (i.e. the opposite of asynchronous). However, the term is also used to mean \u0026ldquo;very fast\u0026rdquo; or \u0026ldquo;low latency\u0026rdquo;.\nsdk SDK stands for \u0026ldquo;Software Development Kit\u0026rdquo;. Software applications designed for a technical/developer audience frequently are considerate enough to provide user-facing SDKs in a few languages (e.g. Golang, Python, JavaScript). These SDKs give users a convenient way to interact with the application using a programming language with which they are familiar.\nEnsign currently offers two SDKs: the Golang SDK and a Watermill API-compatible SDK.\nstream An event stream is a flow composed of many, many individual pieces of data called events.\nsubscriber In an event-driven context, a subscriber is a downstream component that is listening for events published by a publisher onto a topic.\ntenant A tenant is a user, group of users, team or company who share computing and/or storage resources.\ntopic In event-driven microservices, a topic is a rough approximation of a traditional relational database table. In a relational DB, a table is a collection of related data fields arrayed as columns and rows. In an eventing context, a topic is a sequence of individual events populated with the same fields (aka schema).\n","description":null,"href":"https://ensign.rotational.dev/eventing/glossary/","section":"eventing","title":"Glossary"},{"categories":null,"content":"The Go SDK is a great choice for developers who wish to integrate Ensign into their existing projects. Or if you\u0026rsquo;re starting from scratch and want to take advantage of Golang\u0026rsquo;s static typing and high-performance multiprocessing, that\u0026rsquo;s cool too.\nIn this example we\u0026rsquo;ll create a simple Go project from scratch to publish and subscribe to Ensign!\nPrerequisites create a free Ensign account and API key download and install Golang according to your operating system Project Setup As with any new Go project, start by creating a directory and module to start adding dependencies to.\nmkdir hello-ensign cd hello-ensign go mod init example.com/hello/ensign The next step is to install the official Go SDK for Ensign.\ngo get github.com/rotationalio/go-ensign Create a Client Create a main.go file and create the Ensign client in code, which is similar to a database client like PostgreSQL or Mongo.\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; ensign \u0026#34;github.com/rotationalio/go-ensign\u0026#34; ) func main() { // Create an Ensign client client, err := ensign.New() if err != nil { panic(fmt.Errorf(\u0026#34;could not create client: %s\u0026#34;, err)) } defer client.Close() // Fetch status from Ensign ctx := context.Background() state, err := client.Status(ctx) if err != nil { panic(fmt.Errorf(\u0026#34;could not get status from Ensign: %s\u0026#34;, err)) } fmt.Println(state.Status, state.Version) } The Go SDK requires a Client ID and Client Secret to communicate with Ensign. We recommend specifying them in the environment like so (replace with the values in your API key).\nexport ENSIGN_CLIENT_ID=DbIxBEtIUgNIClnFMDmvoZeMrLxUTJVa export ENSIGN_CLIENT_SECRET=wAfRpXLTiWn7yo7HQzOCwxMvveqiHXoeVJghlSIK2YbMqOMCUiSVRVQOLT0ORrVS If you find yourself having to manage multiple API keys on the same machine, you can also specify a path to a JSON file with your credentials.\nmy_project_key.json\n{ \u0026#34;ClientID\u0026#34;: \u0026#34;DbIxBEtIUgNIClnFMDmvoZeMrLxUTJVa\u0026#34;, \u0026#34;ClientSecret\u0026#34;: \u0026#34;wAfRpXLTiWn7yo7HQzOCwxMvveqiHXoeVJghlSIK2YbMqOMCUiSVRVQOLT0ORrVS\u0026#34; } client, err := ensign.New(ensign.WithLoadCredentials(\u0026#34;my_project_key.json\u0026#34;)) Run the code as a Go program.\ngo run main.go If you see a message like the following, then congratulations! You\u0026rsquo;ve successfully connected to Ensign!\nHEALTHY 0.12.7-beta.21 ([GIT HASH])\nMake Some Data Next, we need some data! Generally this is the place where you\u0026rsquo;d connect to your live data source (a database, weather data, etc). But to keep things simple, we\u0026rsquo;ll just create a single event, which starts with a map.\ndata := make(map[string]string) data[\u0026#34;sender\u0026#34;] = \u0026#34;Twyla\u0026#34; data[\u0026#34;timestamp\u0026#34;] = time.Now().String() data[\u0026#34;message\u0026#34;] = \u0026#34;Let\u0026#39;s get this started!\u0026#34; Next, we will convert our map into an event, which will allow you to specify the mimetype of the message you intend to send (in this case, we\u0026rsquo;ll say it\u0026rsquo;s JSON), and the event type (which will be a generic event for this example).\nevent := \u0026amp;ensign.Event{ Mimetype: mimetype.ApplicationJSON, Type: \u0026amp;api.Type{ Name: \u0026#34;Generic\u0026#34;, MajorVersion: 1, MinorVersion: 0, PatchVersion: 0, }, } Next, we\u0026rsquo;ll marshal our dictionary into the Data attribute of our sample event\nif event.Data, err = json.Marshal(data); err != nil { panic(\u0026#34;could not marshal data to JSON: \u0026#34; + err.Error()) } Publish Your Event Now we can publish your event by calling the Publish method on the Ensign client we created above. You\u0026rsquo;ll also need to pass in a topic name, which will be a string. If you aren\u0026rsquo;t sure what topic to use, you can quickly log into your Ensign dashboard and look it up.\nclient.Publish(\u0026#34;quality-lemon-time\u0026#34;, event) You can publish many events at a time if you want!\nclient.Publish(\u0026#34;quality-lemon-time\u0026#34;, event, event2, event3, event4) Create a Subscriber So now you\u0026rsquo;ve published some events to a topic. We can consume those events using the Subscribe method. Subscribe works a bit differently than Publish; it returns a Subscription with a Go channel that you can read events from.\nsub, err := client.Subscribe(\u0026#34;quality-lemon-time\u0026#34;) if err != nil { panic(fmt.Errorf(\u0026#34;could not create subscriber: %s\u0026#34;, err)) } for event := range sub.C { var m map[string]string if err := json.Unmarshal(event.Data, \u0026amp;m); err != nil { panic(fmt.Errorf(\u0026#34;failed to unmarshal message: %s\u0026#34;, err)) } fmt.Println(m[\u0026#34;message\u0026#34;]) } Try running the program again and see if you can get the message!\ngo run main.go Let's get this started!\nNext Steps You\u0026rsquo;re already well on your way to building your first event-driven microservice with Ensign!\nIf you\u0026rsquo;re ready to see some more advanced examples with code, check out the End-to-end Examples.\nIf you\u0026rsquo;re looking for more on the basics of event-driven systems, check out Eventing 101.\nHappy eventing!\n","description":null,"href":"https://ensign.rotational.dev/sdk/golang/","section":"sdk","title":"Golang"},{"categories":null,"content":"Note: This page is for internal Ensign development and will probably not be very useful to Ensign users. The staging environment has the latest code deployed frequently, may introduce breaking changes, and has it\u0026rsquo;s data routinely deleted.\nStaging Environment Field Value Alias Beacon UI https://ensign.world Endpoint \u0026quot;staging.ensign.world:443\u0026quot; \u0026quot;ensign.ninja:443\u0026quot; AuthURL \u0026quot;https://auth.ensign.world\u0026quot; Ensign developers can access the staging environment in order to perform testing and development or to QA release candidates before they are deployed.\nTo get started, make sure that you\u0026rsquo;ve created an API Key in the staging environment using the Beacon UI at https://ensign.world. Once you\u0026rsquo;ve obtained those credentials, add the following environment variables so that your script can access the credentials:\n$ENSIGN_CLIENT_ID $ENSIGN_CLIENT_SECRET Go Snippet If you\u0026rsquo;re working on the Go SDK in staging, make sure you have the latest version from the commit rather than the latest tagged version so that your client code is up to date with what is in staging:\n$ go get github.com/rotationalio/go-ensign@main By default the Ensign client connects to the Ensign production environment. To connect to Staging you need to specify the staging endpoints in your credentials:\nclient, err := ensign.New(\u0026amp;ensign.Options{ Endpoint: \u0026#34;staging.ensign.world:443\u0026#34;, ClientID: os.GetEnv(\u0026#34;ENSIGN_CLIENT_ID\u0026#34;), ClientSecret: os.GetEnv(\u0026#34;ENSIGN_CLIENT_SECRET\u0026#34;), AuthURL: \u0026#34;https://auth.ensign.world\u0026#34;, }) If you\u0026rsquo;re feeling extra, you can also use the ensign.ninja:443 endpoint which is an alias for staging.ensign.world:443.\nPython Snippet If you\u0026rsquo;re working with PyEnsign in staging, make sure you have the latest version from the commit rather than the latest tagged version so that your client code is up to date with what is in staging:\n$ pip install git+https://github.com/rotationalio/pyensign.git@develop By default the PyEnsign client connects to the Ensign production environment. To connect to Staging you need to specify the staging endpoints in your credentials:\nimport os from pyensign.ensign import Ensign client = Ensign( endpoint=\u0026#34;staging.ensign.world:443\u0026#34;, client_id=os.getenv(\u0026#34;ENSIGN_CLIENT_ID), client_secret=os.getenv(\u0026#34;ENSIGN_CLIENT_SECRET), auth_url=\u0026#34;https://auth.ensign.world\u0026#34; ) If you\u0026rsquo;re feeling extra, you can also use the ensign.ninja:443 endpoint which is an alias for staging.ensign.world:443.\n","description":null,"href":"https://ensign.rotational.dev/system/staging/","section":"system","title":"Staging"}]