[
    {
        "uri": "/_index.en",
        "content": "---\r\ntitle: \"Ensign Squad, Assemble!\"\r\ndate: 2023-05-16T11:16:03-04:00\r\ndescription: \"Welcome to the Ensign docs!\"\r\n---\r\n\r\nEnsign is a new eventing tool that makes it fast, convenient, and fun to create  event-driven microservices without needing a big team of devOps or platform engineers.\r\n\r\nEnsign is an eventing system distributed in time and space. It stores chronological changes to all objects, which you can query with SQL to get change vectors in addition to static snapshots.\r\n\r\nThis is Ensign's developer documentation; we're glad you're here!\r\n\r\n{{ image src=\"img/ensign-squad.png\" alt=\"A friendly group of cartoon sea otters collaborating on tough problems together\" zoomable=\"true\" }}\r\n\r\n",
        "tags": []
    },
    {
        "uri": "/ensql/_index.en",
        "content": "---\ntitle: \"EnSQL Reference\"\nweight: 40\ndate: 2023-07-29T12:03:04-05:00\n---\n\nEnsign implements a lightweight structured query language called EnSQL that should be familiar to users of relational databases. The twist for Ensign is that EnSQL allows users to query an Ensign topic over specific windows of time to capture and filter events. While the base langauge will be familiar and easy to pick up if you've used ANSI or Postgres SQL in the past, there are a few differences and gotchas that are described in detail in this documentation!\n\n",
        "tags": []
    },
    {
        "uri": "/ensql/operators.en",
        "content": "---\ntitle: \"Query Operators\"\nweight: 20\ndate: 2023-07-29T12:22:42-05:00\n---\n\nEnSQL supports a subset of standard operators that are defined in SQL:1999. Operators include comparison operators and logical/boolean operators.\n\nComparison Operators\n\nComparison operators are used to evaluate an expression that is composed of a left-side (e.g. a in the examples below), the operator, and the right-side (e.g. b in the examples below). These operators are typically used in the WHERE clause of a query for filtering results returned in an Ensign stream.\n\n| Operator |        Syntax       | Description                             |\n|:--------:|:-------------------:|-----------------------------------------|\n|    =   |       a = b       | a is equal to b                     |\n|   !=   |       a != b      | a is not equal to b                 |\n|   `   | a  b            | a is not equal to b` (alternate)     |\n|    `   |       a  b       | a is greater than b`                 |\n|   =   |       a = b      | a is greater than or equal to b     |\n|    <   |       a < b       | a is less than b                    |\n|   <=   |       a <= b      | a is less than or equal to b        |\n| like   | a like 'pattern'  | The pattern 'pattern' is found in a |\n| ilike  | a ilike 'pattern' | Case-insensitive like search          |\n\n Logical/Boolean Operators\n\nLogical operators return the result of a Boolean operation on an input expression that is composed of a left-side (e.g. a in the examples below), the operator, and the right-side (e.g. b in the examples below). Both input expressions on the left and ride side must evaluate to a boolean value.\n\nLogical operators can only be used as a predicate/condition e.g. in the WHERE clause of a SQL statement.\n\n| Operator |   Syntax  | Description                                                              |\n|:--------:|:---------:|--------------------------------------------------------------------------|\n|   AND  | a AND b | Both a and b must evaluate to true to be true otherwise false  |\n|   OR   |  a OR b | Either a or b must evaluate to true to be true otherwise false |\n\nThe order of precedence of these operators is shown below from highest to lowest:\n\nAND\nOR\n\nNOTE: When we add the NOT logical operator in the future, it will have the highest precedence.",
        "tags": []
    },
    {
        "uri": "/ensql/syntax/_index.en",
        "content": "---\ntitle: \"Query Syntax\"\nweight: 10\ndate: 2023-07-29T12:03:04-05:00\n---\n\nEnsign supports querying using standard SELECT statements using the following basic syntax:\n\nSELECT\n\nSELECT is used as to begin an EnSQL statement to query events from a topic.\n\nSyntax is as follows:\n\nSELECT * | { fieldname[ AS fieldalias] }\n\nTo select all fields from a specified topic, you would use:\n\nSELECT * FROM topic\n\nHowever, you can also specify one or more field names with optional aliases to create a projection of the fields that you would like to return. For example, given an event that has the fields id, timestamp, sensor, reading, you can specify only a subset of the fields as follows:\n\nSELECT timestamp, reading FROM topic\n\nYou can also alias fields in the returned result using AS:\n\nSELECT reading AS sensor_reading FROM topic\n\n FROM\n\nSpecifies the topic to use in a SELECT statement optionally with the schema and version of an event type in order to support robust queries.\n\nSyntax is as follows:\n\nSELECT ...\nFROM topicReference\n\nWhere:\n\ntopicReference := topicname[.schemaname[.schema_version]]\n\nSELECT statements are required to have a FROM clause.\n\nWHERE\n\nA WHERE clause specifies conditions that act as a filter to ensure that only events that meet the predictate defined by the WHERE clause are returned.\n\nSyntax is as follows:\n\nWHERE predicate\n\nSuch that:\n\npredicate :=\n    -- expression\n    field operator value |\n\n    -- logical operator\n    expression logical operator expression |\n\n    -- parenthetical grouping\n    (expression logical operator expression) logical operator expression\n\nSee Query Operators for more detail on expression and logical operators.\n\n LIMIT\n\nConstrains the maximum number of events returned by a query.\n\nSyntax:\n\nSELECT ...\nFROM ...\nLIMIT count\n\nWhere count is a numeric value that specifies the maximum number of events returned.\n\nOFFSET\n\nOffset specifies where the query should start in the event stream. While OFFSET is typically used in conjunction with LIMIT, unlike in standard SQL, it is not necessary to specify a LIMIT to use OFFSET since all events are totally ordered.\n\nSyntax:\n\nSELECT ...\nFROM ...\nOFFSET start\n\nWhere start is one of the following:\n\nA numeric integer, specifies how far from the beginning of the stream to start returning events. E.g. OFFSET 100 will start returning events afters skipping the first 99 events.\nAn event ID as a quoted string, specifies a specific event ID to start querying from.\n\n BEFORE | AFTER\n\nComing soon!\n\nThe BEFORE and AFTER statements allows EnSQL users to specify queries relative to time or event ordering. EnSQL does not currently have an ORDER BY clause since data is returned in time ordering; the BEFORE and AFTER statements allow users to traverse events either forward or backward in time.",
        "tags": []
    },
    {
        "uri": "/ensql/syntax/images/SELECT",
        "content": "ï¿½PNG\r\n\u001a\n\u0000\u0000\u0000\rIHDR\u0000\u0000\u0002ï¿½\u0000\u0000\u0002\b\b\u0006\u0000\u0000\u0000iï¿½L\u0000\u0000ï¿½\u0000IDATx^ï¿½ï¿½\u0005ï¿½\u0015U\u001b\u0007ï¿½\r\u000bï¿½ï¿½ï¿½\u0012ï¿½Ý¡\"ï¿½\u0002bï¿½ï¿½ï¿½ï¿½\u0001v\u0007Xï¿½ï¿½ï¿½Xï¿½ï¿½\u001dï¿½(vï¿½g`ï¿½\"ï¿½\u0001\u0012ï¿½ï¿½ï¿½ï¿½wï¿½ï¿½=ï¿½ï¿½wï¿½ï¿½Þ½;sï¿½ï¿½ï¿½ï¿½ï¿½yï¿½\u0003;7fï¿½Ô¼sï¿½ï¿½ï¿½Pï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½]ZZï¿½rï¿½ï¿½ï¿½L)##ceï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½lï¿½ï¿½2\u0013ï¿½ï¿½ï¿½ï¿½(yï¿½x1ï¿½ï¿½2\u0013ï¿½ï¿½ï¿½ï¿½(yï¿½x1ï¿½ï¿½2\u0013ï¿½ï¿½ï¿½ï¿½(yï¿½x1ï¿½ï¿½2\u0013ï¿½ï¿½ï¿½ï¿½(yï¿½x1ï¿½ï¿½2\u0013ï¿½ï¿½ï¿½ï¿½(yï¿½x1ï¿½ï¿½2\u0013ï¿½ï¿½ï¿½ï¿½(yï¿½x1ï¿½ï¿½2\u0013ï¿½ï¿½ï¿½ï¿½(yï¿½x1ï¿½ï¿½2\u0013ï¿½ï¿½ï¿½ï¿½(yï¿½x1.Ö®]kï¿½Xï¿½B.ï¿½\u000bï¿½ï¿½\u0004\"\"\"\"\nï¿½Bï¿½ï¿½ï¿½ï¿½\u0017ï¿½ï¿½ï¿½Ï¶Fï¿½\u0018!\u0017ï¿½\u0005ï¿½Yf\u0002\u0011\u0011\u0011\u0011\u0005ï¿½zï¿½ï¿½Tiï¿½X.ï¿½EW}ï¿½ï¿½Wï¿½ï¿½kï¿½ï¿½ï¿½:ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\u0007kï¿½Æï¿½ï¿½ï¿½\nï¿½,ò€ˆˆï¿½ï¿½ï¿½\u0000ï¿½ï¿½5*-Sï¿½Oï¿½ï¿½\nm\u001d5ï¿½ï¿½kfÍšeï¿½ï¿½ï¿½ZÝºuï¿½ï¿½8ï¿½\bkï¿½ï¿½ï¿½\u0006\r\u001aï¿½\u0003ï¿½ï¿½_]ï¿½ï¿½5eyADDD\u00147KB[\u0003\u0010&ï¿½Ò²Wï¿½ï¿½xZï¿½hï¿½ï¿½ê«¯Zï¿½[ï¿½ï¿½ï¿½ï¿½ï¿½gï¿½}ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\\\u0015!?ï¿½ï¿½ï¿½ï¿½ï¿½pL!\"ï¿½\u001dt`ï¿½x(ï¿½y*ï¿½ï¿½ÒŒP\u0002Fï¿½ï¿½K.ï¿½ï¿½ï¿½yç­ï¿½?ï¿½:ï¿½Ã¬-[ï¿½È·ï¿½ï¿½,/ï¿½ÈŸï¿½~ï¿½È—ï¿½yy\u0003ï¿½\u0010?\u001eJï¿½\u001cï¿½\u00193fï¿½i\u0013ï¿½ï¿½ï¿½5ï¿½|ï¿½~ï¿½ï¿½ï¿½+ï¿½ï¿½\"ï¿½ï¿½*ï¿½ï¿½\u0003\"ï¿½\u000fï¿½_\"ï¿½%v^ï¿½ï¿½ï¿½ï¿½L,Xï¿½ ï¿½ï¿½ï¿½\u0005ï¿½,3ï¿½ï¿½|ï¿½í—ˆ|ï¿½ï¿½ï¿½ï¿½ï¿½x1ï¿½ï¿½2\u0013ï¿½ï¿½7ï¿½~ï¿½È—ï¿½yyï¿½ï¿½\u0017\u0003\u000fï¿½,3ï¿½ï¿½|ï¿½í—ˆ|ï¿½ï¿½ï¿½ï¿½ï¿½x1ï¿½ï¿½2\u0013ï¿½ï¿½7ï¿½~ï¿½È—ï¿½yyï¿½ï¿½\u0017\u0003\u000fï¿½,3ï¿½ï¿½|ï¿½í—ˆ|ï¿½ï¿½ï¿½ï¿½ï¿½x1ï¿½ï¿½2\u0013ï¿½ï¿½7ï¿½~ï¿½È—ï¿½yyï¿½ï¿½\u0017\u0003\u000fï¿½,3ï¿½ï¿½|ï¿½í—ˆ|ï¿½ï¿½ï¿½ï¿½ï¿½x1ï¿½ï¿½2\u0013ï¿½ï¿½7ï¿½~ï¿½È—ï¿½yyï¿½ï¿½\u0017\u0003\u000fï¿½,3ï¿½ï¿½|ï¿½í—ˆ|ï¿½ï¿½ï¿½ï¿½ï¿½x1ï¿½ï¿½2\u0013ï¿½ï¿½7ï¿½~ï¿½È—ï¿½yyXZZï¿½ï¿½ï¿½ï¿½2Jï¿½ï¿½ï¿½ï¿½ï¿½2DD~ï¿½vLDï¿½;ì¼ˆï¿½ï¿½)ï¿½oï¿½ï¿½ï¿½\u000bfÊ”)ï¿½eG`ï¿½dï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Jrï¿½",
        "tags": []
    },
    {
        "uri": "/eventing/_index",
        "content": "---\ntitle: \"Eventing 101\"\nweight: 100\ndate: 2023-05-17T17:01:45-04:00\ndescription: \"Eventing Basics\"\n---\n\nStill getting familiar with eventing basics? You've come to the right place!\n\nIf you're familiar with relational databases and thinking about incorporating some stream elements, here's a helpful guide for how they relate to the traditional model:\n\nYou belong here!\n\nWant to get a better handle on eventing lingo? Check out our eventing glossary.\n\nCurious about how Ensign fits into your work? Check out our frequently asked questions.\n\nMoar Resources\n\nWe get it, it can be a little tricky to shift from thinking about objects and batches to events and streams. It's not just you! Here are some additional resources to help you on your journey:\n\nPrototyping Event-Driven Applications\nThe Eventing Platform Landscape\nGently Down the Stream\nWatermill",
        "tags": []
    },
    {
        "uri": "/eventing/data_sources",
        "content": "---\ntitle: \"Data Sources\"\nweight: 20\ndate: 2023-05-17T17:03:41-04:00\n---\n\nLooking for inspiration? Check out some of these public streaming data sources!\n\n| Realtime Source                                                                       | Data                                                    | API Type              | SDKs       | Account Required | Limits             |\n|:------:|:------:|:------:|:------:|:------:|:------:|\n| Finnhub                                                        | Stock prices, company profiles, company & market news   | REST, Web Socket      | Go SDK     | Yes              | Unknown            |\n| CoinCap                                                   | Cryptocurrency prices across exchanges                  | REST, Web Socket      | N/A        | No               | 200 requests/min   |\n| Flight Data                          | Vehicle and flight locations                            | Open REST API         | Python API | No               | 4000 daily credits |\n| DC WMATA | Bus & train trip updates, alerts, and vehicle positions | GTFS protocol buffers | N/A        | Yes              | No                 |\n| Weather API                                             | Weather data                                            | REST                  | Python, Go | Yes              | No                 |\n| USGS Earthquake Data                   | Earthquake data (time, location, etc)                   | REST API              | No         | No               | No                 |\n\n",
        "tags": []
    },
    {
        "uri": "/eventing/glossary",
        "content": "---\ntitle: \"Glossary\"\nweight: 30\ndate: 2023-05-17T17:03:41-04:00\n---\n\nWhen you're learning a new technology, there always a LOT of new lingo. We've tried to gather them all together here to help get you started:\n\napi key a name=\"api-key\"/a\n\"API\" stands for \"Application Programming Interface\", which is a very broad term that refers (super high level) to the ways in which users or other applications can interact with an application.\n\nSome applications (like Ensign) require permission to interact with, such as a password, token, or key.\n\nYou can get a free Ensign API key by visiting rotational.io/ensign. Your key will consist of two parts, a ClientID and a ClientSecret. The ClientID uniquely identifies you, and the ClientSecret proves that you have permission to create and access event data. You will need to pass both of these in to create an Ensign client connection.\n\n asynchronous a name=\"asynchronous\"/a\nAn asynchronous microservice is one in which requests to a service and the subsequent responses are decoupled and can occur independently of each other.\n\nThis differs from the synchronous pattern, in which a client request (e.g. a query) is blocked from moving forward until a server response is received. Synchronous microservices can result in cascading failures and compounding latencies in applications.\n\nAsynchronous microservices can make it a lot easier for teams to develop and deploy components independently.\n\nAsynchronous microservices require an intermediary service usually known as a broker to hold messages emitted by a publisher that are awaiting retrieval from subscribers.\n\nbroker a name=\"broker\"/a\nAn event broker is an intermediary service inside an asynchronous eventing system that stores events sent by publishers until they are received by all subscribers.\n\nBrokers are also in charge of things like keeping events in the correct order, remembering which subscribers are listening to a topic stream, recording the last message each subscriber retrieved, etc.\n\nIn Ensign, brokers can save events permanently even after they have been retrieved (to support \"time travel\" &mdash; the ability to retroactively scan through an event stream to support analytics and machine learning).\n\n client a name=\"client\"/a\nIn order to write or read data from an underlying data system (like a database or event stream), you need a client to connect to the data system and interact with it as needed (such as reading and writing data). This connection often looks something like conn = DBConnection(credentials), and after creating the conn variable, subsequent lines of code can leverage it to perform the kinds of data interactions you wish to make.\n\nTo establish a client in Ensign you need an API key.\nBy default Ensign will read credentials from the ENSIGNCLIENTID and ENSIGNCLIENTSECRET environment variables. If you include these in your bash profile, you can connect to Ensign with the following without having to specify your credentials in code.\n\npackage main\n\nimport (\n    \"fmt\"\n\n\tensign \"github.com/rotationalio/go-ensign\"\n)\n\nclient, err := ensign.New()\nif err != nil {\n\tpanic(fmt.Errorf(\"could not create client: %s\", err))\n}\n\nevent a name=\"events\"/a\nIn an event-driven or microservice architecture, an event is the atomic element of data.\n\nAn event might look something like a dictionary, which is then wrapped in an object or struct that provides some schema information to help Ensign know how to serialize and deserialize your data.\n\norder := make(map[string]string)\norder[\"item\"] = \"large mushroom pizza\"\norder[\"customer_id\"] = \"984445\"\norder[\"customer_name\"] = \"Enson J. Otterton\"\norder[\"timestamp\"] = time.Now().String()\n\nevt := &ensign.Event{\n    Mimetype: mimetype.ApplicationJSON,\n    Type: &api.Type{\n        Name:    \"Generic\",\n\t\tMajorVersion: 1,\n\t\tMinorVersion: 0,\n\t\tPatchVersion: 0,\n    },\n}\n\nevt.Data, _ = json.Marshal(order)\n\n latency a name=\"latency\"/a\nLatency can refer to both application-level communication lag (e.g. the time it takes for one part of the code to finish running before moving on to the next part) or to network communication lag (e.g. the time it takes for two remote servers on two different continents to send a single message back and forth).\n\nLess latency is better, generally speaking.\n\nIn a microservices context, we can reduce application latency by using asynchronous communications and parallelizing functions so they run faster. Network latency can be reduced by creating more efficient communications between servers (e.g. using more scalable consensus algorithms).\n\nmicroservice a name=\"microservice\"/a\nA microservice is a computer application composed of a collection of lightweight services, each of which is responsible for some discrete task.\n\nMicroservices can be coordinated to communicate via events.\n\n mime type a name=\"mimetype\"/a\nA MIME (Multipurpose Internet Mail Extensions) type is a label that identifies a type of data, such as CSV, HTML, JSON, or protocol buffer.\n\nMIME types allow an application to understand how to handle incoming and outgoing data.\n\norganization\nAn Ensign organization is a collection of users who are working under the same Ensign tenant.\n\n publisher a name=\"publisher\"/a\nIn an event-driven microservice, a publisher is responsible for emitting events to a topic stream.\n\nIn Ensign, you can create a publisher once you have established a client. On publish, the client checks to see if it has an open publish stream created and if it doesn't, it opens a stream to the correct Ensign node.\n\nclient.Publish(yourTopic, yourEvents...)\n\nreal-time\nThis is a tricky one because real-time can be used to mean different things. In some cases, \"real-time\" is used as a synonym for synchronous (i.e. the opposite of asynchronous). However, the term is also used to mean \"very fast\" or \"low latency\".\n\n sdk\nSDK stands for \"Software Development Kit\". Software applications designed for a technical/developer audience frequently are considerate enough to provide user-facing SDKs in a few languages (e.g. Golang, Python, JavaScript). These SDKs give users a convenient way to interact with the application using a programming language with which they are familiar.\n\nEnsign currently offers two SDKs: the Golang SDK and a Watermill API-compatible SDK.\n\nstream\nAn event stream is a flow composed of many, many individual pieces of data called events.\n\n subscriber\nIn an event-driven context, a subscriber is a downstream component that is listening for events published by a publisher onto a topic.\n\ntenant a name=\"tenant\"/a\nA tenant is a user, group of users, team or company who share computing and/or storage resources.\n\n topic\nIn event-driven microservices, a topic is a rough approximation of a traditional relational database table. In a relational DB, a table is a collection of related data fields arrayed as columns and rows. In an eventing context, a topic is a sequence of individual events populated with the same fields (aka schema).\n",
        "tags": []
    },
    {
        "uri": "/eventing/use_cases",
        "content": "---\ntitle: \"Use Cases\"\nweight: 10\ndate: 2023-05-17T17:03:41-04:00\n---\n\nOh, The Places Youâ€™ll Go! with Ensign ðŸ˜‰\n\nEnsign is an eventing platform for developers that dramatically simplifies real-time apps and analytics. Hereâ€™s a list of ideas we dreamed up that are possible to build on Ensign. We grouped them by use case, but it's by no means exhaustive. We hope it gets your creative wheels turning!\n\nCivic Engagement\nActive First Responders\nReal-Time Civic Notices\nDigital Democracy\nModerator-less Message Boards\nAnti-Human Trafficking Tools\n\n Climate Change\nEnvironmental, Social, and Governance (ESG) Dashboard\nClimate Change Monitor\nCarbon Credit Exchange\nWeather Change Monitors\n\nCustomer Experience\nLive Customer Support & On-Call Management\nIn-Store IoT for Point of Sale (e.g. \"Buy as you shop\" inventory)\nReal-Time Package Tracking\nBetter Online Restaurant Ordering System\nNext Generation Ad Tracking\n\n Developer Tools\nApplication Performance Monitoring (APM) & Alerting\nObservability Tools\nProduction & Test Environments Synchronization (that don't interfere with process)\nSpot Instance Price Alerting Tool for Multi-Cloud\n\nEnterprise Experience\nReal-time Anonymization\nAccess Control & Identity Management\nEmployee Performance Management/Human Performance Management\n\n Health\nIntegrated Patient Therapy Management\nContract Tracing/ Outbreak Modeling\nUnified Digital Self (for Holistic Nutrition/Health)\n\nIndustrial Applications\nIndustrial Maintenance & Repairs\nSynthetic Swarms\n\n Machine Learning Applications\nTime Series Analytics\nCRDT-Powered Collaborative Jupyter Notebooks\nReal-Time Entity Resolution (De-Duplication & Canonicalization)\n\nMobility\nFlight Tracker\nPublic Transport Tracker\nAdvanced Car Maintenance & Diagnostics\n\n Social Events\nMassively Multiplayer Online Live (MMOL) Scavenger Hunts\nLive Streaming Events (sports, conventions, etc.)\nWatch Party for Geo-Distributed Friends (virtual events)\nFantasy Sports Gambling (as you watch)\nSocial Media Feed Aggregation\nMulti-channel/Thread Dynamic Group Chat\nPersonalized Multi-Source Newsfeed\n\nSupply Chain\nAdvanced Inventory Control & Sales Forecasting\nDisaster Recovery\nSupply Chain Dynamics Monitoring\n\nReady to get started?\n\nWant to brainstorm a use case with us? Let us know at support@rotational.io.\n\nHappy Eventing!\n",
        "tags": []
    },
    {
        "uri": "/examples/_index",
        "content": "---\ntitle: \"End-to-End Examples\"\nweight: 30\ndate: 2023-05-17T17:03:41-04:00\n---\n\nThis section of the documentation provides end-to-end examples using Ensign to help get you started!\n\nEnsign for Data Engineers: This end-to-end example demonstrates how to retrieve and save weather data using Ensign and Watermill. Create a publisher to call the Weather API and emit the data to a topic stream and use Watermill's router and SQL Pub/Sub to save the data into a PostgreSQL database.\n\nEnsign for Data Scientists: What does event-driven data science look like? In this example, see how to create an Ensign subscriber to Baleen, a live RSS ingestion engine, and use the incoming data to perform streaming HTML parsing, entity extraction, and sentiment analysis.",
        "tags": []
    },
    {
        "uri": "/examples/data_engineers",
        "content": "---\ntitle: \"Ensign for Data Engineers\"\nweight: 20\ndate: 2023-05-17T17:03:41-04:00\n---\n\nWe love data engineers &mdash; it's how a lot of us got our starts in tech. One of the main reasons we made Ensign is to make it easier for you to put your data in motion. We know that a clumsy ETL routine can quickly turn a data lake into a data landfill.\n\nIn this example we'll see how to move data around with Ensign. We'll be calling a weather API and using PyEnsign as a way to both stream and persist weather updates.\n\nJust want the code? Check out this repo for the full example.\n\nETL Design\n\nThe architecture for this weather ingestor is composed of two components:\nAn Ensign publisher that calls the Weather API and publishes the weather data to a topic.\nAn Ensign subscriber that listens on this topic for weather updates.\n\n Prerequisites\n\nThis tutorial assumes that the following steps have been completed:\nYou have received an Ensign Client ID and Client Secret.  Refer to the getting started guide on how to obtain the key.\nYou have received an API key from the Weather API website (it's free!).\nYou have Docker installed and running on your machine.\n\nProject Setup\n\nFirst, you will need to set the environment variables for ENSIGNCLIENTID and ENSIGNCLIENTSECRET from your API Key. (Need a new key?). You will also need to set your weather API key to some environment variable you can retrieve later.\n\nexport ENSIGNCLIENTID=your-client-id\nexport ENSIGNCLIENTSECRET=your-client-secret\nexport WEATHERAPIKEY=your-weather-api-key\n\nNext, let's create a root directory called weather_data for the application.\n\nmkdir weather_data\n\nWe will then create two files, one for the publisher that calls the Weather API to get the latest weather data and the other for the subscriber that consumes the weather updates from the topic stream.\n\ncd weather_data\ntouch publisher.py\ntouch subscriber.py\n\nWe'll also need a requirements.txt to install the two main dependencies for the project: the Ensign Python SDK (PyEnsign) and the ever-helpful requests library for making HTTP requests to the weather API.\n\nrequirements.txt\npyensign=0.8b0\nrequests==2.31.0\n\n Create the Ensign Publisher\n\nClasses in Python are a good way to organize code and create useful abstractions. In publisher.py, we'll create a WeatherPublisher class to publish weather updates.\n\nimport os\nimport json\nimport asyncio\nfrom datetime import datetime\n\nimport requests\nfrom pyensign.events import Event\nfrom pyensign.ensign import Ensign\n\nclass WeatherPublisher:\n    def init(self, topic=\"current-weather\", location=\"Washington, DC\"):\n        \"\"\"\n        Create a publisher that publishes weather events for a location to a topic.\n        \"\"\"\n        self.topic = topic\n        self.location = location\n        self.weatherapikey = os.environ.get(\"WEATHERAPIKEY\")\n        self.ensign = Ensign()\n\nPro Tip: Calling Ensign() will automatically load your client ID and client secret from the environment\n\nReceive and Publish\n\nMost publishers follow the wait-and-publish pattern. They do a lot of waiting, and then occasionally publish one or more events when something happens (e.g. a timer expires or an asynchronous signal is received). In Python, this usually looks like a coroutine with a loop.\n\n    async def recvandpublish(self):\n\t\t\"\"\"\n        Receive weather events and publish them to the topic.\n        \"\"\"\n\n         Ensure the topic exists\n        await self.ensign.ensuretopicexists(self.topic)\n\n        while True:\n            # Make a request to the weather API\n            response = requests.get(\"http://api.weatherapi.com/v1/current.json\", params={\n                \"key\": self.weatherapikey,\n                \"q\": self.location,\n            })\n            try:\n                response.raiseforstatus()\n            except requests.exceptions.HTTPError as e:\n                print(\"Error fetching weather data: {}\".format(e))\n                await asyncio.sleep(60)\n                continue\n\n            # Parse the response and publish the event\n            data = response.json()\n            event = Event(json.dumps(data).encode(\"utf-8\"), mimetype=\"application/json\")\n            await self.ensign.publish(self.topic, event, onack=self.printack, onnack=self.printnack)\n\n            # Wait 60 seconds in between requests\n            await asyncio.sleep(60)\n\nLet's break this down. We first make a call to ensure that the topic exists in the Ensign project that's associated with the API key. This will create the topic if it doesn't already exist. Alternatively, we could create the topic from the project dashboard and skip this step.\n\nEnsure the topic exists\nawait self.ensign.ensuretopicexists(self.topic)\nNote: The await syntax is necessary because the PyEnsign client is asynchronous. If you're unfamiliar with the asyncio library, read more about that here.\n\nNext is the loop to query the weather API and create events. We'll also include try/except handling to catch HTTP exceptions. HTTP errors can be anything from running into rate limits to the weather API being deprecated. Ideally we would want to utilize a logging tool here to be able to tell what happened externally, but for right now we'll settle for printing to STDOUT.\n\nwhile True:\n\t Make a request to the weather API\n\tresponse = requests.get(\"http://api.weatherapi.com/v1/current.json\", params={\n\t\t\"key\": self.weatherapikey,\n\t\t\"q\": self.location,\n\t})\n\ttry:\n\t\tresponse.raiseforstatus()\n\texcept requests.exceptions.HTTPError as e:\n\t\tprint(\"Error fetching weather data: {}\".format(e))\n\t\tawait asyncio.sleep(60)\n\t\tcontinue\n\nThe requests library gives us a dictionary, but Ensign requires event data to be bytes. We could choose any serialization format. For this example we'll use JSON, so we'll create an Event with the encoded JSON data and corresponding mimetype.\n\n\t# Parse the response and publish the event\n\tdata = response.json()\n\tevent = Event(json.dumps(data).encode(\"utf-8\"), mimetype=\"application/json\")\n\tawait self.ensign.publish(self.topic, event, onack=self.printack, onnack=self.printnack)\n\nThe publish API allows us to define asynchronous callbacks to be invoked when an event is acked or nacked by the Ensign service. These are optional, but are useful for debugging and/or logging.\n\nasync def print_ack(self, ack):\n\tts = datetime.fromtimestamp(ack.committed.seconds + ack.committed.nanos / 1e9)\n\tprint(\"Event committed at {}\".format(ts))\n\nasync def print_nack(self, nack):\n\tprint(\"Event was not committed with error {}: {}\".format(nack.code, nack.error))\n\nFinally, we will sleep until the next time we want to call the weather API. How long to sleep is dependent on the use case; some factors to be considered are API rate limits, how often the data source changes, and the desired event granularity (e.g. do we want to capture weather updates every hour? every day? every week?).\n\nKicking off the publish loop\n\nFinally we need a way to run the publisher. The easiest way to run coroutines in Python is asyncio.run, but we will also potentially want some additional configuration.\n\n    def run_forever(self):\n        \"\"\"\n        Run the publisher forever.\n        \"\"\"\n        asyncio.run(self.recvandpublish())\n\nif name == \"main\":\n     Create a publisher\n    topic = os.environ.get(\"WEATHER_TOPIC\")\n    location = os.environ.get(\"WEATHER_LOCATION\")\n    publisher = WeatherPublisher(topic=topic, location=location)\n\n    # Run the publisher forever\n    publisher.run_forever()\n\nCreate the Ensign Subscriber\n\nNext we'll create the subscriber to consume from the weather events topic. Subscribers also usually have a loop; they listen on a topic for events and process events as they come in. In order to process events one at a time, we can use the async for syntax.\n\nimport os\nimport json\nimport asyncio\n\nfrom pyensign import nack\nfrom pyensign.ensign import Ensign\n\nclass WeatherSubscriber:\n    def init(self, topic=\"current-weather\"):\n        \"\"\"\n        Create a subscriber that subscribes to the weather topic.\n        \"\"\"\n        self.topic = topic\n        self.ensign = Ensign()\n\n    async def subscribe(self):\n        \"\"\"\n        Subscribe to weather events on the topic.\n        \"\"\"\n\n         Ensure the topic exists\n        await self.ensign.ensuretopicexists(self.topic)\n\n        async for event in self.ensign.subscribe(self.topic):\n            # Attempt to decode the JSON event\n            try:\n                data = json.loads(event.data.decode(\"utf-8\"))\n            except json.JSONDecodeError as e:\n                print(\"Error decoding event data: {}\".format(e))\n                await event.nack(nack.UnknownType)\n                continue\n\n            print(\"Received weather event for {} at {} local time\".format(data\"location\", data\"location\"))\n            print(\"Current temperature is {}Â°F\".format(data\"current\"))\n            print(\"Feels like {}Â°F\".format(data\"current\"))\n            print(\"Humidity is {}%\".format(data\"current\"))\n            print(\"Wind is {} mph from {}\".format(data\"current\", data\"current\"))\n            print(\"Visibility is {} miles\".format(data\"current\"))\n            print(\"Precipitation is {} inches\".format(data\"current\"))\n\n            # Success! Acknowledge the event\n            await event.ack()\n\n    def run_forever(self):\n        \"\"\"\n        Run the subscriber forever.\n        \"\"\"\n        asyncio.run(self.subscribe())\n\nif name == \"main\":\n    topic = os.environ.get(\"WEATHER_TOPIC\")\n    subscriber = WeatherSubscriber(topic)\n    subscriber.run_forever()\n\nRemember that when publishing an event we wrap the data into the Event object. The subscribe API yields the same Event data type, so we can directly inspect the data payload, metadata, and other attributes on the event. This also allows us to ack an event, indicating to the server that it was successfully processed, or nack an event, indicating to the server that it should be redelivered to another subscriber.\n\nWhat it means to process an event is different depending on the use case. In this example, we are just interested in viewing the event data, but in other cases a subscriber might perform intermediate processing, train an online model, etc. and publish new events to downstream topics.\n\nDocker-izing the application\n\nEventually we will probably want to deploy our app somewhere. For now we'll settle for running things locally, but building a docker image is a first step towards running the app in production. The minimal Dockerfile just needs to install the project requirements and the Python source files.\n\nDockerfile\nFROM python:3.8-slim-buster\n\nWORKDIR /app\n\nCOPY requirements.txt requirements.txt\nRUN pip3 install -r requirements.txt\n\nCOPY . .\n\nWe can test the application locally by creating a docker compose file, making sure to include all of the required environment variables:\n\ndocker-compose.yaml\nversion: '3'\nservices:\n  publisher:\n    build: .\n    command: python -u publisher.py\n    environment:\n      WEATHERAPIKEY: ${WEATHERAPIKEY}\n      WEATHERTOPIC: ${WEATHERTOPIC}\n      WEATHERLOCATION: ${WEATHERLOCATION}\n      ENSIGNCLIENTID: ${ENSIGNCLIENTID}\n      ENSIGNCLIENTSECRET: ${ENSIGNCLIENTSECRET}\n\n  subscriber:\n    build: .\n    command: python -u subscriber.py\n    environment:\n      WEATHERTOPIC: ${WEATHERTOPIC}\n      ENSIGNCLIENTID: ${ENSIGNCLIENTID}\n      ENSIGNCLIENTSECRET: ${ENSIGNCLIENTSECRET}\n\n Let's Gooooooooo\n\nWe made it to the end! Once you have all of the code in place, ensure that you have the WEATHERTOPIC and WEATHERLOCATION environment variables set to your preference.\n\nexport WEATHER_TOPIC=current-weather\nexport WEATHER_LOCATION=\"Washington, DC\"\n\nThen, use the following commands on the terminal to build and run the application.\n\ndocker-compose -f docker-compose.yaml build\ndocker-compose -f docker-compose.yaml up\n\nYou should see the publisher and subcriber running and printing messages to the screen.\n\nNext Steps\n\nHopefully running this example gives you a general idea on how to build an event-driven application using PyEnsign. You can challenge yourself by creating another subscriber that takes the records produced by the publisher and updates a front end application with the latest weather data.\n\nEnsign is an event streaming platform, but it's also a database! This means that you don't have to worry about events being deleted, and you can even execute SQL queries over topics using enSQL! Embracing event-driven architectures and data streams gives you more flexibility. You no longer have to deal with all your users hitting a single database. Instead, you can simply publish different data streams from your database to meet all your various end user data requirements. By controlling access to data streams, you can enable developers to build applications directly with production data and help them deploy those applications faster and with less headache.\n\nLet us know (info@rotational.io) what you end up making with Ensign!",
        "tags": []
    },
    {
        "uri": "/examples/data_scientists",
        "content": "---\ntitle: \"Ensign for Data Scientists\"\nweight: 10\ndate: 2023-05-17T17:03:41-04:00\n---\n\nWhat does event-driven data science even look like??\n\nIn this tutorial we'll find out! Join along for a tour of implementing an event-driven Natural Language Processing tool that does streaming HTML parsing, entity extraction, and sentiment analysis.\n\nJust here for the code? Check it out here!\n\nBack to the Future\n\niframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/NgsiZoHmsBk\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen/iframe\n\nSome of the earliest deployed machine learning apps were event-driven.\n\nSpam filtering is an awesome example of a natural use case for online modeling. Each newly flagged spam message was a new training event, an opportunity to update the model in real time. While most machine learning bootcamps teach us to expect data in batches, there are a TON of natural use cases for streaming data science (maybe even more than for offline aka batchwise modeling!).\n\nAnother great use case for event-driven data science is Natural Language Processing tasks such as:\nnamed entity recognition\ntext classification\nsentiment analysis\n\nIn this tutorial, we'll tap into a live data feed and see how to process the text content as it streams in.\n\n A Whale of a Problem\n\nBaleen is a project incubated at Rotational Labs for building experimental corpora for Natural Language Processing\n\nBaleen works on a schedule; every hour it fetches news articles from public RSS feeds and stores them to Ensign. Baleenâ€™s Ensign Publisher stores each news article as an event in a topic stream called documents. You can think of a topic stream like a database table in a traditional relational database.\n\nOur app is going to read off of that documents stream using an Ensign Subscriber to perform and report analytics on the text of each article as soon as it was published.\n\nCreating our Ensign Subscriber\n\nWe can write a Subscriber to connect to the Baleen documents topic feed in order to tap into the stream of parsed RSS news articles:\n\nclass BaleenSubscriber:\n    \"\"\"\n    Implementing an event-driven Natural Language Processing tool that\n    does streaming HTML parsing, entity extraction, and sentiment analysis\n    \"\"\"\n    def init(self, topic=\"documents\", ensign_creds=\"\"):\n        \"\"\"\n        Initialize the BaleenSubscriber, which will allow a data consumer\n        to subscribe to the topic that the publisher is pushing articles to\n        \"\"\"\n\n        self.topic = topic\n        self.ensign = Ensign(\n            credpath=ensigncreds\n        )\n        self.NER = spacy.load('encoreweb_sm')\n\nThe next step is to add a subscribe method to access the topic stream:\n\n    async def subscribe(self):\n       \"\"\"\n       Subscribe to the article and parse the events.\n       \"\"\"\n       id = await self.ensign.topic_id(self.topic)\n       async for event in self.ensign.subscribe(id):\n           await self.parse_event(event)\n\nAnd another method to run the subscribe method in a continuous loop:\n\n    def run(self):\n        \"\"\"\n        Run the subscriber forever.\n        \"\"\"\n        asyncio.run(self.subscribe())\n\nIf we were to run the BaleenSubscriber now, e.g. with this if-main block:\n\nif name == \"main\":\n    subscriber = BaleenSubscriber(ensigncreds = 'secret/ensigncreds.json')\n    subscriber.run()\nNote: This code assumes you have defined a JSON file with your Ensign API key credentials at secret/ensigncreds.json, however you can also specify your credentials in the environment_\n... you'd see your terminal run the command and just... wait!\n\nDon't worry, that's normal. The job of an Ensign Subscriber is to do exactly that; it will come online and just wait for an upstream Publisher to start sending data.\n\nOnce it's running, our BaleenSubscriber will wait until the next batch of RSS feeds is available.\n\n NLP Magic Time\n\nNow it's time to write the fun data science parts!\n\nIn this section, we'll add some functionality for text processing, entity recognition, and sentiment analysis so that these tasks are performed in real time on every new RSS document published to the documents feed.\n\nWe'll write this as a function called parse_event. The first step is to unmarshal each new document from MessagePack format into json (the Baleen application publishes documents in msgpack because it's more efficient!):\n\n    async def parse_event(self, event):\n        \"\"\"\n        Decode the msgpack payload, in preparation for applying our NLP \"magic\"\n        \"\"\"\n\n        try:\n            data = msgpack.unpackb(event.data)\n        except Exception:\n            print(\"Received invalid msgpack data in event payload:\", event.data)\n            await event.nack(Nack.Code.UNKNOWN_TYPE)\n            return\n\n        # Parse the soup next!\n\nParsing the Beautiful Soup\n\nThe first step in all real world text processing and modeling projects (well, after ingestion of course ;-D) is parsing. The specific parsing technique has a lot to do with the data; but in this case we're starting with HTML documents, which is what Baleen's Publisher delivers.\n\nWe'll use the amazing BeautifulSoup library:\n\n    async def parse_event(self, event):\n        \"\"\"\n        Decode the msgpack payload, in preparation for applying our NLP \"magic\"\n        \"\"\"\n\n        try:\n            data = msgpack.unpackb(event.data)\n        except json.JSONDecodeError:\n            print(\"Received invalid JSON in event payload:\", event.data)\n            await event.nack(Nack.Code.UNKNOWN_TYPE)\n            return\n\n         Parsing the content using BeautifulSoup\n        soup = BeautifulSoup(data[b'content'], 'html.parser')\n\n        # Finding all the 'p' tags in the parsed content\n        paras = soup.find_all('p')\n\nNow we can iterate over paras to process each paragraph chunk by chunk.\n\nMore than a Feeling\n\nLet's say that we want to do streaming sentiment analysis so that we can gauge the sentiment levels of the documents right away rather than in a batch analysis a month from now, when it may be too late to intervene!\n\nFor this we'll leverage the sentiment analysis tools implemented in textblob, iterating over the paras we extracted from the HTML in the section above and score the text of each using the pre-trained TextBlob sentiment model.\n\nWe could look at the sentiment of each paragraph, but for tutorial purposes we'll just take an average sentiment for the overall article:\n\n    async def handle(self, event):\n         ...\n        # ...\n\n        # Finding all the 'p' tags in the parsed content\n        paras = soup.find_all('p')\n        score = []\n        # ...\n        for para in paras:\n            text = TextBlob(para.get_text())\n            score.append(text.sentiment.polarity)\n\nLet's add an entity extraction step to our iteration over the paras using the excellent SpaCy NLP libary. You first create a spacy.Document by passing in the text content to the pretrained parser (which we previously added to our BaleenSubscriber class with spacy.load('encoreweb_sm')). This invokes the entity parsing, after which you can iterate over the resulting entities (ents), which consist of tuples of the form (text, label).\n\n        # ..\n        # ..\n\n        ner_dict = {}\n        for para in paras:\n            nertext = self.NER(str(para.gettext()))\n            for word in ner_text.ents:\n                if word.label_ in ner_dict.keys():\n                    if word.text not in nerdict[word.label]:\n                        nerdict[word.label].append(word.text)\n                else :\n                    nerdict[word.label] = [word.text]\n\nFinally, we'll acknowledge that we've received the event and print out some feedback to ourselves on the command line so we can see what's happening!\n\n        # ...\n        # ...\n\n        print(\"\\nSentiment Average Score : \", sum(score) / len(score))\n        print(\"\\n------------------------------\\n\")\n        print(\"Named Entities : \\n\",json.dumps(\n                ner_dict,\n                sort_keys=True,\n                indent=4,\n                separators=(',', ': ')\n                )\n              )\n        await event.ack()\n\nNow, every time a new article is published, we'll get something like this:\n\nSentiment Average Score :  0.05073840565119635\n\n------------------------------\n\nNamed Entities :\n {\n    \"CARDINAL\": [\n        \"two\",\n        \"one\",\n        \"five\",\n        \"18\",\n        \"2\"\n    ],\n    \"DATE\": [\n        \"recent months\",\n        \"Friday\",\n        \"her first day\",\n        \"four years\",\n        \"March\",\n        \"The next month\",\n        \"this week\",\n        \"Saturday\",\n        \"the next two days\"\n    ],\n    \"FAC\": [\n        \"the Great Hall of the People\",\n        \"Tiananmen Square\"\n    ],\n    \"GPE\": [\n        \"U.S.\",\n        \"China\",\n        \"the United States\",\n        \"Beijing\",\n        \"Shanghai\",\n        \"The United States\",\n        \"Washington\",\n        \"Hong Kong\",\n        \"Detroit\"\n    ],\n    \"NORP\": [\n        \"American\",\n        \"Chinese\",\n        \"Americans\"\n    ],\n    \"ORDINAL\": [\n        \"first\"\n    ],\n    \"ORG\": [\n        \"Treasury\",\n        \"the Treasury Department\",\n        \"the American Chamber of Commerce\",\n        \"Boeing\",\n        \"Bank of America\",\n        \"the Mintz Group\",\n        \"Bain & Company\",\n        \"TikTok\",\n        \"ByteDance\",\n        \"the Center for American Studies at\",\n        \"Peking University\",\n        \"Renmin University\",\n        \"The U.S. State Department\",\n        \"the Chamber of Commerce\",\n        \"the People\\u2019s Bank of China\",\n        \"Treasury Department\",\n        \"CCTV\",\n        \"The Financial Times\",\n        \"The Times\"\n    ],\n    \"PERSON\": [\n        \"Janet Yellen\",\n        \"Alan Rappeport\",\n        \"Keith Bradsher\",\n        \"Janet L. Yellen\",\n        \"Yellen\",\n        \"Biden\",\n        \"Li Qiang\",\n        \"Cargill\",\n        \"Wang Yong\",\n        \"Wang\",\n        \"Shi Yinhong\",\n        \"Michael Hart\",\n        \"Hart\",\n        \"Liu He\",\n        \"Yi Gang\",\n        \"Li\",\n        \"Claire Fu\",\n        \"Christopher Buckley\"\n    ],\n    \"TIME\": [\n        \"five hours\",\n        \"more than an hour\",\n        \"afternoon\",\n        \"over an hour\"\n    ]\n}\n\nThanks to BeautifulSoup, TextBlob, SpaCy, and Ensign we now have:\n\n a live feed of RSS articles\n a way to parse incoming HTML text into component parts\n a way to score the sentiment of incoming articles\n a way to extract entities from those articles\n\nWhat's Next?\n\nSo many possibilities! We could create a live alerting system that throws a flag every time a specific entity is mentioned. We could configure those alerts to fire only when the sentiment is below some threshold.\n\nWant to try your hand with real time NLP? Check out the Data Playground to look for interesting data sets to experiment with doing event-driven data science!\n\nReach out to us at info@rotational.io and let us know what else you'd want to make!\n\n Breaking Free from the Batch\n\nApplied machine learning has come a loooong way in the last ten years. Open source libraries like scikit-learn, TensorFlow, spaCy, and HuggingFace have put ML into the hands of everyday practitioners like us. However, many of us are still struggling to get our models into production.\n\nAnd if you know how applied machine learning works, you know delays are bad! As new data naturally \"drifts\" away from historic data, the training input of our models becomes less and less relevent to the real world problems we're trying to use prediction to solve. Imagine how much more robust your applications would be if they were not only trained on the freshest data, but could alert you to drifts as soon as they happen -- you'd be able to react immediately as opposed to a batchwise process where you'd be lucky to catch the issue within a day!\n\nEvent-driven data science is one of the best solutions to the MLOps problem. MLOps often requires us to shoehorn our beautiful models into the existing data flows of our organizations. With a few very special exceptions (we especially love Vowpal Wabbit and Chip Huyen's introduction to streaming for data scientists), ML tools and training teach us to expect our data in batches, but that's not usually how data flows organically through an app or into a database. If you can figure out how to reconfigure your data science flow to more closely match how data travels in your organization, the pain of MLOps can be reduced to almost nil.\n\nHappy Eventing!\n\n",
        "tags": []
    },
    {
        "uri": "/faq/_index",
        "content": "---\ntitle: \"FAQ\"\nweight: 120\ndate: 2023-08-21T17:09:36-04:00\n---\n\nGot a question you don't see here? Let us know so we can add it! support@rotational.io\n\n!--more--\n\nAccount Creation\n\nQuestions about setting up your account.\n\nHow do I sign up for an Ensign account?\n\nWe are so glad you are interested in checking out Ensign!  You can register here and check out the quickstart guide to get started!\n\nHow much does Ensign cost?\n\nWe are working on figuring that out. For now, there is no cost because we want developers and data scientists to experiment and build what they can imagine with eventing or streaming superpowers ;-D\n\nEventually, we will have to charge to be sustainable, but we do commit to always having a free tier for experimenters and builders.\n\nOur paid tier will be 1-2 orders of magnitude cheaper than other managed eventing platforms (e.g. Confluence, Redpanda. Google Pubsub).\n\nWhat are Ensign \"tenants\"?\n\nThe term \"tenant\" in cloud computing refers to the sharing of compute and storage resources in virtual machines and servers.\n\nWhen you create a new account with a cloud service or SaaS provider like AWS, Auth0, or Ensign, the service provider instantiates a \"tenant\" for you. The provider then allocates some amount of compute and storage resources to your tenant. You and only you (well, and the other people at your organization) may read from, write to, delete, or otherwise access the resources on your tenant.\n\nAn organization can have multiple tenants, e.g. Dev, Prod, Staging.\n\nWhat are Ensign \"organizations\"?\n\nWho should have permission to read from, write to, delete, or otherwise access your topics? In Ensign, you create an organization and invite those people so that you can grant roles and adjust permissions for each teammate. You can have an organization of one, but it's not as fun ðŸ˜‰\n\nWhat do I need to do to get set up?\n\nWith Ensign, you can set up any number of secure event streams &mdash; also called topics &mdash; for a project or use case. This allows you to customize how to move data in your application or organization, to where it is most useful. You donâ€™t need specialized skills, tools, or infrastructure. Just an API key and a few lines of code.\n\nAre my event streams and data secure?\n\nWe designed Ensign with security-first principles, including Transparent Data Encryption (TDE), symmetric key cryptography, key rotation, JWT-based authentication and authorization, and Argon2 hashing. All events are encrypted by default, and customers are empowered to control how cryptography occurs for their own data within the Ensign system.\n\nWhen you set up your Ensign tenant, you're issued server-side global keys. Your API keys are shown to you and only you, once and only once. These keys are stored in a globally replicated key management system and you can revoke them using the management UI. Once you revoke keys, data encrypted with those keys is no longer accessible.\n\nEnsign never stores raw passwords to access the developer portal. We use the Argon2 key derivation algorithm to store passwords and API key secrets as hashes that add computation and memory requirements for validation.\n\nFor enterprise clients, we deploy instances of Ensign in virtual private clouds (VPCs).\n\n What is Ensign?\n\nWhat is Ensign? How does it fit into other database, eventing, and mlops tools I've heard about?\n\nWhat is an event stream (aka data stream)?\n\nYou can think of an event stream as a pipeline through which data flows, much like pipes that move water in your home. You can use an event stream to connect data sources that produce or emit data (events) to data sinks that consume or process the data. A data source or producer that emits data can be a database, a data warehouse, a data lake, machines, edge devices or sensors, microservices, or applications. A data sink or consumer can be a web or mobile application, a machine learning model, a custom dashboard, or other downstream microservices, sensors, machines, or devices like wearables.\n\nWhat can I use an event stream for?\n\nEvent streams are applicable to almost any use case that benefits from the continuous flow of data.\n\nGenerally speaking, developers can use an event stream to build event-driven applications. Think about how satisfying it is as a customer when you see your data update in real time &mdash; whether it's being able to watch your package move closer and closer to being delivered, or your fitness tracker updating throughout your cardio session. These kind of rich, interactive, and personalized experiences depend on an underlying stream of data flowing in to the application.\n\nData scientists can also find substantial value in event streams; in fact, some of the earliest deployed machine learning apps were event-driven! Thatâ€™s right â€” back in the 90â€™s, email spam filters used Bayesian models to learn on the fly. Imagine how much more robust our models would be if they were not only trained on the freshest data, but could alert you to things like data drift as soon as they happened &mdash; youâ€™d be able to react immediately as opposed to a batchwise process where youâ€™d be lucky to catch the issue within a day! Check out our Ensign resources for data scientists here.\n\nEvent streams can also play a huge role in process automation for data engineers and database administrators. Eventing can streamline ETL processes and enable database maintenance and scaling without the danger of data loss or application downtime. You can even use eventing to provision data more securely, updating event encryption to turn on and off access for downstream processes and users. Check out our Ensign resources for data engineers here.\n\nWhat are Ensign \"projects\"?\n\nHow does your organization group its datasets? In traditional storage, related data is stored in a database. An Ensign project defines a collection of datasets related by use case.\n\nWhat are Ensign \"topics\"?\n\nHow does your organization normalize data? Traditional relational database management systems (RDBMS) break data down into tables that describe objects. Ensign topics are the same, but also capture all changes to every object (in chronological order!).\n\nWhat should I name my topics? Are there good naming conventions?\n\nFirst and foremost, consider your teammates when you name your topics.  Come up with names that facilitate communication, not cause confusion.  Check out this handy resource we put together to help you.\n\nWhere does the data go? Where do my topics live?\n\nEnsign is a hosted solution, which means that the data goes to (\"lives in\") virtual servers and machines that are maintained by Rotational Labs.\n\nFor enterprise clients, we happily deploy private instances of Ensign within the cloud provider or virtual private cloud (VPC) of your choice, or even on-prem. We have deployed on Google Cloud, AWS, Azure, and Linode.\n\nWhat does it mean to persist data?\n\nData persistence is a mechanism for saving data in a way that will protect it from failures like power outages, earthquakes, server crashes, etc.\n\nThink about how stressful it would be if you were depositing money at an ATM, and the ATM screen shorted out and went black before your bank had a chance to update your account balance in their database! Or what if you were buying concert tickets online, and the ticket sales website crashed after your payment went through but before your Taylor Swift tickets were issued!\n\nConsidering how important data persistence is to fostering trust in consumer-facing contexts, you might be surprised to learn that most streaming tools don't provide data persistence! Some only save data for a few days before it is discarded, some must be specially configured to save data, and others do not have an ability to save data at all.\n\nEnsign, on the other hand, persists all data by default &mdash; because it's better to be safe than sorry! Read more about how the Ensign Broker (which is responsible for persisting data) works here!\n\nWhat is change data capture (CDC)?\n\nDatabases are constantly changing, particularly transactional databases. However, databases must also support the underlying consistency models of the applications and organizations they support.\n\nFor example, when you look at your checking account balance, you expect to get a single amount, even though in reality that amount is continually going up and down (hopefully more up than down :-D). And let's say at the same time you were checking your account balance, your account manager also checks your balance &mdash; you would expect to both see the same value, even if you were checking from your phone while on vacation in Tokyo and your account manager is checking from her desk in Nebraska.\n\nBut it's tough for a database to be able to provide that level of consistency while also providing detailed insights about the up-and-down movement of your account balance. That's where change data capture comes in!\n\nEnsign's event streams can provide bolt-on change data capture (CDC) by logging all changes to a database over time. In our bank example, an Ensign CDC log could be used for a lot of useful things &mdash; from training models to forecast negative balances and flag account holders before they incur fines, to real time alerting to protect customers from fraud.\n\nArchitecting and Interacting with Ensign\n\nQuestions for builders, analysts, and implementers.\n\nWhat is EnSQL and how do I use it?\n\nEnSQL is basically the same as SQL (because who wants to learn yet another query language??) &mdash; a lightweight structured query language that helps you query your topics just like you would any table in a relational database. With EnSQL, you can also retrieve changes to objects over time rather than being limited to querying current state or version of the data.\n\nWe've made EnSQL's syntax as close to actual SQL as possible; this reference provides an in-depth tutorial on how to use it.\n\nCan you share any data flow diagrams as a starting point?\n\nSure thing!  Check out this link that contains a data flow diagram and a primer on how to design event-driven architectures for building streaming applications.\n\nDoes PyEnsign integrate with leading streaming platforms (e.g. Apache Kafka, AWS Kinesis etc.)?\n\nAbsolutely!  Since Ensign is also a streaming engine, you can use the Pub/Sub framework to interact with Kafka and Kinesis.  For example, you can create an Ensign subscriber that subscribes to a Kafka topic or ingests from a Kinesis stream and does some data transformations.  On the flip side, you can have an Ensign publisher that publishes data to a Kafka topic or Kinesis stream.\n\nEnsign is cloud agnostic and therefore it can be used in any environment with no additional infrastructure requirements. All you have to do is create an Ensign account and pip install pyensign and write your application code.\n\nWhy am I not seeing any events coming to my subscriber?\n\nIn order for an event to be seen by a subscriber, two things need to happen.  First, the ensign server needs to have successfully received the event from the ensign client.  The server communicates the successful receipt by sending an ack message.  Second, the publisher needs to receive this ack message before publishing the event to the topic.  If there is a failure in any of the steps, the event won't be seen by the subscriber.  For this reason, it is recommended that users write code to handle acks and nacks (successful and unsuccessful receipt of messages sent by the ensign server to the ensign client).\n\nAlso, ensure that the subscriber is stood up first to receive messages because it is possible for messages to have already been sent to a topic before the subscriber started listening to them.\n\nFurthermore, make sure that a sleep interval is set on the publisher because it is possible that there can be delays in receiving data from either the ensign server or the data source (e.g. an API).  If the sleep interval is not set, it is possible that the publisher terminates before an acknowledgement from the ensign server or before receiving data from the data source. In such cases, the event does not get added to the topic and so the subscriber will not see it.\n\nHow do you deploy models?\n\nIn a streaming application, you can deploy a model by publishing it to a model topic from which a subscriber can retrieve the model and use it for generating predictions.\n\nFor example, we can create a publisher that will periodically send requests to an API to receive data to train a classification model.  Once we are satisfied with the model's performance and choose to deploy, we can pickle the model and the performance metrics and then publish the event to a model topic.  We can then create a subscriber on the other end that is listening for new messages on this topic as well as a scoredata topic that sends new data that will be used to generate predictions.  Once the subscriber receives the message, it will extract the model from the event.  Now, it is ready to use this model to generate productions on data it receives from the scoredata topic.\n\nThe added benefit of this architecture is that the model topic serves as a model registry that can be used to version control models and because we are storing metrics, it is easy to evaluate different model versions.\n\nCheck out this repository for an implementation of this design pattern.\n\nI'm curious about using Ensign to version models using topics. How does this compare to using something like Artifactory or H20.ai?\n\nIf you already use Ensign, you eliminate the need for a different tool, and you don't need to spend additional time learning and configuring another component in your environment. If your data is in Ensign, there will also be reduced latency because the model and the data are on the same cluster.\n\nIf you already use and love Artifactory, H20.ai, or another model registry, we love that for you; keeping using it!\n\nWhat are Ensign's limits?\n\nEnsign is a streaming engine, and therefore, events cannot be larger than 5 megabytes.  If you have data that is larger, you can compress it and/or break it down before publishing and provide a way for the subscriber on the other end to put it together for processing.  If you are attempting to publish a model that is simply too large to be streamed, then one option is to store it on AWS S3 or another storage solution and include the link to the location in the event that is published to the model topic.  A similar strategy can be employed for other types of data.\n\nWhat are protocol buffers and why does Ensign use them?\n\nEventing is all about machines talking to each other, and that needs to happen as efficiently as possible. Binary serialization is an excellent way to enable message transmission to be as fast as possible.\n\nProtocol buffers (aka gRPC) is a free and open-source cross-platform data format used to serialize structured data. Ensign uses protobuf to help the machines in our clusters quickly send and receive messages to and from each another, so that your publishers and subscribers can publish and consume your events as quickly as possible. You can read a bit more about how we use protobuf at the bottom of the SDK page.\n\n What/Who is Ensign for?\n\nWho is using Ensign and what are they using it for?\n\nWhat are some good use cases for Ensign?\n\nEnsign can be used for many different types of real-time applications.  We put together this guide to help you come up with ideas for your next project.\n\nWhat can I build - how do I get started?\n\nGlad you asked! Check out some ideas here.\n\nIn what agencies or companies is this kind of work relevant? Where can I work if I want to do projects like this?\n\nEnsign serves a broad set of use cases, some of which we have outlined in this guide.\n\nAs Faraz Rahman says:\n\n All data is inherently streaming in nature. The only real difference is how we decide to conceptualize that data â€” as what it really is (a stream of changes) or as what it looks like right now (a snapshot).\n\nSuffice to say, the data flowing through most workplaces is already happening in a stream &mdash; which means it's pretty likely that being able to do preprocessing, analytics and modeling on streams of data is relevant to your industry or organization, whatever that industry or organization happens to be.\n\n",
        "tags": []
    },
    {
        "uri": "/getting-started/_index",
        "content": "---\ntitle: \"Quickstart\"\nweight: 5\ndate: 2023-05-17T17:03:41-04:00\ndescription: \"Getting Started with Ensign\"\n---\n\nThe first step is to get an Ensign API key by visiting the sign-up page. Similar to getting a developer API key for Youtube or Data.gov, you will need an API key to use Ensign and to follow along with the rest of this Quickstart guide.\n\na name=\"ensign-keys\"/a\nEnsign API Keys\n\nAn API key consists of two parts, a ClientID and a ClientSecret. The ClientID uniquely identifies a project, and the ClientSecret allows you to create and access event data within that project.\n\n| API Key Component Name | Length | Characters | Example |\n|:------:|:------:|:------:|:------:|\n| ClientID          | 32     | alphabetic (no digits) | DbIxBEtIUgNIClnFMDmvoZeMrLxUTJVa                                 |\n| ClientSecret      | 64     | alphanumeric           | wAfRpXLTiWn7yo7HQzOCwxMvveqiHXoeVJghlSIK2YbMqOMCUiSVRVQOLT0ORrVS |\n\nTogether, the ClientID and ClientSecret provide access to a project. They enable you to create Ensign topics, publishers, and subscribers, which will be the building blocks of your microservice! Anybody with the ClientID and ClientSecret has access to your project, so these values should be kept private and not shared.\n\n SDKs\n\nThe SDKs are where you use your API key and allow you to integrate Ensign into your projects. There are currently two SDKs supported.\n\nPyEnsign is the official SDK for Python\ngo-ensign is the official SDK for Golang\n\nAuthentication\n\nUsing the SDKs requires authenticating with your API key. By default the SDKs will read the Client ID and Client Secret from your environment.\n\nexport ENSIGNCLIENTID=DbIxBEtIUgNIClnFMDmvoZeMrLxUTJVa\nexport ENSIGNCLIENTSECRET=wAfRpXLTiWn7yo7HQzOCwxMvveqiHXoeVJghlSIK2YbMqOMCUiSVRVQOLT0ORrVS\n\n Publish and Subscribe\n\nThe most common use of the SDKs is to publish events to topics, and subscribe to events from somewhere else. Topics can be created from your dashboard or directly from the SDKs. See the SDK guides for a quick example of how to get started.\n\nPython SDK\nGo SDK\n",
        "tags": []
    },
    {
        "uri": "/getting-started/edas",
        "content": "---\ntitle: \"Designing Event-Driven Architectures\"\nweight: 20\ndate: 2023-05-23T10:51:25-04:00\ndescription: \"Designing your first EDA\"\n---\n\nAn event-driven architecture (EDA) is a plan for how data will flow through your application. It can be helpful to decompose these architectures into different handlers that are responsible for performing operations on the data (e.g. ingestion, statistical inference, prediction) and routing it between the layers of your application via the topics.\n\n{{ image src=\"img/sample-eda.png\" alt=\"Publishers and subscribers route data between the layers of a sample application from ingestion to analytics to the Web UI.\" zoomable=\"true\" }}\n\nHandlers usually fall into two categories, \"Publishers\" and \"Subscribers\". Publishers are responsible for writing data to topics, while consumers read data from those topics and perform some transformation on it (e.g. feature extraction, normalization, standardization, de-noising, model training). Some layers of your application may include both a publisher and a subscriber, or even multiple subscribers and publishers!\n\nIn the figure above, we have an architecture for a lightweight Python web-based application that uses raw data from a streaming weather API, trains an online model, predicts the weather for tomorrow, and displays it alongside a timeseries plot of the last two weeks of weather reports.",
        "tags": []
    },
    {
        "uri": "/getting-started/topics",
        "content": "---\ntitle: \"Naming Topics\"\nweight: 30\ndate: 2023-05-23T11:17:31-04:00\ndescription: \"Best practices for naming your topics\"\n---\n\nWhat's the best way to name your topics? This is an excellent question!\n\nRemember, a topic is just like a table in a traditional relational database, so it can be helpful to think about that as you name them.\n\nEach team may have slightly different naming conventions, and the most important thing when it comes to naming is that your teammates understand the names you use!\n\nThat said, our favorite technique is to give each data source and type its own topic, for instance:\n\nuser_logins_plaintext:\nWe might expect this topic to contain data about user logins that could be stored as plaintext, meaning it doesn't contain any publicly identifiable information (PII).\n\nproduct_reviews_xml:\nHere the topic likely contains multi-field product reviews that might include text content, numeric ratings (e.g. stars), etc., stored as XML.\n\nweather_reports_json:\nWith this topic, you could expect the data to be weather reports formatted as JSON data.\n\nmodel_results_pickle:\nThis topic might contain machine learning models that have been trained and serialized in the Python pickle format.\n\nAdding the type at the end of the topic name might not always be necessary, but it can be a very helpful way for the Publishers to communicate to the Subscribers what the MIME type of the data will be.\n\nThink about how it will feel to query the topic later (which you can do with EnSQL!), e.g.\n\nSELECT DISTINCT reviewerid FROM productreviews_xml WHERE timestamp < 2022-05-11T13:00:00-04:00\n",
        "tags": []
    },
    {
        "uri": "/sdk/_index",
        "content": "---\ntitle: \"Developer SDKs\"\nweight: 20\ndate: 2023-05-17T17:03:41-04:00\n---\n\nWelcome Ensign user!\n\nIf you're looking to figure out how to write code to connect to Ensign, you've come to the right place.\n\n!--more--\n\nEnsign is written primarily in Go and Protocol Buffers because that's what we like, but hey, this isn't about us, it's about you.\n\nWe want Ensign to be as accessible as possible, so we made you some SDKs ðŸ’™\n\nAvailable SDKs\n\nHere is a list of the SDKs that are currently available.\n\nYou should be able to install each using your usual language-specific package manager (e.g. go get, pip install, npm install, etc.):\n\ngo-ensign\npyensign\nensignJS Note: ðŸ™ˆ this is just an empty repo for now, but we're working on it!!\n\nEach SDK is structured in a language-specific fashion and contains cross-sdk driver methods as well as tooling that may be tailored to users of specific languages.\n\nDon't see an Ensign SDK for the language you love most? Tell us (support@rotational.io), and we'll get right on it!\n\n What's in the SDK Repos?\n\nThe SDK repos above contain driver and library code for interacting with Ensign. They each have their own language-specific docs.\n\nCommon Methods\n\nHere are some of the common driver methods that all of the SDKs have in some shape or form (there are some implementation differences due to the ways that different languages handle concurrency).\n\nCreate an open Client to Ensign using your API keys\nCreate a new Topic on your Client\nStructure valid Ensign Events of various possible MimeTypes\nCreate a new Publisher on your Client and invoke this Publisher's Publish method to publish an Event to a Topic\nCreate a new Subscriber on your Client and create an event stream to collect published Events using this Subscriber's Subscribe method\n\n What are these Protobuf Things?\n\nProtocol buffers are useful and help us make Ensign real fast, but we know they aren't exactly common.\n\nIf you're familiar with other serialization formats like JSON and XML and are just getting up to speed with protobuf, check out this post.\n\nIf you're less familiar with serialization methods, the main thing to understand is that the protocol buffers are what define the Ensign service; they're a set of eventing-related rules and components that Ensign understands, and if you want to write code that interacts with Ensign, you have to explain what you want in terms of those rules and components.\n\nProtocol buffers are like a recipe for ingredients that have to be combined and baked (aka compiled) into code like a casserole before you can eat use it. But compiling protocol buffers is not always the easiest thing, so our SDK libraries each have a copy of the protobufs compiled in the SDK language.\n\nYou shouldn't (hopefully) have to jump through the compilation hoops &mdash; just install and import the SDK you need in the language you prefer. Hopefully this makes it as easy as possible for you!\n",
        "tags": []
    },
    {
        "uri": "/sdk/golang",
        "content": "---\ntitle: \"Golang\"\nweight: 50\ndate: 2023-08-11T09:03:41-04:00\n---\n\nThe Go SDK is a great choice for developers who wish to integrate Ensign into their existing projects. Or if you're starting from scratch and want to take advantage of Golang's static typing and high-performance multiprocessing, that's cool too.\n\nIn this example we'll create a simple Go project from scratch to publish and subscribe to Ensign!\n\nPrerequisites\n\ncreate a free Ensign account and API key\ndownload and install Golang according to your operating system\n\n Project Setup\n\nAs with any new Go project, start by creating a directory and module to start adding dependencies to.\n\nmkdir hello-ensign\ncd hello-ensign\ngo mod init example.com/hello/ensign\n\nThe next step is to install the official Go SDK for Ensign.\n\ngo get github.com/rotationalio/go-ensign\n\na name=\"create-a-client\"/a\nCreate a Client\n\nCreate a main.go file and create the Ensign client in code, which is similar to a database client like PostgreSQL or Mongo.\n\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\tensign \"github.com/rotationalio/go-ensign\"\n)\n\nfunc main() {\n    // Create an Ensign client\n\tclient, err := ensign.New()\n\tif err != nil {\n\t\tpanic(fmt.Errorf(\"could not create client: %s\", err))\n\t}\n\tdefer client.Close()\n\n    // Fetch status from Ensign\n\tctx := context.Background()\n\tstate, err := client.Status(ctx)\n\tif err != nil {\n\t\tpanic(fmt.Errorf(\"could not get status from Ensign: %s\", err))\n\t}\n\tfmt.Println(state.Status, state.Version)\n}\n\nThe Go SDK requires a Client ID and Client Secret to communicate with Ensign. We recommend specifying them in the environment like so (replace with the values in your API key).\n\nexport ENSIGNCLIENTID=DbIxBEtIUgNIClnFMDmvoZeMrLxUTJVa\nexport ENSIGNCLIENTSECRET=wAfRpXLTiWn7yo7HQzOCwxMvveqiHXoeVJghlSIK2YbMqOMCUiSVRVQOLT0ORrVS\n\nIf you find yourself having to manage multiple API keys on the same machine, you can also specify a path to a JSON file with your credentials.\n\nmy_project_key.json\n{\n    \"ClientID\": \"DbIxBEtIUgNIClnFMDmvoZeMrLxUTJVa\",\n    \"ClientSecret\": \"wAfRpXLTiWn7yo7HQzOCwxMvveqiHXoeVJghlSIK2YbMqOMCUiSVRVQOLT0ORrVS\"\n}\n\nclient, err := ensign.New(ensign.WithLoadCredentials(\"myprojectkey.json\"))\n\nRun the code as a Go program.\n\ngo run main.go\n\nIf you see a message like the following, then congratulations! You've successfully connected to Ensign!\n\n Make Some Data\n\nNext, we need some data! Generally this is the place where you'd connect to your live data source (a database, weather data, etc). But to keep things simple, we'll just create a single event, which starts with a map.\n\ndata := make(map[string]string)\ndata[\"sender\"] = \"Twyla\"\ndata[\"timestamp\"] = time.Now().String()\ndata[\"message\"] = \"Let's get this started!\"\n\nNext, we will convert our map into an event, which will allow you to specify the mimetype of the message you intend to send (in this case, we'll say it's JSON), and the event type (which will be a generic event for this example).\n\nevent := &ensign.Event{\n    Mimetype: mimetype.ApplicationJSON,\n    Type: &api.Type{\n\t\tName:         \"Generic\",\n\t\tMajorVersion: 1,\n\t\tMinorVersion: 0,\n\t\tPatchVersion: 0,\n    },\n}\n\nNext, we'll marshal our dictionary into the Data attribute of our sample event\n\nif event.Data, err = json.Marshal(data); err != nil {\n    panic(\"could not marshal data to JSON: \" + err.Error())\n}\n\nPublish Your Event\n\nNow we can publish your event by calling the Publish method on the Ensign client we created above. You'll also need to pass in a topic name, which will be a string. If you aren't sure what topic to use, you can quickly log into your Ensign dashboard and look it up.\n\nclient.Publish(\"quality-lemon-time\", event)\n\nYou can publish many events at a time if you want!\n\nclient.Publish(\"quality-lemon-time\", event, event2, event3, event4)\n\n Create a Subscriber\n\nSo now you've published some events to a topic. We can consume those events using the Subscribe method. Subscribe works a bit differently than Publish; it returns a Subscription with a Go channel that you can read events from.\n\nsub, err := client.Subscribe(\"quality-lemon-time\")\nif err != nil {\n    panic(fmt.Errorf(\"could not create subscriber: %s\", err))\n}\n\nfor event := range sub.C {\n    var m map[string]string\n    if err := json.Unmarshal(event.Data, &m); err != nil {\n        panic(fmt.Errorf(\"failed to unmarshal message: %s\", err))\n    }\n    fmt.Println(m[\"message\"])\n}\n\nTry running the program again and see if you can get the message!\n\ngo run main.go\n\nNext Steps\n\nYou're already well on your way to building your first event-driven microservice with Ensign!\n\nIf you're ready to see some more advanced examples with code, check out the End-to-end Examples.\n\nIf you're looking for more on the basics of event-driven systems, check out Eventing 101.\n\nHappy eventing!",
        "tags": []
    },
    {
        "uri": "/sdk/python",
        "content": "---\ntitle: \"Python\"\nweight: 30\ndate: 2023-08-11T11:03:41-04:00\n---\n\nThe Python SDK is the quickest way to get started with Ensign. In this example we'll create a simple Python project from scratch to publish and subscribe to Ensign!\n\nPrerequisites\n\ncreate a free Ensign account and API key\ndownload and install Python according to your operating system\n\nThe Python SDK is currently compatible with Python 3.7, 3.8, 3.9, and 3.10.\n\n Project Setup\n\nCreate a project directory and install the official Python SDK using pip.\n\nmkdir hello-ensign\ncd hello-ensign\npip install pyensign\n\na name=\"create-a-client\"/a\nCreate a Client\n\nCreate a main.py file and create the Ensign client in code, which is similar to a database client like PostgreSQL or Mongo. The Python SDK is an asyncio API, which means we need to run the client methods as coroutines. Python's async/await syntax abstracts most of this for us.\n\nimport asyncio\nfrom datetime import datetime\n\nfrom pyensign.events import Event\nfrom pyensign.ensign import Ensign\n\nasync def main():\n    client = Ensign()\n    status = await client.status()\n    print(status)\n\nif name == 'main':\n    asyncio.run(main())\n\nThe Python SDK requires a Client ID and Client Secret to communicate with Ensign. We recommend specifying them in the environment like so (replace with the values in your API key).\n\nexport ENSIGNCLIENTID=DbIxBEtIUgNIClnFMDmvoZeMrLxUTJVa\nexport ENSIGNCLIENTSECRET=wAfRpXLTiWn7yo7HQzOCwxMvveqiHXoeVJghlSIK2YbMqOMCUiSVRVQOLT0ORrVS\n\nIf you find yourself having to manage multiple API keys on the same machine, you can also specify a path to a JSON file with your credentials.\n\nmy_project_key.json\n{\n    \"ClientID\": \"DbIxBEtIUgNIClnFMDmvoZeMrLxUTJVa\",\n    \"ClientSecret\": \"wAfRpXLTiWn7yo7HQzOCwxMvveqiHXoeVJghlSIK2YbMqOMCUiSVRVQOLT0ORrVS\"\n}\n\nclient = Ensign(credpath=\"myproject_key.json\")\n\nRun the code as a Python program.\n\npython main.py\n\nIf you see a message like the following, then congratulations! You've successfully connected to Ensign!\n\nstatus: 1\nversion: 0.9.0-beta.12 (478db92)\nuptime: seconds: 130150\nnanos: 862300696\n\n Make Some Data\n\nNext, we need some data! Generally this is the place where you'd connect to your live data source (a database, weather data, etc). But to keep things simple, we'll just create a single event, which starts with a dictionary.\n\ndata = {\n    \"sender\": \"Twyla\",\n    \"timestamp\": datetime.now(),\n    \"message\": \"Let's get this started!\"\n}\n\nNext, we will convert our map into an event. Usually, an event contains some data (encoded to bytes), a mimetype which indicates how the data was encoded, and a schema type. The schema type consists of a name and a semantic version string.\n\nevent = Event(\n    json.dumps(data).encode(\"utf-8\"),\n    mimetype=\"application/json\",\n    schema_name=\"Generic\",\n    schema_version=\"1.0.0\"\n)\n\nPublish Your Event\n\nNow we can publish your event by awaiting the publish method on the Ensign client we created above. You'll also need to pass in a topic name, which will be a string. If you aren't sure what topic to use, you can quickly log into your Ensign dashboard and look it up.\n\nawait client.publish(\"quality-lemon-time\", event)\n\nYou can publish many events at a time if you want!\n\nawait client.publish(\"quality-lemon-time\", event, event2, event3, event4)\n\n Create a Subscriber\n\nSo now you've published some events to a topic. We can consume those events with the subscribe method. subscribe works a bit differently than publish. Instead of immediately returning, it yields events to the caller. We can use the async for syntax to process the events as they come in on the stream.\n\nasync for event in client.subscribe(\"quality-lemon-time\"):\n    msg = json.loads(event.data)\n    print(msg[\"message\"])\n\nTry running the program again and see if you can get the message!\n\npython main.go\n\nNext Steps\n\nYou're already well on your way to building your first event-driven microservice with Ensign!\n\nIf you're ready to see some more advanced examples with code, check out the End-to-end Examples.\n\nIf you're looking for more on the basics of event-driven systems, check out Eventing 101.\n\nHappy eventing!",
        "tags": []
    },
    {
        "uri": "/system/_index",
        "content": "---\ntitle: \"Ensign Internals\"\nweight: 90\ndate: 2023-05-17T17:03:41-04:00\n---\n\nThis section of the documentation describes Ensign Core &mdash; the internals of the Ensign system.\n",
        "tags": []
    },
    {
        "uri": "/system/broker",
        "content": "---\ntitle: \"Event Brokering\"\nweight: 3\ndate: 2023-05-17T17:03:41-04:00\n---\n\nAt the core of Ensign is the Broker.\n\nEvent brokers are what differentiate eventing systems (e.g. Kafka, Pulsar, Redpanda, Google PubSub, Ensign) from synchronous messaging systems (e.g. RabbitMQ, Ably, Amazon SQS).\n\nWhat Does a Broker Do?\n\nBrokers are responsible for a lot.\n\n{{ image src=\"img/broker.png\" alt=\"Brokers are responsible for persisting data, ordering events, and remembering subscriber offsets.\" zoomable=\"true\" }}\n\nHere are some of the main responsibilities of an Ensign broker:\n\nPersisting Data\n\nThe Ensign Broker persists data written to any topic by the Publisher so that multiple Subscribers can read data from that same topic. Persisting data also means that Ensign has geodistributed backups of data, so it's safe from things like earthquakes, floods, and other catastrophes.\n\nIn a synchronous messaging system like RabbitMQ, messages are discarded after they are acknowledged, so there is no expectation of persistence whatsoever.\n\nWhile tools like Kafka, Pulsar, and Google PubSub can be configured by editing YAML files to persist data indefinitely, enabling persistence has two consequences. First, it significantly reduces throughput and increases latency, because it requires the broker to do more work than usual. Enabling persistence by default also increases costs in these other systems, because storing data costs money.\n\nKeeping Events in the Right Order\n\nThe Ensign Broker keeps all events in order for every topic, even if there are publishers writing to that topic from two different sides of the Earth. Knowing that your events will always be totally ordered provides a powerful semantic that teams can use to design applications that will always respect the same ordering of events.\n\nEnsign events are ordered by the Broker using an RLID. An RLID is a totally ordered, 80-byte data structure that encodes both time and a monotonically increasing sequence number using Crockford's base32. It is inspired by ULID and Snowflake IDs.\n\nTo date, Ensign is the only eventing system in the world that guarantees totally ordered concurrent events within topics.\n\nRemembering the Offsets\n\nThe Ensign Broker remembers subscriber offsets. This means that an application reading data from Ensign does not have to maintain any state related to Ensign &mdash; this is why Ensign is particularly convenient for teams that deploy stateless applications.\n\nConsider two subscribers connecting to their Ensign Broker, Subscriber A that is connecting for the first time, and Subscriber B that has been dormant/inactive for some period of time. Presumably, Subscriber A and Subscriber B will want to start reading the data from different points in the topic stream; Subscriber A might want to read in all the events from the very beginning, while Subscriber B might prefer to start back from where it left off so that it can recover its function using the minimum needed computation or memory. Ensign's Broker maintains a mapping between topics and their subscribers.\n\n What Makes the Ensign Broker Unique?\n\nThere are several things that are unique in the implementation of the Ensign Broker that diverge from similar systems. Here are a few differentiators:\n\nIn Ensign, Data is persisted by default: In most similar systems, persistence is disabled by default, must be activated using configuration/YAML, and will instantly and significantly increase the costs.\nThe Ensign Broker stores the Subscriber offsets: This means Ensign Subscribers can be fully stateless.\nConsensus is flexible: Similar systems use Raft or Zookeeper for consensus. Ensign is built so that decision-making in the Broker can automatically shift into a hierarchical consensus mode. Hierarchical consensus enables small local consensus groups to function independently when possible, enabling far more geographic scaling than would be possible in any other system.\n\nIn combination, these differences mean that Ensign fulfills the criteria of both a database and an eventing platform, and is far safer for data (aka more fault tolerant) than any comparable eventing system.",
        "tags": []
    },
    {
        "uri": "/system/configuration",
        "content": "---\ntitle: \"Configuration\"\nweight: 10\ndate: 2023-05-17T17:03:41-04:00\n---\n\nNote: This page is for internal Ensign development and will probably not be very useful to Ensign users.\n\nEnsign services are primarily configured using environment variables and will respect dotenv files in the current working directory. The canonical reference of the configuration for an Ensign service is the config package of that service (described below). This documentation enumerates the most important configuration variables, their default values, and any hints or warnings about how to use them.\n\nRequired Configurationbr /\nIf a configuration parameter does not have a default value that means it is required and must be specified by the user! If the configuration parameter does have a default value then that environment variable does not have to be set.\n\n!--more--\n\nEnsign\n\nThe Ensign node is a replica of the Ensign eventing system. Its environment variables are all prefixed with the ENSIGN_ tag. The primary configuration is as follows:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| ENSIGN_MAINTENANCE | bool   | false   | Set node to maintenance mode, which will respond to requests with Unavailable except for status requests. |\n| ENSIGNLOGLEVEL   | string | info    | The verbosity of logging, one of trace, debug, info, warn, error, fatal, or panic.                             |\n| ENSIGNCONSOLELOG | bool   | false   | If true will print human readable logs instead of JSON logs for machine consumption.                           |\n| ENSIGNBINDADDR   | string | :5356   | The address and port the Ensign service will listen on.\n\n/div\n\n Monitoring\n\nEnsign uses Prometheus for metrics and observability. The Prometheus metrics server is configured as follows:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| ENSIGNMONITORINGENABLED   | bool   | true    | If true, the Prometheus metrics server is served.       |\n| ENSIGNMONITORINGBIND_ADDR | string | :1205   | The address and port the metrics server will listen on. |\n| ENSIGNMONITORINGNODE_ID   | string |         | Optional - a server name to tag metrics with.           |\n/div\n\nStorage\n\nThe Ensign storage configuration defines on disk where Ensign keeps its data. Configure storage as follows:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| ENSIGNSTORAGEREAD_ONLY | bool   | false   | If true then no writes or deletes will be allowed to the database.                                       |\n| ENSIGNSTORAGEDATA_PATH | string |         | The path to a directory on disk where Ensign will store its meta and event data.                         |\n| ENSIGNSTORAGETESTING   | bool   | false   | If true then a mock store will be opened rather than a leveldb store (should not be used in production). |\n\n/div\n\nNote that the Ensign data path must be to a directory. If the directory does not exist, it is created. An error occurs if the path is to a file or the process doesn't have permissions to access the directory. Ensign will open two different data stores in the data path: one for metadata and the other to store event data locally.\n\nIf the testing flag is set to true, a mock store is created that can be used in unit and integration tests.\n\n Authentication\n\nEnsign uses Quarterdeck to authenticate and authorize requests. This configuration defines how Ensign accesses public keys for JWT verification and how the authentication interceptor behaves.\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| ENSIGNAUTHKEYS_URL             | string   | https://auth.rotational.app/.well-known/jwks.json | The path to the public keys used to verify JWT tokens.                            |\n| ENSIGNAUTHAUDIENCE             | string   | https://ensign.rotational.app                     | The audience to verify inside of JWT tokens.                                      |\n| ENSIGNAUTHISSUER               | string   | https://auth.rotational.app                       | The issuer to verify inside of JWT tokens.                                        |\n| ENSIGNAUTHMINREFRESHINTERVAL | duration | 5m                                                | The minimum time to wait before refreshing the JWKS public keys in the validator. |\n/div\n\nSentry\n\nEnsign uses Sentry to assist with error monitoring and performance tracing. Configure Ensign to use Sentry as follows:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| ENSIGNSENTRYDSN               | string  |             | The DSN for the Sentry project. If not set then Sentry is considered disabled.                    |\n| ENSIGNSENTRYSERVER_NAME       | string  |             | Optional - a server name to tag Sentry events with.                                               |\n| ENSIGNSENTRYENVIRONMENT       | string  |             | The environment to report (e.g. development, staging, production). Required if Sentry is enabled. |\n| ENSIGNSENTRYRELEASE           | string  | {{version}} | Specify the release of Ensign for Sentry tracking. By default this will be the package version.   |\n| ENSIGNSENTRYTRACK_PERFORMANCE | bool    | false       | Enable performance tracing to Sentry with the specified sample rate.                              |\n| ENSIGNSENTRYSAMPLE_RATE       | float64 | 0.2         | The percentage of transactions to trace (0.0 to 1.0).                                             |\n| ENSIGNSENTRYDEBUG             | bool    | false       | Set Sentry to debug mode for testing.\n/div                                                         |\n\nSentry is considered enabled if a DSN is configured. Performance tracing is only enabled if Sentry is enabled and track performance is set to true. If Sentry is enabled, an environment is required, otherwise the configuration will be invalid.\n\nGenerally speaking, Ensign should enable Sentry for panic reports but should not enable performance tracing as this slows down the server too much. Note also that the sentry.Config object has a field Repanic that should not be set by the user. This field is used to manage panics in chained interceptors.\n\n Tenant\n\nThe Tenant API powers the user front-end for tenant management and configuration. Its environment variables are all prefixed with the TENANT_ tag. The primary configuration is as follows:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| TENANT_MAINTENANCE   | bool   | false                 | Sets the server to maintenance mode, which will respond to requests with Unavailable except for status requests. |\n| TENANTBINDADDR     | string | :8080                 | The address and port the Tenant service will listen on.                                                          |\n| TENANT_MODE          | string | release               | Sets the Gin mode, one of debug, release, or test.                                                               |\n| TENANTLOGLEVEL     | string | info                  | The verbosity of logging, one of trace, debug, info, warn, error, fatal, or panic.                               |\n| TENANTCONSOLELOG   | bool   | false                 | If true will print human readable logs instead of JSON logs for machine consumption.                             |\n| TENANTALLOWORIGINS | string | http://localhost:3000 | A comma separated list of allowed origins for CORS. Set to \"*\" to allow all origins.                             |\n/div\n\nAuthentication\n\nTenant uses Quarterdeck to authenticate and authorize requests. The following configuration defines how Tenant validates JWT tokens passed in from the user that were created by Quarterdeck and secures cookies:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| TENANTAUTHKEYS_URL      | string | https://auth.rotational.app/.well-known/jwks.json | The path to the public keys used to verify JWT tokens.                              |\n| TENANTAUTHAUDIENCE      | string | https://rotational.app                            | The audience to verify inside of JWT tokens.                                        |\n| TENANTAUTHISSUER        | string | https://auth.rotational.app                       | The issuer to verify inside of JWT tokens.                                          |\n| TENANTAUTHCOOKIE_DOMAIN | string | rotational.app                                    | Defines the domain that is set on cookies including CSRF cookies and token cookies. |\n/div\n\n Database\n\nTenant connects to a replicated trtl database for its data storage; the trtl connection is configured as follows:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| TENANTDATABASEURL       | string | trtl://localhost:4436 | The endpoint of the trtl database including the scheme (usually the k8s trtl service).         |\n| TENANTDATABASEINSECURE  | bool   | true                  | Connect to the trtl database without TLS (default true inside of a k8s cluster).               |\n| TENANTDATABASECERT_PATH | string |                       | The path to the mTLS certificate of the client to connect to trtl in an authenticated fashion. |\n| TENANTDATABASEPOOL_PATH | string |                       | The path to an x509 pool file with trusted trtl servers to connect to in.                      |\n/div\n\nQuarterdeck\n\nTenant connects directly to Quarterdeck in order to send pass-through requests from the user (e.g. for registration, login, etc). This is separate from the authentication configuration used in middleware as this configuration is used to setup a Quarterdeck API client.\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| TENANTQUARTERDECKURL            | string   | https://auth.rotational.app | The Quarterdeck endpoint to create the API client on.                                |\n| TENANTQUARTERDECKWAITFORREADY | duration | 5m                          | The amount of time to wait for Quarterdeck to come online before exiting with fatal. |\n/div\n\n SendGrid\n\nTenant uses SendGrid to assist with email notifications. Configure Tenant to use SendGrid as follows:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| TENANTSENDGRIDAPI_KEY        | string |                      | API Key to authenticate to SendGrid with.                                                            |\n| TENANTSENDGRIDFROM_EMAIL     | string | ensign@rotational.io | The email address in the \"from\" field of emails being sent to users.                                 |\n| TENANTSENDGRIDADMIN_EMAIL    | string | admins@rotational.io | The email address to send admin emails to from the server.                                           |\n| TENANTSENDGRIDENSIGNLISTID | string |                      | A contact list to add users to if they sign up for notifications.                                    |\n| TENANTSENDGRIDTESTING        | bool   | false                | If in testing mode no emails are actually sent but are stored in a mock email collection.            |\n| TENANTSENDGRIDARCHIVE        | string |                      | If in testing mode, specify a directory to save emails to in order to review emails being generated. |\n/div\n\nSendGrid is considered enabled if the SendGrid API Key is set. The from and admin email addresses are required if SendGrid is enabled.\n\nIf the Ensign List ID is configured then Tenant will add the contact requesting private beta access to that list, otherwise it will simply add the contact to \"all contacts\".\n\nSentry\n\nTenant uses Sentry to assist with error monitoring and performance tracing. Configure Tenant to use Sentry as follows:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| TENANTSENTRYDSN               | string  |             | The DSN for the Sentry project. If not set then Sentry is considered disabled.                    |\n| TENANTSENTRYSERVER_NAME       | string  |             | Optional - a server name to tag Sentry events with.                                               |\n| TENANTSENTRYENVIRONMENT       | string  |             | The environment to report (e.g. development, staging, production). Required if Sentry is enabled. |\n| TENANTSENTRYRELEASE           | string  | {{version}} | Specify the release of Ensign for Sentry tracking. By default this will be the package version.   |\n| TENANTSENTRYTRACK_PERFORMANCE | bool    | false       | Enable performance tracing to Sentry with the specified sample rate.                              |\n| TENANTSENTRYSAMPLE_RATE       | float64 | 0.2         | The percentage of transactions to trace (0.0 to 1.0).                                             |\n| TENANTSENTRYDEBUG             | bool    | false       | Set Sentry to debug mode for testing.                                                             |\n/div\n\nSentry is considered enabled if a DSN is configured. Performance tracing is only enabled if Sentry is enabled and track performance is set to true. If Sentry is enabled, an environment is required, otherwise the configuration will be invalid.\n\nNote also that the sentry.Config object has a field Repanic that should not be set by the user. This field is used to manage panics in chained interceptors.\n\n Quarterdeck\n\nThe Quarterdeck API handles authentication and authorization as well as API keys and billing management for the Ensign managed service. Its environment variables are all prefixed with the QUARTERDECK_ tag. The primary configuration is as follows:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| QUARTERDECK_MAINTENANCE     | bool   | false                         | Sets the server to maintenance mode, which will respond to requests with Unavailable except for status requests. |\n| QUARTERDECKBINDADDR       | string | :8088                         | The address and port the Quarterdeck service will listen on.                                                     |\n| QUARTERDECK_MODE            | string | release                       | Sets the Gin mode, one of debug, release, or test.                                                               |\n| QUARTERDECKLOGLEVEL       | string | info                          | The verbosity of logging, one of trace, debug, info, warn, error, fatal, or panic.                               |\n| QUARTERDECKCONSOLELOG     | bool   | false                         | If true will print human readable logs instead of JSON logs for machine consumption.                             |\n| QUARTERDECKALLOWORIGINS   | string | http://localhost:3000         | A comma separated list of allowed origins for CORS. Set to \"*\" to allow all origins.                             |\n| QUARTERDECKVERIFYBASE_URL | string | https://rotational.app/verify | The base url to generate verify email links to direct the user to in the email verification path.                |\n/div\n\nSendGrid\n\nQuarterdeck uses SendGrid to assist with email notifications. Configure Quarterdeck to use SendGrid as follows:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| QUARTERDECKSENDGRIDAPI_KEY        | string |                      | API Key to authenticate to SendGrid with.                                                            |\n| QUARTERDECKSENDGRIDFROM_EMAIL     | string | ensign@rotational.io | The email address in the \"from\" field of emails being sent to users.                                 |\n| QUARTERDECKSENDGRIDADMIN_EMAIL    | string | admins@rotational.io | The email address to send admin emails to from the server.                                           |\n| QUARTERDECKSENDGRIDENSIGNLISTID | string |                      | A contact list to add users to if they sign up for notifications.                                    |\n| QUARTERDECKSENDGRIDTESTING        | bool   | false                | If in testing mode no emails are actually sent but are stored in a mock email collection.            |\n| QUARTERDECKSENDGRIDARCHIVE        | string |                      | If in testing mode, specify a directory to save emails to in order to review emails being generated. |\n/div\n\nSendGrid is considered enabled if the SendGrid API Key is set. The from and admin email addresses are required if SendGrid is enabled.\n\nIf the Ensign List ID is configured then Quarterdeck will add the contact to that list to ensure they receive marketing emails about Ensign.\n\n Rate Limit\n\nIn order to prevent brute force attacks on the Quarterdeck system we've implemented a rate limiting middleware to prevent abuse. Rate limiting is configured as follows:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| QUARTERDECKRATELIMITPERSECOND | float64  | 10      | The maximum number of allowed requests per second.                                                             |\n| QUARTERDECKRATELIMIT_BURST      | int      | 30      | Maximum number of requests that is used to track rate of requests (if zero then all requests will be rejected) |\n| QUARTERDECKRATELIMIT_TTL        | duration | 5m      | How long an IP address is cached for rate limiting purposes.                                                   |\n/div\n\nIt is strongly recommended that the default configuration is used.\n\nDatabase\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| QUARTERDECKDATABASEURL       | string | sqlite3:////data/db/quarterdeck.db | The DSN for the sqlite3 database.                |\n| QUARTERDECKDATABASEREAD_ONLY | bool   | false                              | If true only read-only transactions are allowed. |\n/div\n\nQuarterdeck uses a Raft replicated Sqlite3 database for authentication. The URI should have the scheme sqlite3:// and then a path to the database. For a relative path, use sqlite3:///path/to/relative.db and for an absolute path use sqlite3:////path/to/absolute.db.\n\n Tokens\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| QUARTERDECKTOKENKEYS             | map[string]string |                             | The private keys to load into quarterdeck to issue JWT tokens with.                                 |\n| QUARTERDECKTOKENAUDIENCE         | string            | https://rotational.app      | The audience to add to the JWT keys for verification.                                               |\n| QUARTERDECKTOKENREFRESH_AUDIENCE | string            |                             | An optional additional audience to add only to refresh tokens.                                      |\n| QUARTERDECKTOKENISSUER           | string            | https://auth.rotational.app | The issuer to add to the JWT keys for verification.                                                 |\n| QUARTERDECKTOKENACCESS_DURATION  | duration          | 1h                          | The amount of time that an access token is valid for before it expires.                             |\n| QUARTERDECKTOKENREFRESH_DURATION | duration          | 2h                          | The amount of time that a refresh token is valid for before it expires.                             |\n| QUARTERDECKTOKENREFRESH_OVERLAP  | duration          | -15m                        | A negative duration that sets how much time the access and refresh tokens overlap in time validity. |\n/div\n\nTo create an environment variable that is a map[string]string use a string in the following form:\n\nkey1:value1,key2:value2\n\nThe token keys should be ULIDs keys (for ordering) and a path value to the key pair to load from disk. Generally speaking there should be two keys - the current key and the most recent previous key, though more keys can be added for verification. Only the most recent key will be used to issue tokens, however. For example, here is a valid key map:\n\n01GECSDK5WJ7XWASQ0PMH6K41K:/data/keys/01GECSDK5WJ7XWASQ0PMH6K41K.pem,01GECSJGDCDN368D0EENX23C7R:/data/keys/01GECSJGDCDN368D0EENX23C7R.pem\n\nFuture Featurebr /\nNote that in the future quarterdeck will generate its own keys and will not need them to be set as in the configuration above.\n\nReporting\n\nEnsign has a Daily PLG report that is sent to the Rotational admins for product led growth. This reporting tool is configured as follows:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| QUARTERDECKREPORTINGENABLEDAILYPLG | bool   | true             | Enables the Daily PLG report scheduler.               |\n| QUARTERDECKREPORTINGDOMAIN           | string | \"rotational.app\" | The domain that the report is being generated for.    |\n| QUARTERDECKREPORTINGDASHBOARD_URL    | string |                  | URL to the Grafana dashboard to include in the email. |\n/div\n\n Sentry\n\nQuarterdeck uses Sentry to assist with error monitoring and performance tracing. Configure Quarterdeck to use Sentry as follows:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| QUARTERDECKSENTRYDSN               | string  |             | The DSN for the Sentry project. If not set then Sentry is considered disabled.                    |\n| QUARTERDECKSENTRYSERVER_NAME       | string  |             | Optional - a server name to tag Sentry events with.                                               |\n| QUARTERDECKSENTRYENVIRONMENT       | string  |             | The environment to report (e.g. development, staging, production). Required if Sentry is enabled. |\n| QUARTERDECKSENTRYRELEASE           | string  | {{version}} | Specify the release of Ensign for Sentry tracking. By default this will be the package version.   |\n| QUARTERDECKSENTRYTRACK_PERFORMANCE | bool    | false       | Enable performance tracing to Sentry with the specified sample rate.                              |\n| QUARTERDECKSENTRYSAMPLE_RATE       | float64 | 0.2         | The percentage of transactions to trace (0.0 to 1.0).                                             |\n| QUARTERDECKSENTRYDEBUG             | bool    | false       | Set Sentry to debug mode for testing.                                                             |\n/div\n\nSentry is considered enabled if a DSN is configured. Performance tracing is only enabled if Sentry is enabled and track performance is set to true. If Sentry is enabled, an environment is required, otherwise the configuration will be invalid.\n\nNote also that the sentry.Config object has a field Repanic that should not be set by the user. This field is used to manage panics in chained interceptors.\n\nBeacon\n\nA React app delivers Beacon, the Ensign UI. Its environment variables are all prefixed with the REACT_APP tag. The primary configuration is as follows:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| REACTAPPVERSION_NUMBER  | string |         | The version number of the application build (set by tags in GitHub actions).       |\n| REACTAPPGIT_REVISION    | string |         | The git revision (short) of the application build (set by tags in GitHub actions). |\n| REACTAPPUSEDASHLOCALE | bool   | false   | If true the \"dash\" language is included for i18n debugging.                        |\n/div\n\n API Information\n\nConnection information for the backend is specified as follows:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| REACTAPPQUARTERDECKBASEURL | string | https://auth.rotational.app/v1 | The endpoint and the version of the API to connect to Quarterdeck on. |\n| REACTAPPTENANTBASEURL      | string | https://api.rotational.app/v1  | The endpoint and the version of the API to connect to Tenant on.      |\n/div\n\nGoogle Analytics\n\nThe React app uses Google Analytics to monitor website traffic. Configure the React app to use Google Analytics as follows:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| REACTAPPANALYTICS_ID          | string  |             | Google Analytics tracking ID for the React App.                                                   |\n/div\n\n Sentry\n\nThe React app uses Sentry to assist with error monitoring and performance tracing. Configure the React app to use Sentry as follows:\n\ndiv class=\"table\"\n\n| EnvVar | Type | Default | Description |\n|:------:|:------:|:------:|:-------:|\n| REACTAPPSENTRY_DSN         | string |         | The DSN for the Sentry project. If not set then Sentry is considered disabled.                    |\n| REACTAPPSENTRY_ENVIRONMENT | string |         | The environment to report (e.g. development, staging, production). Required if Sentry is enabled. |\n| REACTAPPSENTRYEVENTID    | string |         |                                                                      /div                             |\n\nSentry is considered enabled if a DSN is configured. If Sentry is enabled, an environment is strongly suggested, otherwise the NODE_ENV environment will be used.\n\nDevelopment\n\nKeep up to Date!br /\nIt is essential that we keep this configuration documentation up to date. The devops team uses it to ensure its services are configured correctly. Any time a configuration is changed ensure this documentation is also updated!\n\nTODO: this section will discuss confire, how to interpret environment variables from the configuration struct, how to test configuration, and how to add and change configuration variables. This section should also discuss dotenv files, docker compose, and all of the places where configuration can be influenced (e.g. GitHub actions for React builds).",
        "tags": []
    },
    {
        "uri": "/system/groups",
        "content": "---\ntitle: \"Consumer Groups\"\nweight: 5\ndate: 2023-05-17T17:03:41-04:00\n---\n\nConsumer groups allow multiple subscribers in different processes to coordinate how they consume events from a topic.\n\n!--more--\n\nConsumer Group Identification\n\nAll subscribers must specify the same consumer group ID or name in order to join the same group. Consumer groups are stored in Ensign with a 16 byte ID; therefore in order to ensure uniqueness, users have the following options:\n\nSpecify a 16 byte ID directly (e.g. a UUID or a ULID)\nSpecify a name or an ID of any length that is unique to the project.\n\nIn the first case, Ensign will not modify the ID at all, guaranteeing its uniqueness. However, in the second case, Ensign will use a murmur3 128 bit hash to ensure that the computed ID is 16 bytes. If the ID is specified it is hashed, otherwise the name of the group is hashed. It is strongly recommended that a name string is used for the hash.\n\nWhile the murmur3 hash does create the possibility of collisions, this will only happen for consumer groups in the same project (e.g. a consumer group with the same name in a different project will not cause a conflict). Therefore the probability is very low that a collision will occur. However, if you are creating a large number of consumer groups, it is generally better to use a UUID or ULID as the ID.",
        "tags": []
    },
    {
        "uri": "/system/staging",
        "content": "---\ntitle: \"Staging\"\nweight: 90\ndate: 2023-05-17T17:03:41-04:00\n---\n\nNote: This page is for internal Ensign development and will probably not be very useful to Ensign users. The staging environment has the latest code deployed frequently, may introduce breaking changes, and has it's data routinely deleted.\n\nStaging Environment\n\nEnsign developers can access the staging environment in order to perform testing and development or to QA release candidates before they are deployed.\n\nTo get started, make sure that you've created an API Key in the staging environment using the Beacon UI at https://ensign.world. Once you've obtained those credentials, add the following environment variables so that your script can access the credentials:\n\n$ENSIGNCLIENTID\n$ENSIGNCLIENTSECRET\n\nIf you're working on the Go SDK in staging, make sure you have the latest version from the commit rather than the latest tagged version so that your client code is up to date with what is in staging:\n\n$ go get github.com/rotationalio/go-ensign@main\n\nBy default the Ensign client connects to the Ensign production environment. To connect to Staging you need to specify the staging endpoints in your credentials:\n\nclient, err := ensign.New(&ensign.Options{\n    Endpoint: \"staging.ensign.world:443\",\n    ClientID: os.GetEnv(\"ENSIGNCLIENTID\"),\n    ClientSecret: os.GetEnv(\"ENSIGNCLIENTSECRET\"),\n    AuthURL: \"https://auth.ensign.world\",\n})\n\nIf you're feeling extra, you can also use the ensign.ninja:443 endpoint which is an alias for staging.ensign.world:443.",
        "tags": []
    }
]